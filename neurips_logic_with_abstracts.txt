{'title': 'Proof Extraction for Logical Neural Networks', 'authors': ['Thabang Lebese', 'Ndivhuwo Makondo', 'Cristina Cornelio', 'Naweed Khan'], 'Conference': 'AIPLANS', 'date': 'Published: 23 Oct 2021, Last Modified: 05 May 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=Xw3kb6UyA31&name=pdf', 'abstract': '</span><span class="note_content_value">Automated Theorem Provers (ATPs) are widely used for the verification of logicalstatements.  Explainability is one of the key advantages of ATPs:  providing anexpert readable proof path which shows the inference steps taken to concludecorrectness. Conversely, Neuro-Symbolic Networks (NSNs) that perform theoremproving, do not have this capability.  We propose a proof-tracing and filteringalgorithm to provide explainable reasoning in the case of Logical Neural Networks(LNNs), a special type of Neural-Theorem Prover (NTP).</span>'}
{'title': 'LogiGAN: Learning Logical Reasoning via Adversarial Pre-training', 'authors': ['Xinyu Pi', 'Wanjun Zhong', 'Yan Gao', 'Nan Duan', 'Jian-Guang Lou'], 'Conference': 'NeurIPS 2022 Accept', 'date': 'Published: 31 Oct 2022, Last Modified: 16 Sept 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=skgJy0CjAO&name=pdf', 'abstract': '</span><span class="note_content_value">We present LogiGAN, an unsupervised adversarial pre-training framework for improving logical reasoning abilities of language models. Upon automatic identification of logical reasoning phenomena in massive text corpus via detection heuristics, we train language models to predict the masked-out logical statements. Inspired by the facilitation effect of reflective thinking in human learning, we analogically simulate the learning-thinking process with an adversarial Generator-Verifier architecture to assist logic learning. LogiGAN implements a novel sequential GAN approach that (a) circumvents the non-differentiable challenge of the sequential GAN by leveraging the Generator as a sentence-level generative likelihood scorer with a learning objective of reaching scoring consensus with the Verifier; (b) is computationally feasible for large-scale pre-training with arbitrary target length. Both base and large size language models pre-trained with LogiGAN demonstrate obvious performance improvement on 12 datasets requiring general reasoning abilities, revealing the fundamental role of logic in broad reasoning, as well as the effectiveness of LogiGAN. Ablation studies on LogiGAN components reveal the relative orthogonality between linguistic and logic abilities and suggest that reflective thinking\'s facilitation effect might also generalize to machine learning.</span>'}
{'title': 'Deep Differentiable Logic Gate Networks', 'authors': ['Felix Petersen', 'Christian Borgelt', 'Hilde Kuehne', 'Oliver Deussen'], 'Conference': 'NeurIPS 2022 Accept', 'date': 'Published: 31 Oct 2022, Last Modified: 16 Sept 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=vF3WefcoePW&name=pdf', 'abstract': '</span><span class="note_content_value">Recently, research has increasingly focused on developing efficient neural network architectures. In this work, we explore logic gate networks for machine learning tasks by learning combinations of logic gates. These networks comprise logic gates such as "AND" and "XOR", which allow for very fast execution. The difficulty in learning logic gate networks is that they are conventionally non-differentiable and therefore do not allow training with gradient descent. Thus, to allow for effective training, we propose differentiable logic gate networks, an architecture that combines real-valued logics and a continuously parameterized relaxation of the network. The resulting discretized logic gate networks achieve fast inference speeds, e.g., beyond a million images of MNIST per second on a single CPU core.</span>'}
{'title': 'Adaptive Pre-training of Language Models for Better Logical Reasoning', 'authors': ['Soumya Sanyal', 'Yichong Xu', 'Shuohang Wang', 'Ziyi Yang', 'Reid Pryzant', 'Wenhao Yu', 'Chenguang Zhu', 'Xiang Ren'], 'Conference': 'NeurIPS 2022 Workshop DistShift Poster', 'date': 'Published: 20 Oct 2022, Last Modified: 05 May 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=SWlp1gdlmd&name=pdf', 'abstract': '</span><span class="note_content_value">Logical reasoning of text is an important ability that requires understanding the logical information present in the text and reasoning through them to infer new conclusions. Prior works on improving the logical reasoning ability of language models require complex processing of training data (e.g., aligning symbolic knowledge to text), yielding task-specific data augmentation solutions that restrict the learning of general logical reasoning skills.  In this work, we propose AERIE, an adaptively pre-trained language model that has improved logical reasoning abilities. We select a subset of Wikipedia, based on a set of logical inference keywords, for continued pretraining of a language model. We use two self-supervised loss functions: a modified masked language modeling loss where only specific parts-of-speech words, that would likely require more reasoning than basic language understanding, are masked, and a sentence classification loss that teaches the model to distinguish between entailment and contradiction types of sentences. The proposed training paradigm is both simple and generalizable across tasks. We demonstrate the effectiveness of AERIE by comparing it with prior baselines on two logical reasoning datasets. AERIE performs comparably on ReClor and outperforms baselines on LogiQA.</span>'}
{'title': 'Improving Certified Robustness via Statistical Learning with Logical Reasoning', 'authors': ['Zhuolin Yang', 'Zhikuan Zhao', 'Boxin Wang', 'Jiawei Zhang', 'Linyi Li', 'Hengzhi Pei', 'Bojan Karlaš', 'Ji Liu', 'Heng Guo', 'Ce Zhang', 'Bo Li'], 'Conference': 'NeurIPS 2022 Accept', 'date': 'Published: 31 Oct 2022, Last Modified: 13 Oct 2022', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=fY6OzqOiTnu&name=pdf', 'abstract': '</span><span class="note_content_value">Intensive algorithmic efforts have been made to enable the rapid improvements of certificated robustness for complex ML models recently. However, current robustness certification methods are only able to certify under a limited perturbation radius. Given that existing pure data-driven statistical approaches have reached a bottleneck, in this paper, we propose to integrate statistical ML models with knowledge (expressed as logical rules) as a reasoning component using Markov logic networks (MLN), so as to further improve the overall certified robustness. This opens new research questions about certifying the robustness of such a paradigm, especially the reasoning component (e.g., MLN). As the first step towards understanding these questions, we first prove that the computational complexity of certifying the robustness of MLN is #P-hard. Guided by this hardness result, we then derive the first certified robustness bound for MLN by carefully analyzing different model regimes. Finally, we conduct extensive experiments on five datasets including both high-dimensional images and natural language texts, and we show that the certified robustness with knowledge-based logical reasoning indeed significantly outperforms that of the state-of-the-arts.</span>'}
{'title': 'Logical Credal Networks', 'authors': ['Radu Marinescu', 'Haifeng Qian', 'Alexander G. Gray', 'Debarun Bhattacharjya', 'Francisco Barahona', 'Tian Gao', 'Ryan Riegel', 'Pravinda Sahu'], 'Conference': 'NeurIPS 2022 Accept', 'date': 'Published: 31 Oct 2022, Last Modified: 11 Oct 2022', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=wiGXs_kS_X&name=pdf', 'abstract': '</span><span class="note_content_value">We introduce Logical Credal Networks (or LCNs for short) -- an expressive probabilistic logic that generalizes prior formalisms that combine logic and probability. Given imprecise information represented by probability bounds and conditional probability bounds on logic formulas, an LCN specifies a set of probability distributions over all its interpretations. Our approach allows propositional and first-order logic formulas with few restrictions, e.g., without requiring acyclicity. We also define a generalized Markov condition that allows us to identify implicit independence relations between atomic formulas. We evaluate our method on benchmark problems such as random networks, Mastermind games with uncertainty and credit card fraud detection. Our results show that the LCN outperforms existing approaches; its advantage lies in aggregating multiple sources of imprecise information.</span>'}
{'title': 'Skill Machines: Temporal Logic Composition in Reinforcement Learning', 'authors': ['Geraud Nangue Tasse', 'Devon Jarvis', 'Steven James', 'Benjamin Rosman'], 'Conference': 'Deep RL Workshop 2022', 'date': '08 Oct 2022 (modified: 05 May 2023)', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=VJ6JTmW5dY&name=pdf', 'abstract': '</span><span class="note_content_value">A major challenge in reinforcement learning is specifying tasks in a manner that is both interpretable and verifiable. One common approach is to specify tasks through reward machines---finite state machines that encode the task to be solved. We introduce skill machines, a representation that can be learned directly from these reward machines that encode the solution to such tasks. We propose a framework where an agent first learns a set of base skills in a reward-free setting, and then combines these skills with the learned skill machine to produce composite behaviours specified by any regular language, such as linear temporal logics. This provides the agent with the ability to map from complex logical task specifications to near-optimal behaviours zero-shot. We demonstrate our approach in both a tabular and high-dimensional video game environment, where an agent is faced with several of these complex, long-horizon tasks. Our results indicate that the agent is capable of satisfying extremely complex task specifications, producing near optimal performance with no further learning. Finally, we demonstrate that the performance of skill machines can be improved with regular off-policy reinforcement learning algorithms when optimal behaviours are desired.</span>'}
{'title': 'Fact-driven Logical Reasoning', 'authors': ['Siru Ouyang', 'Zhuosheng Zhang', 'hai zhao'], 'Conference': 'NeurIPS 2021 Submitted', 'date': '21 May 2021 (modified: 16 Sept 2023)', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=MBH29cOeohr&name=pdf', 'abstract': '</span><span class="note_content_value">Logical reasoning deeply relies on accurate, clearly presented clue forms which are usually modeled as entity-like knowledge in existing studies. However, in real hierarchical reasoning motivated machine reading comprehension (MRC), such one-side modeling are insufficient for those indispensable local complete facts or events when only "global" knowledge is really paid attention to. Thus, in view of language being a complete knowledge/clue carrier, we propose a general formalism to support representing logic units by extracting backbone constituents of the sentence such as the subject-verb-object formed "facts", covering both global and local knowledge pieces that are necessary as the basis for logical reasoning. Beyond building the ad-hoc graphs, we propose a more general and convenient fact-driven approach to construct a supergraph on top of our newly defined fact units, and enhance the supergraph with further explicit guidance of local question and option interactions. Experiments on two challenging logical reasoning MRC benchmarks show that our proposed model, \\textsc{Focal Reasoner}, outperforms the baseline models dramatically.</span>'}
{'title': 'Inductive Logical Query Answering in Knowledge Graphs', 'authors': ['Mikhail Galkin', 'Zhaocheng Zhu', 'Hongyu Ren', 'Jian Tang'], 'Conference': 'NeurIPS 2022 Accept', 'date': 'Published: 31 Oct 2022, Last Modified: 16 Sept 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=-vXEN5rIABY&name=pdf', 'abstract': '</span><span class="note_content_value">Formulating and answering logical queries is a standard communication interface for knowledge graphs (KGs). \nAlleviating the notorious incompleteness of real-world KGs, neural methods achieved impressive results in link prediction and complex query answering tasks by learning representations of entities, relations, and queries. Still, most existing query answering methods rely on transductive entity embeddings and cannot generalize to KGs containing new entities without retraining entity embeddings. \nIn this work, we study the inductive query answering task where inference is performed on a graph containing new entities with queries over both seen and unseen entities. To this end, we devise two mechanisms leveraging inductive node and relational structure representations powered by graph neural networks (GNNs).\nExperimentally, we show that inductive models are able to perform logical reasoning at inference time over unseen nodes generalizing to graphs up to 500% larger than training ones. Exploring the efficiency--effectiveness trade-off, we find the inductive relational structure representation method generally achieves higher performance, while the inductive node representation method is able to answer complex queries in the inference-only regime without any training on queries and scale to graphs of millions of nodes. Code is available at \n<a href="https://github.com/DeepGraphLearning/InductiveQE" target="_blank" rel="nofollow">https://github.com/DeepGraphLearning/InductiveQE</a></span>'}
{'title': 'VAEL: Bridging Variational Autoencoders and Probabilistic Logic Programming', 'authors': ['Eleonora Misino', 'Giuseppe Marra', 'Emanuele Sansone'], 'Conference': 'NeurIPS 2022 Accept', 'date': 'Published: 31 Oct 2022, Last Modified: 15 Jan 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=0xbP4W7rdJW&name=pdf', 'abstract': '</span><span class="note_content_value">We present VAEL, a neuro-symbolic generative model integrating variational autoencoders (VAE) with the reasoning capabilities of probabilistic logic (L) programming.  Besides standard latent subsymbolic variables, our model exploits a probabilistic logic program to define a further structured representation, which is used for logical reasoning. The entire process is end-to-end differentiable. Once trained, VAEL can solve new unseen generation tasks by (i) leveraging the previously acquired knowledge encoded in the neural component and (ii) exploiting new logical programs on the structured latent space. Our experiments provide support on the benefits of this neuro-symbolic integration both in terms of task generalization and data efficiency. To the best of our knowledge, this work is the first to propose a general-purpose end-to-end framework integrating probabilistic logic programming into a deep generative model.</span>'}
{'title': "Don't Pour Cereal into Coffee: Differentiable Temporal Logic for Temporal Action Segmentation", 'authors': ['Ziwei Xu', 'Yogesh S Rawat', 'Yongkang Wong', 'Mohan Kankanhalli', 'Mubarak Shah'], 'Conference': 'NeurIPS 2022 Accept', 'date': 'Published: 31 Oct 2022, Last Modified: 11 Jan 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=PCQyUvAmKs&name=pdf', 'abstract': '</span><span class="note_content_value">We propose Differentiable Temporal Logic (DTL), a model-agnostic framework that introduces temporal constraints to deep networks. DTL treats the outputs of a network as a truth assignment of a temporal logic formula, and computes a temporal logic loss reflecting the consistency between the output and the constraints. We propose a comprehensive set of constraints, which are implicit in data annotations, and incorporate them with deep networks via DTL. We evaluate the effectiveness of DTL on the temporal action segmentation task and observe improved performance and reduced logical errors in the output of different task models. Furthermore, we provide an extensive analysis to visualize the desirable effects of DTL.</span>'}
{'title': 'Fuzzy Logic for Biological Networks as ML Regression: Scaling to Single-Cell Datasets With Autograd', 'authors': ['Constance LE GAC', 'Alice Driessen', 'Nicolas Deutschmann', 'Maria Rodriguez Martinez'], 'Conference': 'LMRL 2022 Poster', 'date': 'Published: 28 Nov 2022, Last Modified: 05 May 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=KhdgF56cbY&name=pdf', 'abstract': '</span><span class="note_content_value">We present the BioFuzzNet module, a fuzzy logic tool to model signal transduction in biological networks. By equating the optimisation of the fuzzy logic transfer functions to a regression problem, we show that gradient descent is a suitable optimisation method for fuzzy logic modelling. The speed of this approach allows us to scale fuzzy logic modelling to single-cell datasets and leverage available transcriptomics data. Furthermore, the flexibility of gradient descent optimisation allows us to perform arbitrary computations, thereby enabling us to model feedback loops and fit them in simple cases. Promising results also suggest that BioFuzzNet can generate insights in the signalling network topology by identifying logical gates and spurious connections.</span>'}
{'title': 'Logical Activation Functions: Logit-space equivalents of Probabilistic Boolean Operators', 'authors': ['Scott C Lowe', 'Robert Earle', "Jason d'Eon", 'Thomas Trappenberg', 'Sageev Oore'], 'Conference': 'NeurIPS 2022 Accept', 'date': 'Published: 31 Oct 2022, Last Modified: 15 Jan 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=m6HNNpQO8dc&name=pdf', 'abstract': '</span><span class="note_content_value">The choice of activation functions and their motivation is a long-standing issue within the neural network community. Neuronal representations within artificial neural networks are commonly understood as logits, representing the log-odds score of presence of features within the stimulus. We derive logit-space operators equivalent to probabilistic Boolean logic-gates AND, OR, and XNOR for independent probabilities. Such theories are important to formalize more complex dendritic operations in real neurons, and these operations can be used as activation functions within a neural network, introducing probabilistic Boolean-logic as the core operation of the neural network. Since these functions involve taking multiple exponents and logarithms, they are computationally expensive and not well suited to be directly used within neural networks. Consequently, we construct efficient approximations named <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="0" style="font-size: 121.3%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mtext class="mjx-n"><mjx-c class="mjx-c41"></mjx-c><mjx-c class="mjx-c4E"></mjx-c><mjx-c class="mjx-c44"></mjx-c></mjx-mtext><mjx-script style="vertical-align: -0.153em;"><mjx-mtext class="mjx-n" size="s"><mjx-c class="mjx-c41"></mjx-c><mjx-c class="mjx-c49"></mjx-c><mjx-c class="mjx-c4C"></mjx-c></mjx-mtext></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mtext>AND</mtext><mtext>AIL</mtext></msub></math></mjx-assistive-mml></mjx-container> (the AND operator Approximate for Independent Logits), <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="1" style="font-size: 121.3%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mtext class="mjx-n"><mjx-c class="mjx-c4F"></mjx-c><mjx-c class="mjx-c52"></mjx-c></mjx-mtext><mjx-script style="vertical-align: -0.153em;"><mjx-mtext class="mjx-n" size="s"><mjx-c class="mjx-c41"></mjx-c><mjx-c class="mjx-c49"></mjx-c><mjx-c class="mjx-c4C"></mjx-c></mjx-mtext></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mtext>OR</mtext><mtext>AIL</mtext></msub></math></mjx-assistive-mml></mjx-container>, and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="2" style="font-size: 121.3%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mtext class="mjx-n"><mjx-c class="mjx-c58"></mjx-c><mjx-c class="mjx-c4E"></mjx-c><mjx-c class="mjx-c4F"></mjx-c><mjx-c class="mjx-c52"></mjx-c></mjx-mtext><mjx-script style="vertical-align: -0.153em;"><mjx-mtext class="mjx-n" size="s"><mjx-c class="mjx-c41"></mjx-c><mjx-c class="mjx-c49"></mjx-c><mjx-c class="mjx-c4C"></mjx-c></mjx-mtext></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mtext>XNOR</mtext><mtext>AIL</mtext></msub></math></mjx-assistive-mml></mjx-container>, which utilize only comparison and addition operations, have well-behaved gradients, and can be deployed as activation functions in neural networks. Like MaxOut, <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="3" style="font-size: 121.3%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mtext class="mjx-n"><mjx-c class="mjx-c41"></mjx-c><mjx-c class="mjx-c4E"></mjx-c><mjx-c class="mjx-c44"></mjx-c></mjx-mtext><mjx-script style="vertical-align: -0.153em;"><mjx-mtext class="mjx-n" size="s"><mjx-c class="mjx-c41"></mjx-c><mjx-c class="mjx-c49"></mjx-c><mjx-c class="mjx-c4C"></mjx-c></mjx-mtext></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mtext>AND</mtext><mtext>AIL</mtext></msub></math></mjx-assistive-mml></mjx-container> and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="4" style="font-size: 121.3%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mtext class="mjx-n"><mjx-c class="mjx-c4F"></mjx-c><mjx-c class="mjx-c52"></mjx-c></mjx-mtext><mjx-script style="vertical-align: -0.153em;"><mjx-mtext class="mjx-n" size="s"><mjx-c class="mjx-c41"></mjx-c><mjx-c class="mjx-c49"></mjx-c><mjx-c class="mjx-c4C"></mjx-c></mjx-mtext></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mtext>OR</mtext><mtext>AIL</mtext></msub></math></mjx-assistive-mml></mjx-container> are generalizations of ReLU to two-dimensions. While our primary aim is to formalize dendritic computations within a logit-space probabilistic-Boolean framework, we deploy these new activation functions, both in isolation and in conjunction to demonstrate their effectiveness on a variety of tasks including tabular classification, image classification, transfer learning, abstract reasoning, and compositional zero-shot learning.</span>'}
{'title': 'A Direct Approximation of AIXI Using Logical State Abstractions', 'authors': ['Samuel Yang-Zhao', 'Tianyu Wang', 'Kee Siong Ng'], 'Conference': 'NeurIPS 2022 Accept', 'date': 'Published: 31 Oct 2022, Last Modified: 12 Oct 2022', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=92leLHqlcvv&name=pdf', 'abstract': '</span><span class="note_content_value">We propose a practical integration of logical state abstraction with AIXI, a Bayesian optimality notion for reinforcement learning agents, to significantly expand the model class that AIXI agents can be approximated over to complex history-dependent and structured environments. The state representation and reasoning framework is based on higher-order logic, which can be used to define and enumerate complex features on non-Markovian and structured environments. We address the problem of selecting the right subset of features to form state abstractions by adapting the <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="0" style="font-size: 121.3%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-n"><mjx-c class="mjx-c3A6"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="normal">Φ</mi></math></mjx-assistive-mml></mjx-container>-MDP optimisation criterion from state abstraction theory. Exact Bayesian model learning is then achieved using a suitable generalisation of Context Tree Weighting over abstract state sequences. The resultant architecture can be integrated with different planning algorithms. Experimental results on controlling epidemics on large-scale contact networks validates the agent\'s performance.</span>'}
{'title': 'Neural Circuit Synthesis from Specification Patterns', 'authors': ['Frederik Schmitt', 'Christopher Hahn', 'Markus Norman Rabe', 'Bernd Finkbeiner'], 'Conference': 'NeurIPS 2021 Poster', 'date': 'Published: 09 Nov 2021, Last Modified: 16 Sept 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=O4TE57kehc1&name=pdf', 'abstract': '</span><span class="note_content_value">We train hierarchical Transformers on the task of synthesizing hardware circuits directly out of high-level logical speciﬁcations in linear-time temporal logic (LTL). The LTL synthesis problem is a well-known algorithmic challenge with a long history and an annual competition is organized to track the improvement of algorithms and tooling over time. New approaches using machine learning might open a lot of possibilities in this area, but suffer from the lack of sufﬁcient amounts of training data. In this paper, we consider a method to generate large amounts of additional training data, i.e., pairs of speciﬁcations and circuits implementing them. We ensure that this synthetic data is sufﬁciently close to human-written speciﬁcations by mining common patterns from the speciﬁcations used in the synthesis competitions. We show that hierarchical Transformers trained on this synthetic data solve a signiﬁcant portion of problems from the synthesis competitions, and even out-of-distribution examples from a recent case study.</span>'}
{'title': 'Self-explaining deep models with logic rule reasoning', 'authors': ['Seungeon Lee', 'Xiting Wang', 'Sungwon Han', 'Xiaoyuan Yi', 'Xing Xie', 'Meeyoung Cha'], 'Conference': 'NeurIPS 2022 Accept', 'date': 'Published: 31 Oct 2022, Last Modified: 16 Sept 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=8SY8ete3zu&name=pdf', 'abstract': '</span><span class="note_content_value">We present SELOR, a framework for integrating self-explaining capabilities into a given deep model to achieve both high prediction performance and human precision. By “human precision”, we refer to the degree to which humans agree with the reasons models provide for their predictions. Human precision affects user trust and allows users to collaborate closely with the model. We demonstrate that logic rule explanations naturally satisfy them with the expressive power required for good predictive performance. We then illustrate how to enable a deep model to predict and explain with logic rules. Our method does not require predefined logic rule sets or human annotations and can be learned efficiently and easily with widely-used deep learning modules in a differentiable way. Extensive experiments show that our method gives explanations closer to human decision logic than other methods while maintaining the performance of the deep learning model.</span>'}
{'title': 'Scallop: From Probabilistic Deductive Databases to Scalable Differentiable Reasoning', 'authors': ['Jiani Huang', 'Ziyang Li', 'Binghong Chen', 'Karan Samel', 'Mayur Naik', 'Le Song', 'Xujie Si'], 'Conference': 'NeurIPS 2021 Poster', 'date': 'Published: 09 Nov 2021, Last Modified: 05 May 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=ngdcA1tlDvj&name=pdf', 'abstract': '</span><span class="note_content_value">Deep learning and symbolic reasoning are complementary techniques for an intelligent system. However, principled combinations of these techniques have limited scalability, rendering them ill-suited for real-world applications. We propose Scallop, a system that builds upon probabilistic deductive databases, to bridge this gap. The key insight underlying Scallop is a provenance framework that introduces a tunable parameter to specify the level of reasoning granularity. Scallop thereby i) generalizes exact probabilistic reasoning, ii) asymptotically reduces computational cost, and iii) provides relative accuracy guarantees. On a suite of tasks that involve mathematical and logical reasoning, Scallop scales significantly better without sacrificing accuracy compared to DeepProbLog, a principled neural logic programming approach. We also create and evaluate on a real-world Visual Question Answering (VQA) benchmark that requires multi-hop reasoning. Scallop outperforms two VQA-tailored models, a Neural Module Networks based and a transformer based model, by 12.42% and 21.66% respectively.\n</span>'}
{'title': 'Bridging Machine Learning and Logical Reasoning by Abductive Learning', 'authors': ['Wang-Zhou Dai'], 'Conference': 'NeurIPS 2019', 'date': '06 Sept 2019 (modified: 05 May 2023)', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=ryxA8NHeLB&name=pdf', 'abstract': '</span><span class="note_content_value">Perception and reasoning are two representative abilities of intelligence that are integrated seamlessly during human problem-solving processes. In the area of artificial intelligence (AI), the two abilities are usually realised by machine learning and logic programming, respectively. However, the two categories of techniques were developed separately throughout most of the history of AI. In this paper, we propose the abductive learning framework targeted at unifying the two AI paradigms in a mutually beneficial way, where the machine learning model learns to perceive primitive logic facts from data, while logical reasoning can correct the wrongly perceived facts for improving the machine learning models. Furthermore, we propose a novel approach to optimise the machine learning model and the logical reasoning model jointly. We demonstrate that by using abductive learning, machines can learn to recognise numbers and resolve unknown mathematical operations simultaneously from images of simple hand-written equations. Moreover, the learned models can be generalised to longer equations and adapted to different tasks, which is beyond the capability of state-of-the-art deep learning models.</span>'}
{'title': 'An "interpretable-by-design" neural network to decipher RNA splicing regulatory logic', 'authors': ['Susan Elizabeth Liao', 'Mukund Sudarshan', 'Oded Regev'], 'Conference': 'AI4Science Oral', 'date': 'Published: 20 Oct 2022, Last Modified: 05 May 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=UPMzi3uCx5p&name=pdf', 'abstract': '</span><span class="note_content_value">Artificial intelligence algorithms, in particular neural networks, capture complex quantitative relationships between input and output. However, as neural networks are typically black box, it is difficult to extract post-hoc insights on how they achieve their predictive success. Furthermore, they easily capture artifacts or biases in the training data, often fail to generalize beyond the datasets used for training and testing, and do not lead to new insights on the underlying processes. To enable scientific progress, machine learning models should not only accurately predict outcomes, but also describe how they arrived at their predictions. In recent years, neural networks have been applied to understanding biological processes, and specifically in deciphering RNA splicing, a fundamental process in the transfer of genomic information into functional biochemical products. Despite recent success using neural networks to predict splicing outcomes, understanding how specific RNA features dictates splicing outcomes remains an open challenge. The challenge is further underscored by the sensitivity of splicing logic, where almost all single nucleotide changes along an exon can lead to dramatic changes in splicing outcomes. Here we demonstrate that an "interpretable-by-design" model achieves predictive accuracy without sacrificing interpretability and captures a unifying decision-making logic. Although we designed our model to emphasize interpretability, its predictive accuracy is on par with state-of-the-art models. Importantly, the model revealed novel components of splicing logic, which we experimentally validated. To demonstrate the model\'s interpretability, we introduce a visualization that, for any given exon, allows us to trace and quantify the entire decision process from input sequence to output splicing prediction. The network\'s ability to quantify contributions of specific features to splicing outcomes for individual exons has considerable potential for a range of medical and biotechnology applications, including genome- or RNA-editing of target exons to correct splicing behavior or guiding rational design of RNA-based therapeutics like antisense oligonucleotides.</span>'}
{'title': 'Generalisation in Lifelong Reinforcement Learning through Logical Composition', 'authors': ['Geraud Nangue Tasse', 'Steven James', 'Benjamin Rosman'], 'Conference': 'Deep RL Workshop NeurIPS 2021', 'date': '12 Oct 2021 (modified: 05 May 2023)', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=kO-Cgmasm7&name=pdf', 'abstract': '</span><span class="note_content_value">We leverage logical composition in reinforcement learning to create a framework that enables an agent to autonomously determine whether a new task can be immediately solved using its existing abilities, or whether a task-specific skill should be learned. In the latter case, the proposed algorithm also enables the agent to learn the new task faster by generating an estimate of the optimal policy. Importantly, we provide two main theoretical results: we give bounds on the performance of the transferred policy on a new task, and we give bounds on the necessary and sufficient number of tasks that need to be learned throughout an agent\'s lifetime to generalise over a distribution. We verify our approach in a series of experiments, where we perform transfer learning both after learning a set of base tasks, and after learning an arbitrary set of tasks. We also demonstrate that as a side effect of our transfer learning approach, an agent can produce an interpretable Boolean expression of its understanding of the current task. Finally, we demonstrate our approach in the full lifelong setting where an agent receives tasks from an unknown distribution and, starting from zero skills, is able to quickly generalise over the task distribution after learning only a few tasks---which are sub-logarithmic in the size of the task space.</span>'}
{'title': 'STAR: A Benchmark for Situated Reasoning in Real-World Videos', 'authors': ['Bo Wu', 'Shoubin Yu', 'Zhenfang Chen', 'Joshua B. Tenenbaum', 'Chuang Gan'], 'Conference': 'NeurIPS 2021 Datasets and Benchmarks Track (Round 2)', 'date': 'Published: 11 Oct 2021, Last Modified: 23 May 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=EfgNF5-ZAjM&name=pdf', 'abstract': '</span><span class="note_content_value">Reasoning in the real world is not divorced from situations. How to capture the present knowledge from surrounding situations and perform reasoning accordingly is crucial and challenging for machine intelligence. This paper introduces a new benchmark that evaluates the situated reasoning ability via situation abstraction and logic-grounded question answering for real-world videos, called Situated Reasoning in Real-World Videos (STAR). This benchmark is built upon the real-world videos associated with human actions or interactions, which are naturally dynamic, compositional, and logical. The dataset includes four types of questions, including interaction, sequence, prediction, and feasibility. We represent the situations in real-world videos by hyper-graphs connecting extracted atomic entities and relations (e.g., actions, persons, objects, and relationships). Besides visual perception, situated reasoning also requires structured situation comprehension and logical reasoning. Questions and answers are procedurally generated. The answering logic of each question is represented by a functional program based on a situation hyper-graph. We compare various existing video reasoning models and find that they all struggle on this challenging situated reasoning task. We further propose a diagnostic neuro-symbolic model that can disentangle visual perception, situation abstraction, language understanding, and functional reasoning to understand the challenges of this benchmark. </span>'}
{'title': 'Implicitly learning to reason in first-order logic', 'authors': ['Brendan Juba'], 'Conference': 'NeurIPS 2019', 'date': '06 Sept 2019 (modified: 05 May 2023)', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=BkMOD4Bg8S&name=pdf', 'abstract': '</span><span class="note_content_value">We consider the problem of answering queries about formulas of first-order logic based on background knowledge partially represented explicitly as other formulas, and partially represented as examples independently drawn from a fixed probability distribution. PAC semantics, introduced by Valiant, is one rigorous, general proposal for learning to reason in formal languages: although weaker than classical entailment, it allows for a powerful model theoretic framework for answering queries while requiring minimal assumptions about the form of the distribution in question. To date, however, the most significant limitation of that approach, and more generally most machine learning approaches with robustness guarantees, is that the logical language is ultimately essentially propositional, with finitely many atoms. Indeed, the theoretical findings on the learning of relational theories in such generality have been resoundingly negative. This is despite the fact that first-order logic is widely argued to be most appropriate for representing human knowledge.  In this work, we present a new theoretical approach to robustly learning to reason in first-order logic, and consider universally quantified clauses over a countably infinite domain. Our results exploit symmetries exhibited by constants in the language, and generalize the notion of implicit learnability to show how queries can be computed against (implicitly) learned first-order background knowledge.  </span>'}
{'title': 'Learning Rules with Stratified Negation in Differentiable ILP.', 'authors': ['Giri P Krishnan', 'Frederick Maier', 'Ramyaa Ramyaa'], 'Conference': 'AIPLANS', 'date': 'Published: 23 Oct 2021, Last Modified: 05 May 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=BOtQHCVIh_K&name=pdf', 'abstract': '</span><span class="note_content_value">Differentiable methods to learn first order rules (logic programs) have the potential to integrate the interpretability, transferability and low data requirements of inductive logic programming with the noise tolerance of non-symbolic learning.Negation is an essential component of reasoning, but incorporating it into logic programming frameworks poses several problems (hence its central place in the logic programming and nonmonotonic reasoning communities). Current implementations of differentiable rule learners do not learn rules with negations. Here,we introduce stratified negation into a differentiable inductive logic programming framework, and we demonstrate that the resulting system can learn recursive pro-grams with inventive predicates in which negation plays a central role. We include examples from multiple domains, e.g., arithmetic, graph, sets and lists.</span>'}
{'title': 'Probabilistic Logic Neural Networks for Reasoning', 'authors': ['Meng Qu'], 'Conference': 'NeurIPS 2019', 'date': '06 Sept 2019 (modified: 05 May 2023)', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=HJx22VrxIB&name=pdf', 'abstract': '</span><span class="note_content_value">Knowledge graph reasoning, which aims at predicting missing facts through reasoning with observed facts, is critical for many applications. Such a problem has been widely explored by traditional logic rule-based approaches and recent knowledge graph embedding methods. A principled logic rule-based approach is the Markov Logic Network (MLN), which is able to leverage domain knowledge with first-order logic and meanwhile handle uncertainty. However, the inference of MLNs is usually very difficult due to the complicated graph structures. Different from MLNs, knowledge graph embedding methods (e.g. TransE, DistMult) learn effective entity and relation embeddings for reasoning, which are much more effective and efficient. However, they are unable to leverage domain knowledge. In this paper, we propose the probabilistic Logic Neural Network (pLogicNet), which combines the advantages of both methods. A pLogicNet defines the joint distribution of all possible triplets by using a Markov logic network with first-order logic, which can be efficiently optimized with the variational EM algorithm. Specifically, in the E-step, a knowledge graph embedding model is used for inferring the missing triplets, while in the M-step, the weights of the logic rules are updated according to both the observed and predicted triplets. Experiments on multiple knowledge graphs prove the effectiveness of pLogicNet over many competitive baselines. </span>'}
{'title': 'Hyperbolic Embedding Inference for Structured Multi-Label Prediction', 'authors': ['Bo Xiong', 'Michael Cochez', 'Mojtaba Nayyeri', 'Steffen Staab'], 'Conference': 'NeurIPS 2022 Accept', 'date': 'Published: 31 Oct 2022, Last Modified: 11 Jan 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=XFnDhcEH9FF&name=pdf', 'abstract': '</span><span class="note_content_value">We consider a structured multi-label prediction problem where the labels are organized under implication and mutual exclusion constraints. A major concern is to produce predictions that are logically consistent with these constraints. To do so, we formulate this problem as an embedding inference problem where the constraints are imposed onto the embeddings of labels by geometric construction. Particularly, we consider a hyperbolic Poincaré ball model in which we encode labels as Poincaré hyperplanes that work as linear decision boundaries. The hyperplanes are interpreted as convex regions such that the logical relationships (implication and exclusion) are geometrically encoded using the insideness and disjointedness of these regions, respectively. We show theoretical groundings of the method for preserving logical relationships in the embedding space. Extensive experiments on 12 datasets show 1) significant improvements in mean average precision; 2) lower number of constraint violations;  3) an order of magnitude fewer dimensions than baselines.</span>'}
{'title': 'Probabilistic Entity Representation Model for Reasoning over Knowledge Graphs', 'authors': ['Nurendra Choudhary', 'Nikhil Rao', 'Sumeet Katariya', 'Karthik Subbian', 'Chandan K. Reddy'], 'Conference': 'NeurIPS 2021 Poster', 'date': 'Published: 09 Nov 2021, Last Modified: 05 May 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=AREHCsLy9oc&name=pdf', 'abstract': '</span><span class="note_content_value">Logical reasoning over Knowledge Graphs (KGs) is a fundamental technique that can provide an efficient querying mechanism over large and incomplete databases. Current approaches employ spatial geometries such as boxes to learn query representations that encompass the answer entities and model the logical operations of projection and intersection. However, their geometry is restrictive and leads to non-smooth strict boundaries, which further results in ambiguous answer entities. Furthermore, previous works propose transformation tricks to handle unions which results in non-closure and, thus, cannot be chained in a stream. In this paper, we propose a Probabilistic Entity Representation Model (PERM) to encode entities as a Multivariate Gaussian density with mean and covariance parameters to capture its semantic position and smooth decision boundary, respectively. Additionally, we also define the closed logical operations of projection, intersection, and union that can be aggregated using an end-to-end objective function. On the logical query reasoning problem, we demonstrate that the proposed PERM significantly outperforms the state-of-the-art methods on various public benchmark KG datasets on standard evaluation metrics. We also evaluate PERM’s competence on a COVID-19 drug-repurposing case study and show that our proposed work is able to recommend drugs with substantially better F1 than current methods. Finally, we demonstrate the working of our PERM’s query answering process through a low-dimensional visualization of the Gaussian representations.</span>'}
{'title': 'Probabilistic Entity Representation Model for Reasoning over Knowledge Graphs', 'authors': ['Nurendra Choudhary', 'Nikhil Rao', 'Sumeet Katariya', 'Karthik Subbian', 'Chandan K. Reddy'], 'Conference': 'NeurIPS 2021 Poster', 'date': 'Published: 09 Nov 2021, Last Modified: 05 May 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=ACV8iBHtbR&name=pdf', 'abstract': '</span><span class="note_content_value">Logical reasoning over Knowledge Graphs (KGs) is a fundamental technique that can provide an efficient querying mechanism over large and incomplete databases. Current approaches employ spatial geometries such as boxes to learn query representations that encompass the answer entities and model the logical operations of projection and intersection. However, their geometry is restrictive and leads to non-smooth strict boundaries, which further results in ambiguous answer entities. Furthermore, previous works propose transformation tricks to handle unions which results in non-closure and, thus, cannot be chained in a stream. In this paper, we propose a Probabilistic Entity Representation Model (PERM) to encode entities as a Multivariate Gaussian density with mean and covariance parameters to capture its semantic position and smooth decision boundary, respectively. Additionally, we also define the closed logical operations of projection, intersection, and union that can be aggregated using an end-to-end objective function. On the logical query reasoning problem, we demonstrate that the proposed PERM significantly outperforms the state-of-the-art methods on various public benchmark KG datasets on standard evaluation metrics. We also evaluate PERM’s competence on a COVID-19 drug-repurposing case study and show that our proposed work is able to recommend drugs with substantially better F1 than current methods. Finally, we demonstrate the working of our PERM’s query answering process through a low-dimensional visualization of the Gaussian representations.</span>'}
{'title': 'Learning to Reason with Neural Networks: Generalization, Unseen Data and Boolean Measures', 'authors': ['Emmanuel Abbe', 'Samy Bengio', 'Elisabetta Cornacchia', 'Jon Kleinberg', 'Aryo Lotfi', 'Maithra Raghu', 'Chiyuan Zhang'], 'Conference': 'NeurIPS 2022 Accept', 'date': 'Published: 31 Oct 2022, Last Modified: 11 Jan 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=hT0RbC2jCYZ&name=pdf', 'abstract': '</span><span class="note_content_value">This paper considers the Pointer Value Retrieval (PVR) benchmark introduced in [ZRKB21], where a `reasoning\' function acts on a string of digits to produce the label. More generally, the paper considers the learning of logical functions with gradient descent (GD) on neural networks. It is first shown that in order to learn logical functions with gradient descent on symmetric neural networks, the generalization error can be lower-bounded in terms of the noise-stability of the target function, supporting a conjecture made in [ZRKB21]. It is then shown that in the distribution shift setting, when the data withholding corresponds to freezing a single feature (referred to as canonical holdout), the generalization error of gradient descent admits a tight characterization in terms of the Boolean influence for several relevant architectures. This is shown on linear models and supported experimentally on other models such as MLPs and Transformers. In particular, this puts forward the hypothesis that for such architectures and for learning logical functions such as PVR functions, GD tends to have an implicit bias towards low-degree representations, which in turn gives the Boolean influence for the generalization error under quadratic loss.</span>'}
{'title': 'GALOIS: Boosting Deep Reinforcement Learning via Generalizable Logic Synthesis', 'authors': ['Yushi Cao', 'Zhiming Li', 'Tianpei Yang', 'Hao Zhang', 'YAN ZHENG', 'Yi Li', 'Jianye HAO', 'Yang Liu'], 'Conference': 'NeurIPS 2022 Accept', 'date': 'Published: 31 Oct 2022, Last Modified: 12 Oct 2022', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=XSV1T9jMuz9&name=pdf', 'abstract': '</span><span class="note_content_value">Despite achieving superior performance in human-level control problems, unlike humans, deep reinforcement learning (DRL) lacks high-order intelligence (e.g., logic deduction and reuse), thus it behaves ineffectively than humans regarding learning and generalization in complex problems. Previous works attempt to directly synthesize a white-box logic program as the DRL policy, manifesting logic-driven behaviors. However, most synthesis methods are built on imperative or declarative programming, and each has a distinct limitation, respectively. The former ignores the cause-effect logic during synthesis, resulting in low generalizability across tasks. The latter is strictly proof-based, thus failing to synthesize programs with complex hierarchical logic. In this paper, we combine the above two paradigms together and propose a novel Generalizable Logic Synthesis (GALOIS) framework to synthesize hierarchical and strict cause-effect logic programs. GALOIS leverages the program sketch and defines a new sketch-based hybrid program language for guiding the synthesis. Based on that, GALOIS proposes a sketch-based program synthesis method to automatically generate white-box programs with generalizable and interpretable cause-effect logic. Extensive evaluations on various decision-making tasks with complex logic demonstrate the superiority of GALOIS over mainstream baselines regarding the asymptotic performance, generalizability, and great knowledge reusability across different environments.</span>'}
{'title': 'Quantum Embedding of Knowledge for Reasoning', 'authors': ['Dinesh Garg'], 'Conference': 'NeurIPS 2019', 'date': '06 Sept 2019 (modified: 05 May 2023)', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=H1lQqVHeLH&name=pdf', 'abstract': '</span><span class="note_content_value">Statistical Relational Learning (SRL) methods are the most widely used techniques to generate distributional representations of the symbolic Knowledge Bases (KBs). These methods embed any given KB into a vector space by exploiting statistical similarities among its entities and predicates but without any guarantee of preserving the underlying logical structure of the KB. This, in turn, results in poor performance of logical reasoning tasks that are solved using such distributional representations. We present a novel approach called Embed2Reason (E2R) that embeds a symbolic KB into a vector space in a logical structure preserving manner. This approach is inspired by the theory of Quantum Logic. Such an embedding allows answering membership based complex logical reasoning queries with impressive accuracy improvements over popular SRL baselines.</span>'}
{'title': 'ConE: Cone Embeddings for Multi-Hop Reasoning over Knowledge Graphs', 'authors': ['Zhanqiu Zhang', 'Jie Wang', 'Jiajun Chen', 'Shuiwang Ji', 'Feng Wu'], 'Conference': 'NeurIPS 2021 Poster', 'date': 'Published: 09 Nov 2021, Last Modified: 16 Sept 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=Twf_XYunk5j&name=pdf', 'abstract': '</span><span class="note_content_value">Query embedding (QE)---which aims to embed entities and first-order logical (FOL) queries in low-dimensional spaces---has shown great power in multi-hop reasoning over knowledge graphs. Recently, embedding entities and queries with geometric shapes becomes a promising direction, as geometric shapes can naturally represent answer sets of queries and logical relationships among them. However, existing geometry-based models have difficulty in modeling queries with negation, which significantly limits their applicability. To address this challenge, we propose a novel query embedding model, namely \\textbf{Con}e \\textbf{E}mbeddings (ConE), which is the first geometry-based QE model that can handle all the FOL operations, including conjunction, disjunction, and negation. Specifically, ConE represents entities and queries as Cartesian products of two-dimensional cones, where the intersection and union of cones naturally model the conjunction and disjunction operations. By further noticing that the closure of complement of cones remains cones, we design geometric complement operators in the embedding space for the negation operations. Experiments demonstrate that ConE significantly outperforms existing state-of-the-art methods on benchmark datasets.</span>'}
{'title': 'Convex Polytope Trees', 'authors': ['Mohammadreza Armandpour', 'Ali Sadeghian', 'Mingyuan Zhou'], 'Conference': 'NeurIPS 2021 Poster', 'date': 'Published: 09 Nov 2021, Last Modified: 16 Sept 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=MvGKpmPsN7c&name=pdf', 'abstract': '</span><span class="note_content_value">A decision tree is commonly restricted to use a single hyperplane to split the covariate space at each of its internal nodes. It often requires a large number of nodes to achieve high accuracy. In this paper, we propose convex polytope trees (CPT) to expand the family of decision trees by an interpretable generalization of their decision boundary. The splitting function at each node of CPT is based on the logical disjunction of a community of differently weighted probabilistic linear decision-makers, which also geometrically corresponds to a convex polytope in the covariate space. We use a nonparametric Bayesian prior at each node to infer the community\'s size, encouraging simpler decision boundaries by shrinking the number of polytope facets. We develop a greedy method to efficiently construct CPT and scalable end-to-end training algorithms for the tree parameters when the tree structure is given. We empirically demonstrate the efficiency of CPT over existing state-of-the-art decision trees in several real-world classification and regression tasks from diverse domains.</span>'}
{'title': 'Compositional Reinforcement Learning from Logical Specifications', 'authors': ['Kishor Jothimurugan', 'Suguman Bansal', 'Osbert Bastani', 'Rajeev Alur'], 'Conference': 'NeurIPS 2021 Poster', 'date': 'Published: 09 Nov 2021, Last Modified: 16 Sept 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=ion6Lo5tKtJ&name=pdf', 'abstract': '</span><span class="note_content_value">We study the problem of learning control policies for complex tasks given by logical specifications. Recent approaches automatically generate a reward function from a given specification and use a suitable reinforcement learning algorithm to learn a policy that maximizes the expected reward. These approaches, however, scale poorly to complex tasks that require high-level planning. In this work, we develop a compositional learning approach, called DIRL, that interleaves high-level planning and reinforcement learning. First, DIRL encodes the specification as an abstract graph; intuitively, vertices and edges of the graph correspond to regions of the state space and simpler sub-tasks, respectively. Our approach then incorporates reinforcement learning to learn neural network policies for each edge (sub-task) within a Dijkstra-style planning algorithm to compute a high-level plan in the graph. An evaluation of the proposed approach on a set of challenging control benchmarks with continuous state and action spaces demonstrates that it outperforms state-of-the-art baselines.</span>'}
{'title': 'Versatile Multi-stage Graph Neural Network for Circuit Representation', 'authors': ['Shuwen Yang', 'Zhihao Yang', 'Dong Li', 'Yingxue Zhang', 'Zhanguang Zhang', 'Guojie Song', 'Jianye HAO'], 'Conference': 'NeurIPS 2022 Accept', 'date': 'Published: 31 Oct 2022, Last Modified: 15 Dec 2022', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=nax3ATLrovW&name=pdf', 'abstract': '</span><span class="note_content_value">Due to the rapid growth in the scale of circuits and the desire for knowledge transfer from old designs to new ones, deep learning technologies have been widely exploited in Electronic Design Automation (EDA) to assist circuit design. In chip design cycles, we might encounter heterogeneous and diverse information sources, including the two most informative ones: the netlist and the design layout. However, handling each information source independently is sub-optimal. In this paper, we propose a novel way to integrate the multiple information sources under a unified heterogeneous graph named Circuit Graph, where topological and geometrical information is well integrated. Then, we propose Circuit GNN to fully utilize the features of vertices, edges as well as heterogeneous information during the message passing process. It is the first attempt to design a versatile circuit representation that is compatible across multiple EDA tasks and stages. Experiments on the two most representative prediction tasks in EDA show that our solution reaches state-of-the-art performance in both logic synthesis and global placement chip design stages. Besides, it achieves a 10x speed-up on congestion prediction compared to the state-of-the-art model.</span>'}
{'title': 'Hyperparameter Optimization Is Deceiving Us, and How to Stop It', 'authors': ['A. Feder Cooper', 'Yucheng Lu', 'Jessica Zosa Forde', 'Christopher De Sa'], 'Conference': 'NeurIPS 2021 Poster', 'date': 'Published: 09 Nov 2021, Last Modified: 16 Sept 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=2lZdja9xYzh&name=pdf', 'abstract': '</span><span class="note_content_value">Recent empirical work shows that inconsistent results based on choice of hyperparameter optimization (HPO) configuration are a widespread problem in ML research. When comparing two algorithms J and K searching one subspace can yield the conclusion that J outperforms K, whereas searching another can entail the opposite. In short, the way we choose hyperparameters can deceive us. We provide a theoretical complement to this prior work, arguing that, to avoid such deception, the process of drawing conclusions from HPO should be made more rigorous. We call this process epistemic hyperparameter optimization (EHPO), and put forth a logical framework to capture its semantics and how it can lead to inconsistent conclusions about performance. Our framework enables us to prove EHPO methods that are guaranteed to be defended against deception, given bounded compute time budget t. We demonstrate our framework\'s utility by proving and empirically validating a defended variant of random search. </span>'}
{'title': 'Embedding Symbolic Knowledge into Deep Networks', 'authors': ['Xie Yaqi'], 'Conference': 'NeurIPS 2019', 'date': '06 Sept 2019 (modified: 05 May 2023)', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=HJltOErlUS&name=pdf', 'abstract': '</span><span class="note_content_value">In this work, we aim to utilize prior knowledge encoded as logical rules to improve the performance of deep models. We propose a logic graph embedding network that projects d-DNNF formulae (and assignments) onto a manifold via an augmented Graph Convolutional Network (GCN). To generate semantically-faithful embeddings, we propose techniques to recognize node heterogeneity, and semantic regularization that incorporate structural constraints into the embedding. Experiments show that our approach improves the performance of models trained to perform model-checking and visual relation prediction.</span>'}
{'title': 'Policy Optimization with Linear Temporal Logic Constraints', 'authors': ['Cameron Voloshin', 'Hoang Minh Le', 'Swarat Chaudhuri', 'Yisong Yue'], 'Conference': 'NeurIPS 2022 Accept', 'date': 'Published: 31 Oct 2022, Last Modified: 12 Oct 2022', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=yZcPRIZEwOG&name=pdf', 'abstract': '</span><span class="note_content_value">We study the problem of policy optimization (PO) with linear temporal logic (LTL) constraints. The language of LTL allows flexible description of tasks that may be unnatural to encode as a scalar cost function. We consider LTL-constrained PO as a systematic framework, decoupling task specification from policy selection, and an alternative to the standard of cost shaping. With access to a generative model, we develop a model-based approach that enjoys a sample complexity analysis for guaranteeing both task satisfaction and cost optimality (through a reduction to a reachability problem). Empirically, our algorithm can achieve strong  performance even in low sample regimes.</span>'}
{'title': 'Class Expression Learning with Permutation-Invariant Embeddings', 'authors': [], 'Conference': 'Submitted to nCSI WS @ NeurIPS 2022', 'date': '04 Oct 2022 (modified: 05 May 2023)', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=ALdGMqCrS_r&name=pdf', 'abstract': '</span><span class="note_content_value">Class expression learning deals with learning description logic concepts from an RDF knowledge base and input examples. \nThe goal is to learn a concept that covers \nall positive examples, while not covering any negative examples.\nAlthough state-of-the-art models have been successfully applied to tackle this problem, their large-scale applications have been severely hindered due to their impractical runtimes.\nArguably, this limitation stems from their needs for exploring numerous expressions.\nHere, we investigate a remedy for this limitation.\nWe formulate the class expression learning problem as a multi-label classification problem and we propose a permutation-invariant embedding model (Nero) to reduce the rate of exploration.\nFor a given learning problem, Nero accurately predicts quality of pre-selected description logic concepts for a given input example sets.\nThrough ranking concepts in descending order of predicted qualities, the standard search procedure can start in multiple advantageous regions of the quasi-ordered search space.\nOur experiments on 5 benchmark datasets with 770 learning problems suggest that using \\approach led to significant improvements (p-value &lt;1\\%) in the number of explored expressions and the total runtime time. </span>'}
{'title': 'SQALER: Scaling Question Answering by Decoupling Multi-Hop and Logical Reasoning', 'authors': ['Mattia Atzeni', 'Jasmina Bogojeska', 'Andreas Loukas'], 'Conference': 'NeurIPS 2021 Poster', 'date': 'Published: 09 Nov 2021, Last Modified: 05 May 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=2CQQ_C1i0b&name=pdf', 'abstract': '</span><span class="note_content_value">State-of-the-art approaches to reasoning and question answering over knowledge graphs (KGs) usually scale with the number of edges and can only be applied effectively on small instance-dependent subgraphs. In this paper, we address this issue by showing that multi-hop and more complex logical reasoning can be accomplished separately without losing expressive power. Motivated by this insight, we propose an approach to multi-hop reasoning that scales linearly with the number of relation types in the graph, which is usually significantly smaller than the number of edges or nodes. This produces a set of candidate solutions that can be provably refined to recover the solution to the original problem. Our experiments on knowledge-based question answering show that our approach solves the multi-hop MetaQA dataset, achieves a new state-of-the-art on the more challenging WebQuestionsSP, is orders of magnitude more scalable than competitive approaches, and can achieve compositional generalization out of the training distribution.</span>'}
{'title': 'Scallop: From Probabilistic Deductive Databases to Scalable Differentiable Reasoning', 'authors': ['Jiani Huang', 'Ziyang Li', 'Binghong Chen', 'Karan Samel', 'Mayur Naik', 'Le Song', 'Xujie Si'], 'Conference': 'AIPLANS', 'date': 'Published: 23 Oct 2021, Last Modified: 05 May 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=qey0t9ivuBv&name=pdf', 'abstract': '</span><span class="note_content_value">Deep learning and symbolic reasoning are complementary techniques for an intelligent system. However, principled combinations of these techniques are typically limited in scalability, rendering them ill-suited for real-world applications. We propose Scallop, a system that builds upon probabilistic deductive databases, to bridge this gap. On synthetic tasks involving mathematical and logical reasoning, Scallop scales significantly better without sacrificing accuracy compared to DeepProbLog, a principled neural logic programming approach. Scallop also scales to a real-world Visual Question Answering (VQA) benchmark that requires multi-hop reasoning, achieving 84.22% accuracy and outperforming two VQA-tailored models based on Neural Module Networks and transformers by 12.42% and 21.66% respectively.</span>'}
{'title': 'Tsetlin Machine for Solving Contextual Bandit Problems', 'authors': ['Raihan Seraj', 'Jivitesh Sharma', 'Ole-Christoffer Granmo'], 'Conference': 'NeurIPS 2022 Accept', 'date': 'Published: 31 Oct 2022, Last Modified: 16 Sept 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=b-WnRS7kSEN&name=pdf', 'abstract': '</span><span class="note_content_value">This paper introduces an interpretable contextual bandit algorithm using Tsetlin Machines, which solves complex pattern recognition tasks using  propositional (Boolean) logic. The proposed bandit learning algorithm relies on straightforward bit manipulation, thus simplifying computation and interpretation. We then present a mechanism for performing Thompson sampling with Tsetlin Machine, given its non-parametric nature. Our empirical analysis shows that Tsetlin Machine as a base contextual bandit learner outperforms other popular base learners on eight out of nine datasets. We further analyze the interpretability of our learner, investigating how arms are selected based on propositional expressions that model the context.</span>'}
{'title': 'Benchmarking the Combinatorial Generalizability of Complex Query Answering on Knowledge Graphs', 'authors': ['Zihao Wang', 'Hang Yin', 'Yangqiu Song'], 'Conference': 'NeurIPS 2021 Datasets and Benchmarks Track (Round 2)', 'date': 'Published: 11 Oct 2021, Last Modified: 16 Sept 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=pX4x8f6Km5T&name=pdf', 'abstract': '</span><span class="note_content_value">Complex Query Answering (CQA) is an important reasoning task on knowledge graphs. Current CQA learning models have been shown to be able to generalize from atomic operators to more complex formulas, which can be regarded as the combinatorial generalizability. In this paper, we present EFO-1-QA, a new dataset to benchmark the combinatorial generalizability of CQA models by including 301 different queries types, which is 20 times larger than existing datasets. Besides, our benchmark, for the first time, provide a benchmark to evaluate and analyze the impact of different operators and normal forms by using (a) 7 choices of the operator systems and (b) 9 forms of complex queries. Specifically, we provide the detailed study of the combinatorial generalizability of two commonly used operators, i.e., projection and intersection, and justify the impact of the forms of queries given the canonical choice of operators. Our code and data can provide an effective pipeline to benchmark CQA models.</span>'}
{'title': 'Systematic Generalization with Edge Transformers', 'authors': ['Leon Bergen', "Timothy J. O'Donnell", 'Dzmitry Bahdanau'], 'Conference': 'NeurIPS 2021 Poster', 'date': 'Published: 09 Nov 2021, Last Modified: 16 Sept 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=UUds0Jr_XWk&name=pdf', 'abstract': '</span><span class="note_content_value">Recent research suggests that systematic generalization in natural language understanding remains a challenge for state-of-the-art neural models such as Transformers and Graph Neural Networks. To tackle this challenge, we propose Edge Transformer, a new model that combines inspiration from Transformers and rule-based symbolic AI. The first key idea in Edge Transformers is to associate vector states with every edge, that is, with every pair of input nodes---as opposed to just every node, as it is done in the Transformer model. The second major innovation is a triangular attention mechanism that updates edge representations in a way that is inspired by unification from logic programming. We evaluate Edge Transformer on compositional generalization benchmarks in relational reasoning, semantic parsing, and dependency parsing. In all three settings, the Edge Transformer outperforms Relation-aware, Universal and classical Transformer baselines.</span>'}
{'title': 'Retrosynthesis Prediction with Conditional Graph Logic Network', 'authors': ['Hanjun Dai'], 'Conference': 'NeurIPS 2019', 'date': '06 Sept 2019 (modified: 05 May 2023)', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=rygzANSlLr&name=pdf', 'abstract': '</span><span class="note_content_value">Retrosynthesis is one of the fundamental problems in organic chemistry. The task is to identify reactants that can be used to synthesize a specified product molecule. Recently, computer-aided retrosynthesis is finding renewed interest from both chemistry and computer science communities. Most existing approaches rely on template-based models that define subgraph matching rules, but whether or not a chemical reaction can proceed is not defined by hard decision rules. In this work, we propose a new approach to this task using the Conditional Graph Logic Network, a conditional graphical model built upon graph neural networks that learns when rules from reaction templates should be applied, implicitly considering whether the resulting reaction would be both chemically feasible and strategic. We also propose an efficient hierarchical sampling to alleviate the computation cost. While achieving a significant improvement of 8.2% over current state-of-the-art methods on the benchmark dataset, our model also offers interpretations for the prediction. </span>'}
{'title': 'Fast Abductive Learning by Similarity-based Consistency Optimization', 'authors': ['Yu-Xuan Huang', 'Wang-Zhou Dai', 'Le-Wen Cai', 'Stephen Muggleton', 'Yuan Jiang'], 'Conference': 'NeurIPS 2021 Poster', 'date': 'Published: 09 Nov 2021, Last Modified: 05 May 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=UMrf6F4Tg9c&name=pdf', 'abstract': '</span><span class="note_content_value">To utilize the raw inputs and symbolic knowledge simultaneously, some recent neuro-symbolic learning methods use abduction, i.e., abductive reasoning, to integrate sub-symbolic perception and logical inference. While the perception model, e.g., a neural network, outputs some facts that are inconsistent with the symbolic background knowledge base, abduction can help revise the incorrect perceived facts by minimizing the inconsistency between them and the background knowledge. However, to enable effective abduction, previous approaches need an initialized perception model that discriminates the input raw instances. This limits the application of these methods, as the discrimination ability is usually acquired from a thorough pre-training when the raw inputs are difficult to classify. In this paper, we propose a novel abduction strategy, which leverages the similarity between samples, rather than the output information by the perceptual neural network, to guide the search in abduction. Based on this principle, we further present ABductive Learning with Similarity (ABLSim) and apply it to some difficult neuro-symbolic learning tasks. Experiments show that the efficiency of ABLSim is significantly higher than the state-of-the-art neuro-symbolic methods, allowing it to achieve better performance with less labeled data and weaker domain knowledge.</span>'}
{'title': 'EuclidNets: Combining hardware and architecture design for Efficient Inference and Training', 'authors': ['Mariana Oliveira Prazeres', 'Xinlin Li', 'Vahid Partovi Nia', 'Adam M Oberman'], 'Conference': 'NeurIPS 2021 Submitted', 'date': '21 May 2021 (modified: 05 May 2023)', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=mW7M0QsDbcw&name=pdf', 'abstract': '</span><span class="note_content_value">In order to deploy deep neural networks on edge devices, compressed (resource efficient) networks need to be developed.  While established compression methods, such as quantization, pruning, and architecture search are designed for conventional hardware,  further gains are possible if compressed architectures are coupled with novel hardware designs.   In this work, we propose EuclidNet, a compressed network designed to be implemented on hardware which replaces multiplication, <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="0" style="font-size: 121.3%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D464 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>w</mi><mi>x</mi></math></mjx-assistive-mml></mjx-container>, with squared difference <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="1" style="font-size: 121.3%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="3"><mjx-c class="mjx-c1D464 TEX-I"></mjx-c></mjx-mi><mjx-msup><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-script style="vertical-align: 0.363em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mi>x</mi><mo>−</mo><mi>w</mi><msup><mo stretchy="false">)</mo><mn>2</mn></msup></math></mjx-assistive-mml></mjx-container>.  EuclidNet allows for a low precision hardware implementation which is about twice as efficient (in term of logic gate counts) as the comparable conventional hardware, with acceptably small loss of accuracy.   Moveover, the network can be trained and quantized using standard methods, without requiring additional training time.  Codes and pre-trained models are available at \\url{<a href="http://github.com/anonymous/}" target="_blank" rel="nofollow">http://github.com/anonymous/}</a>.</span>'}
{'title': 'New Definitions and Evaluations for Saliency Methods: Staying Intrinsic, Complete and Sound', 'authors': ['Arushi Gupta', 'Nikunj Saunshi', 'Dingli Yu', 'Kaifeng Lyu', 'Sanjeev Arora'], 'Conference': 'NeurIPS 2022 Accept', 'date': 'Published: 31 Oct 2022, Last Modified: 13 Jan 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=opw858PBJl6&name=pdf', 'abstract': '</span><span class="note_content_value">Saliency methods compute heat maps that highlight portions of an input that were most important for the label assigned to it by a deep net. Evaluations of saliency methods convert this heat map into a new masked input by retaining the <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="0" style="font-size: 121.3%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></mjx-assistive-mml></mjx-container> highest-ranked pixels of the original input and replacing the rest with "uninformative" pixels, and checking if the net\'s output is mostly unchanged. This is usually seen as an explanation of the output, but the current paper highlights reasons why this inference of causality may be suspect. Inspired by logic concepts of completeness &amp; soundness, it observes that the above type of evaluation focuses on completeness of the explanation, but ignores soundness.  New evaluation metrics are introduced to capture both notions, while staying in an intrinsic framework---i.e., using the dataset and the net, but no separately trained nets, human evaluations, etc. A simple saliency method is described that matches or outperforms prior methods in the evaluations. Experiments also suggest new intrinsic justifications, based on soundness, for popular heuristic tricks such as TV regularization and upsampling.</span>'}
{'title': 'Neural-Symbolic Entangled Framework for Complex Query Answering', 'authors': ['Zezhong Xu', 'Wen Zhang', 'Peng Ye', 'Hui Chen', 'Huajun Chen'], 'Conference': 'NeurIPS 2022 Accept', 'date': 'Published: 31 Oct 2022, Last Modified: 11 Jan 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=ch5Uth1IGj_&name=pdf', 'abstract': '</span><span class="note_content_value">Answering complex queries over knowledge graphs (KG) is an important yet challenging task because of the KG incompleteness issue and cascading errors during reasoning. Recent query embedding (QE) approaches embed the entities and relations in a KG and the first-order logic (FOL) queries into a low dimensional space, making the query can be answered by dense similarity searching. However, previous works mainly concentrate on the target answers, ignoring intermediate entities\' usefulness, which is essential for relieving the cascading error problem in logical query answering. In addition, these methods are usually designed with their own geometric or distributional embeddings to handle logical operators like union, intersection, and negation, with the sacrifice of the accuracy of the basic operator -- projection, and they could not absorb other embedding methods to their models. In this work, we propose a Neural and Symbolic Entangled framework (ENeSy) for complex query answering, which enables the neural and symbolic reasoning to enhance each other to alleviate the cascading error and KG incompleteness. The projection operator in ENeSy could be any embedding method with the capability of link prediction, and the other FOL operators are handled without parameters. With both neural and symbolic reasoning results contained, ENeSy answers queries in ensembles. We evaluate ENeSy on complex query answering benchmarks, and ENeSy achieves the state-of-the-art, especially in the setting of training model only with the link prediction task.</span>'}
{'title': 'Foundations of Symbolic Languages for Model Interpretability', 'authors': ['Marcelo Arenas', 'Daniel Báez', 'Pablo Barcelo', 'Jorge Pérez', 'Bernardo Subercaseaux'], 'Conference': 'NeurIPS 2021 Spotlight', 'date': 'Published: 09 Nov 2021, Last Modified: 16 Sept 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=Jyxmk4wUoQV&name=pdf', 'abstract': '</span><span class="note_content_value">Several queries and scores have recently been proposed to explain individual predictions over ML models. Examples include queries based on “anchors”, which are parts of an instance that are sufficient to justify its classification, and “feature-perturbation” scores such as SHAP. Given the need for flexible, reliable, and easy-to-apply interpretability methods for ML models, we foresee the need for developing declarative languages to naturally specify different explainability queries. We do this in a principled way by rooting such a language in a logic called FOIL, which allows for expressing many simple but important explainability queries, and might serve as a core for more expressive interpretability languages. We study the computational complexity of FOIL queries over two classes of ML models often deemed to be easily interpretable: decision trees and more general decision diagrams. Since the number of possible inputs for an ML model is exponential in its dimension, tractability of the FOIL evaluation problem is delicate but can be achieved by either restricting the structure of the models, or the fragment of FOIL being evaluated.  We also present a prototype implementation of FOIL wrapped in a high-level declarative language and perform experiments showing that such a language can be used in practice.</span>'}
{'title': 'Learning to Follow Instructions in Text-Based Games', 'authors': ['Mathieu Tuli', 'Andrew C Li', 'Pashootan Vaezipoor', 'Toryn Q. Klassen', 'Scott Sanner', 'Sheila A. McIlraith'], 'Conference': 'NeurIPS 2022 Accept', 'date': 'Published: 31 Oct 2022, Last Modified: 16 Sept 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=StlwkcFsjaZ&name=pdf', 'abstract': '</span><span class="note_content_value">Text-based games present a unique class of sequential decision making problem in which agents interact with a partially observable, simulated environment via actions and observations conveyed through natural language. Such observations typically include instructions that, in a reinforcement learning (RL) setting, can directly or indirectly guide a player towards completing reward-worthy tasks. In this work, we study the ability of RL agents to follow such instructions. We conduct experiments that show that the performance of state-of-the-art text-based game agents is largely unaffected by the presence or absence of such instructions, and that these agents are typically unable to execute tasks to completion. To further study and address the task of instruction following, we equip RL agents with an internal structured representation of natural language instructions in the form of Linear Temporal Logic (LTL), a formal language that is increasingly used for temporally extended reward specification in RL. Our framework both supports and highlights the benefit of understanding the temporal semantics of instructions and in measuring progress towards achievement of such a temporally extended behaviour. Experiments with 500+ games in TextWorld demonstrate the superior performance of our approach.</span>'}
{'title': 'Open Rule Induction', 'authors': ['Wanyun Cui', 'Xingran Chen'], 'Conference': 'NeurIPS 2021 Poster', 'date': 'Published: 09 Nov 2021, Last Modified: 16 Sept 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=MzOB5DAuHR&name=pdf', 'abstract': '</span><span class="note_content_value">Rules have a number of desirable properties. It is easy to understand,  infer new knowledge, and communicate with other inference systems. \nOne weakness of the previous rule induction systems is that they only find rules within a knowledge base (KB) and therefore cannot generalize to more open and complex real-world rules. Recently, the language model (LM)-based rule generation are proposed to enhance the expressive power of the rules.\nIn this paper, we revisit the differences between KB-based rule induction and LM-based rule generation. We argue that, while KB-based methods inducted rules by discovering data commonalitiess, the current LM-based methods are ``learning rules from rules\'\'. This limits these methods to only produce ``canned\'\' rules whose patterns are constrained by the annotated rules, while discarding the rich expressive power of LMs for free text.\n\nTherefore, in this paper, we propose the open rule induction problem, which aims to induce open rules utilizing the knowledge in LMs. Besides, we propose the Orion (\\underline{o}pen \\underline{r}ule \\underline{i}nducti\\underline{on}) system to automatically mine open rules from LMs without supervision of annotated rules. We conducted extensive experiments to verify the quality and quantity of the inducted open rules. Surprisingly, when applying the open rules in downstream tasks (i.e. relation extraction), these automatically inducted rules even outperformed the manually annotated rules.</span>'}
{'title': 'Open Rule Induction', 'authors': ['Wanyun Cui', 'Xingran Chen'], 'Conference': 'NeurIPS 2021 Poster', 'date': 'Published: 09 Nov 2021, Last Modified: 16 Sept 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=Tku-9lhJC5&name=pdf', 'abstract': '</span><span class="note_content_value">Rules have a number of desirable properties. It is easy to understand,  infer new knowledge, and communicate with other inference systems. \nOne weakness of the previous rule induction systems is that they only find rules within a knowledge base (KB) and therefore cannot generalize to more open and complex real-world rules. Recently, the language model (LM)-based rule generation are proposed to enhance the expressive power of the rules.\nIn this paper, we revisit the differences between KB-based rule induction and LM-based rule generation. We argue that, while KB-based methods inducted rules by discovering data commonalitiess, the current LM-based methods are ``learning rules from rules\'\'. This limits these methods to only produce ``canned\'\' rules whose patterns are constrained by the annotated rules, while discarding the rich expressive power of LMs for free text.\n\nTherefore, in this paper, we propose the open rule induction problem, which aims to induce open rules utilizing the knowledge in LMs. Besides, we propose the Orion (\\underline{o}pen \\underline{r}ule \\underline{i}nducti\\underline{on}) system to automatically mine open rules from LMs without supervision of annotated rules. We conducted extensive experiments to verify the quality and quantity of the inducted open rules. Surprisingly, when applying the open rules in downstream tasks (i.e. relation extraction), these automatically inducted rules even outperformed the manually annotated rules.</span>'}
{'title': 'Self-Instantiated Recurrent Units with Dynamic Soft Recursion', 'authors': ['Aston Zhang', 'Yi Tay', 'Yikang Shen', 'Alvin Chan', 'Shuai Zhang'], 'Conference': 'NeurIPS 2021 Poster', 'date': 'Published: 09 Nov 2021, Last Modified: 05 May 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=7Da3azsjjlh&name=pdf', 'abstract': '</span><span class="note_content_value">While standard recurrent neural networks explicitly impose a chain structure on different forms of data, they do not have an explicit bias towards recursive self-instantiation where the extent of recursion is dynamic.  Given diverse and even growing data modalities (e.g., logic, algorithmic input and output, music, code, images, and language) that can be expressed in sequences and may benefit from more architectural flexibility, we propose the self-instantiated recurrent unit (Self-IRU) with a novel inductive bias towards dynamic soft recursion. On one hand, theSelf-IRU is characterized by recursive self-instantiation via its gating functions, i.e., gating mechanisms of the Self-IRU are controlled by instances of the Self-IRU itself, which are repeatedly invoked in a recursive fashion. On the other hand, the extent of the Self-IRU recursion is controlled by gates whose values are between 0 and 1 and may vary across the temporal dimension of sequences,  enabling dynamic soft recursion depth at each time step. The architectural flexibility and effectiveness of our proposed approach are demonstrated across multiple data modalities. For example, the Self-IRU achieves state-of-the-art performance on the logical inference dataset [Bowman et al., 2014] even when comparing with competitive models that have access to ground-truth syntactic information.</span>'}
{'title': 'Improving Coherence and Consistency in Neural Sequence Models with Dual-System, Neuro-Symbolic Reasoning', 'authors': ['Maxwell Nye', 'Michael Henry Tessler', 'Joshua B. Tenenbaum', 'Brenden M. Lake'], 'Conference': 'NeurIPS 2021 Poster', 'date': 'Published: 09 Nov 2021, Last Modified: 05 May 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=P7GUAXxS3ym&name=pdf', 'abstract': '</span><span class="note_content_value">Human reasoning can be understood as an interplay between two systems: the intuitive and associative ("System 1") and the deliberative and logical ("System 2"). Neural sequence models---which have been increasingly successful at performing complex, structured tasks---exhibit the advantages and failure modes of System 1: they are fast and learn patterns from data, but are often inconsistent and incoherent. In this work, we seek a lightweight, training-free means of improving existing System 1-like sequence models by adding System 2-inspired logical reasoning. We explore several variations on this theme in which candidate generations from a neural sequence model are examined for logical consistency by a symbolic reasoning module, which can either accept or reject the generations. Our approach uses neural inference to mediate between the neural System 1 and the logical System 2. Results in robust story generation and grounded instruction-following show that this approach can increase the coherence and accuracy of neurally-based generations.</span>'}
{'title': 'Improving Coherence and Consistency in Neural Sequence Models with Dual-System, Neuro-Symbolic Reasoning', 'authors': ['Maxwell Nye', 'Michael Henry Tessler', 'Joshua B. Tenenbaum', 'Brenden M. Lake'], 'Conference': 'NeurIPS 2021 Poster', 'date': 'Published: 09 Nov 2021, Last Modified: 05 May 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=uyKk_avJ-p4&name=pdf', 'abstract': '</span><span class="note_content_value">Human reasoning can be understood as an interplay between two systems: the intuitive and associative ("System 1") and the deliberative and logical ("System 2"). Neural sequence models---which have been increasingly successful at performing complex, structured tasks---exhibit the advantages and failure modes of System 1: they are fast and learn patterns from data, but are often inconsistent and incoherent. In this work, we seek a lightweight, training-free means of improving existing System 1-like sequence models by adding System 2-inspired logical reasoning. We explore several variations on this theme in which candidate generations from a neural sequence model are examined for logical consistency by a symbolic reasoning module, which can either accept or reject the generations. Our approach uses neural inference to mediate between the neural System 1 and the logical System 2. Results in robust story generation and grounded instruction-following show that this approach can increase the coherence and accuracy of neurally-based generations.</span>'}
{'title': 'Learning Symmetric Rules with SATNet', 'authors': ['Sangho Lim', 'Eun-Gyeol Oh', 'Hongseok Yang'], 'Conference': 'NeurIPS 2022 Accept', 'date': 'Published: 31 Oct 2022, Last Modified: 16 Sept 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=OQtY993Y4TV&name=pdf', 'abstract': '</span><span class="note_content_value">SATNet is a differentiable constraint solver with a custom backpropagation algorithm, which can be used as a layer in a deep-learning system. It is a promising proposal for bridging deep learning and logical reasoning. In fact, SATNet has been successfully applied to learn, among others, the rules of a complex logical puzzle, such as Sudoku, just from input and output pairs where inputs are given as images. In this paper, we show how to improve the learning of SATNet by exploiting symmetries in the target rules of a given but unknown logical puzzle or more generally a logical formula. We present SymSATNet, a variant of SATNet that translates the given symmetries of the target rules to a condition on the parameters of SATNet and requires that the parameters should have a particular parametric form that guarantees the condition. The requirement dramatically reduces the number of parameters to learn for the rules with enough symmetries, and makes the parameter learning of SymSATNet much easier than that of SATNet. We also describe a technique for automatically discovering symmetries of the target rules from examples. Our experiments with Sudoku and Rubik\'s cube show the substantial improvement of SymSATNet over the baseline SATNet.</span>'}
{'title': 'Type Inference as Optimization', 'authors': ['Eirene V. Pandi', 'Earl T. Barr', 'Andrew D. Gordon', 'Charles Sutton'], 'Conference': 'AIPLANS', 'date': 'Published: 23 Oct 2021, Last Modified: 05 May 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=yHYZaQ0Zvml&name=pdf', 'abstract': '</span><span class="note_content_value">Optionally typed dynamic languages can permit multiple valid type assignments. When this happens, developers can prefer one valid type assignment over another because it better reflects how they think about the program and the problem it solves. Natural type inference (NTI) uses natural language text within source code, such as identifiers, to help choose valid programming language types. A growing body of techniques has been proposed for NTI. These techniques predict types; they seek to return natural type assignments (assignments that reflect developer preferences) while striving for correctness. They are empirically effective, but they are not sound by construction: they do not leverage programming language theory to formalize their algorithms and show correctness and termination. Filling this foundational gap is the purpose of this paper. We are the first to present a detailed algorithm for NTI that is validated with theorems and proofs. Valid type assignments obey logical constraints arising from type rules; natural type assignments obey natural constraints arising from the natural language text associated with a variable and its uses.The core intuition of this work is that logical and natural constraints can interact to speed finding a type valuation that 1. type checks (satisfies the logical constraints) and 2. is most natural.We formulate NTI as a joint optimization problem.  To do this, we define a numerical relaxation over boolean logical constraints that give us a condition that we treat as a hard constraint, while simultaneously we minimize distance from natural constraints, which we treat as soft constraints for our optimization problem. Our main result, the first formal proof of soundness for natural type inference, is that our algorithm always terminates, either with an error or with a tuple that is guaranteed to be a type signature for its input.</span>'}
{'title': 'DRUM: End-To-End Differentiable Rule Mining On Knowledge Graphs', 'authors': ['Mohammadreza Armandpour'], 'Conference': 'NeurIPS 2019', 'date': '06 Sept 2019 (modified: 05 May 2023)', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=B1ziaBrxLS&name=pdf', 'abstract': '</span><span class="note_content_value">In this paper, we study the problem of learning probabilistic logical rules for inductive and interpretable link prediction. Despite the importance of inductive link prediction, most previous work focused on transductive link prediction and cannot manage previously unseen entities. Moreover, they are black-box models that are not easily explainable for humans. We propose DRUM, a scalable and differentiable approach for mining first-order logical rules from knowledge graphs which resolves these problems. We motivate our method by making a connection between learning confidence scores for each rule and low-rank tensor approximation. DRUM uses bidirectional RNNs to share useful information across the tasks of learning rules for different relations. We also empirically demonstrate the efficiency of DRUM over existing rule mining methods for inductive link prediction on a variety of benchmark datasets.</span>'}
{'title': 'Transforming Probabilistic Programs into Algebraic Circuits for Inference and Learning', 'authors': ['Pedro Zuidberg Dos Martires', 'Vincent Derkinderen', 'Robin Manhaeve', 'Wannes Meert', 'Angelika Kimmig', 'Luc De Raedt'], 'Conference': 'Program Transformations @NeurIPS2019 Poster', 'date': 'Published: 07 Oct 2019, Last Modified: 05 May 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=SygbjU6iBS&name=pdf', 'abstract': '</span><span class="note_content_value">Probabilistic (logic) programs are routinely compiled into arithmetic circuits. During such a compilation step, the logic representation of a probabilistic program is transformed into an arithmetic representation. We show that this transformation and the resulting circuits cannot only be used for discrete probabilistic inference, but also for a number of other tasks such as differentiation, learning and probabilistic inference in the discrete-continuous domain.</span>'}
{'title': 'Measuring Mathematical Problem Solving With the MATH Dataset', 'authors': ['Dan Hendrycks', 'Collin Burns', 'Saurav Kadavath', 'Akul Arora', 'Steven Basart', 'Eric Tang', 'Dawn Song', 'Jacob Steinhardt'], 'Conference': 'NeurIPS 2021 Datasets and Benchmarks Track (Round 2)', 'date': 'Published: 18 Oct 2021, Last Modified: 16 Sept 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=7Bywt2mQsCe&name=pdf', 'abstract': '</span><span class="note_content_value">Many intellectual endeavors require mathematical problem solving, but this skill remains beyond the capabilities of computers. To measure this ability in machine learning models, we introduce MATH, a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations. To facilitate future research and increase accuracy on MATH, we also contribute a large auxiliary pretraining dataset which helps teach models the fundamentals of mathematics. Even though we are able to increase accuracy on MATH, our results show that accuracy remains relatively low, even with enormous Transformer models. Moreover, we find that simply increasing budgets and model parameter counts will be impractical for achieving strong mathematical reasoning if scaling trends continue. While scaling Transformers is automatically solving most other text-based tasks, scaling is not currently solving MATH. To have more traction on mathematical problem solving we will likely need new algorithmic advancements from the broader research community.</span>'}
{'title': 'The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning', 'authors': ['Xi Ye', 'Greg Durrett'], 'Conference': 'NeurIPS 2022 Accept', 'date': 'Published: 31 Oct 2022, Last Modified: 22 Jan 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=Bct2f8fRd8S&name=pdf', 'abstract': '</span><span class="note_content_value">Does prompting a large language model (LLM) like GPT-3 with explanations improve in-context learning? We study this question on two NLP tasks that involve reasoning over text, namely question answering and natural language inference. We test the performance of four LLMs on three textual reasoning datasets using prompts that include explanations in multiple different styles. For these tasks, we find that including explanations in the prompts for OPT, GPT-3 (davinci), and InstructGPT (text-davinci-001) only yields small to moderate accuracy improvements over standard few-show learning. However, text-davinci-002 is able to benefit more substantially.\n\nWe further show that explanations generated by the LLMs may not entail the models’ predictions nor be factually grounded in the input, even on simple tasks with extractive explanations. However, these flawed explanations can still be useful as a way to verify LLMs’ predictions post-hoc. Through analysis in our three settings, we show that explanations judged by humans to be good—logically consistent with the input and the prediction—more likely cooccur with accurate predictions. Following these observations, we train calibrators using automatically extracted scores that assess the reliability of explanations, allowing us to improve performance post-hoc across all of our datasets.</span>'}
{'title': 'The Impact of Symbolic Representations on In-context Learning for Few-shot Reasoning', 'authors': ['Hanlin Zhang', 'YiFan Zhang', 'Li Erran Li', 'Eric Xing'], 'Conference': 'nCSI WS @ NeurIPS 2022 Poster', 'date': 'Published: 21 Oct 2022, Last Modified: 05 May 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=qLgQpeQX3x1&name=pdf', 'abstract': '</span><span class="note_content_value">Pre-trained language models (LMs) have shown remarkable reasoning performance using explanations (or "chain-of-thought" (CoT)) for in-context learning. On the other hand, those reasoning tasks are usually presumed to be more approachable for symbolic programming. To make progress towards understanding in-context learning, we curate synthetic datasets containing equivalent (natural, symbolic) data pairs, where symbolic examples contain first-order logic rules and predicates from knowledge bases (KBs). Then we revisit neuro-symbolic approaches and design a model LMLP that learns from demonstrations containing logic rules and corresponding examples to iteratively reason over KBs, recovering Prolog\'s backward chaining algorithm. Comprehensive experiments are included to systematically compare LMLP with CoT in deductive and inductive reasoning settings, showing that LMLP enjoys much better length generalization even with substantially less parameters.</span>'}
{'title': 'Predicting Label Distribution from Multi-label Ranking', 'authors': ['Yunan Lu', 'Xiuyi Jia'], 'Conference': 'NeurIPS 2022 Accept', 'date': 'Published: 31 Oct 2022, Last Modified: 14 Dec 2022', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=8wtaJ9dE9Y2&name=pdf', 'abstract': '</span><span class="note_content_value">Label distribution can provide richer information about label polysemy than logical labels in multi-label learning. There are currently two strategies including LDL (label distribution learning) and LE (label enhancement) to predict label distributions. LDL requires experts to annotate instances with label distributions and learn a predictive mapping on such a training set. LE requires experts to annotate instances with logical labels and generates label distributions from them. However, LDL requires costly annotation, and the performance of the LE is unstable. In this paper, we study the problem of predicting label distribution from multi-label ranking which is a compromise w.r.t. annotation cost but has good guarantees for performance. On the one hand, we theoretically investigate the relation between multi-label ranking and label distribution. We define the notion of EAE (expected approximation error) to quantify the quality of an annotation, give the bounds of EAE for multi-label ranking, and derive the optimal range of label distribution corresponding to a particular multi-label ranking. On the other hand, we propose a framework of label distribution predicting from multi-label ranking via conditional Dirichlet mixtures. This framework integrates the processes of recovering and learning label distributions end-to-end and allows us to easily encode our knowledge about current tasks by a scoring function. Finally, we implement extensive experiments to validate our proposal.</span>'}
{'title': 'Learning Division with Neural Arithmetic Logic Modules', 'authors': ['Bhumika Mistry', 'Katayoun Farrahi', 'Jonathon Hare'], 'Conference': 'NeurIPS 2021 Submitted', 'date': '21 May 2021 (modified: 16 Sept 2023)', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=3WbWmdTd8fN&name=pdf', 'abstract': '</span><span class="note_content_value">To achieve systematic generalisation, it first makes sense to master simple tasks such as arithmetic. \nOf the four fundamental arithmetic operations (+,-,<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="0" style="font-size: 121.3%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>×</mo></math></mjx-assistive-mml></mjx-container>,<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="1" style="font-size: 121.3%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-cF7"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>÷</mo></math></mjx-assistive-mml></mjx-container>), division is considered the most difficult for both humans and computers. In this paper we show that robustly learning division in a systematic manner remains a challenge even at the simplest level of dividing two numbers. We propose two novel approaches for division which we call the Neural Reciprocal Unit (NRU) and the Neural Multiplicative Reciprocal Unit (NMRU), and present improvements for an existing division module, the Real Neural Power Unit (Real NPU). Experiments in learning division with input redundancy on 225 different training sets, find that our proposed modifications to the Real NPU obtains an average success of 85.3<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="2" style="font-size: 121.3%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container> improving over the original by 15.1<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="3" style="font-size: 121.3%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container>. In light of the suggestion above, our NMRU approach can further improve the success to 91.6<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="4" style="font-size: 121.3%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container>. </span>'}
{'title': 'Causal Abstractions of Neural Networks', 'authors': ['Atticus Geiger', 'Hanson Lu', 'Thomas F Icard', 'Christopher Potts'], 'Conference': 'NeurIPS 2021 Poster', 'date': 'Published: 09 Nov 2021, Last Modified: 16 Sept 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=RmuXDtjDhG&name=pdf', 'abstract': '="ltr" lang="en"><head>\n  <meta charset="utf-8">\n  <meta name="color-scheme" content="light dark">\n  <meta name="theme-color" content="#fff">\n  <meta name="viewport" content="width=device-width, initial-scale=1.0,\n                                 maximum-scale=1.0, user-scalable=no">\n  <title>openreview.net</title>\n  <style>/* Copyright 2017 The Chromium Authors. All rights reserved.\n * Use of this source code is governed by a BSD-style license that can be\n * found in the LICENSE file. */\n\na {\n  color: var(--link-color);\n}\n\nbody {\n  --background-color: #fff;\n  --error-code-color: var(--google-gray-700);\n  --google-blue-100: rgb(210, 227, 252);\n  --google-blue-300: rgb(138, 180, 248);\n  --google-blue-600: rgb(26, 115, 232);\n  --google-blue-700: rgb(25, 103, 210);\n  --google-gray-100: rgb(241, 243, 244);\n  --google-gray-300: rgb(218, 220, 224);\n  --google-gray-500: rgb(154, 160, 166);\n  --google-gray-50: rgb(248, 249, 250);\n  --google-gray-600: rgb(128, 134, 139);\n  --google-gray-700: rgb(95, 99, 104);\n  --google-gray-800: rgb(60, 64, 67);\n  --google-gray-900: rgb(32, 33, 36);\n  --heading-color: var(--google-gray-900);\n  --link-color: rgb(88, 88, 88);\n  --popup-container-background-color: rgba(0,0,0,.65);\n  --primary-button-fill-color-active: var(--google-blue-700);\n  --primary-button-fill-color: var(--google-blue-600);\n  --primary-button-text-color: #fff;\n  --quiet-background-color: rgb(247, 247, 247);\n  --secondary-button-border-color: var(--google-gray-500);\n  --secondary-button-fill-color: #fff;\n  --secondary-button-hover-border-color: var(--google-gray-600);\n  --secondary-button-hover-fill-color: var(--google-gray-50);\n  --secondary-button-text-color: var(--google-gray-700);\n  --small-link-color: var(--google-gray-700);\n  --text-color: var(--google-gray-700);\n  background: var(--background-color);\n  color: var(--text-color);\n  word-wrap: break-word;\n}\n\n.nav-wrapper .secondary-button {\n  background: var(--secondary-button-fill-color);\n  border: 1px solid var(--secondary-button-border-color);\n  color: var(--secondary-button-text-color);\n  float: none;\n  margin: 0;\n  padding: 8px 16px;\n}\n\n.hidden {\n  display: none;\n}\n\nhtml {\n  -webkit-text-size-adjust: 100%;\n  font-size: 125%;\n}\n\n.icon {\n  background-repeat: no-repeat;\n  background-size: 100%;\n}\n\n@media (prefers-color-scheme: dark) {\n  body {\n    --background-color: var(--google-gray-900);\n    --error-code-color: var(--google-gray-500);\n    --heading-color: var(--google-gray-500);\n    --link-color: var(--google-blue-300);\n    --primary-button-fill-color-active: rgb(129, 162, 208);\n    --primary-button-fill-color: var(--google-blue-300);\n    --primary-button-text-color: var(--google-gray-900);\n    --quiet-background-color: var(--background-color);\n    --secondary-button-border-color: var(--google-gray-700);\n    --secondary-button-fill-color: var(--google-gray-900);\n    --secondary-button-hover-fill-color: rgb(48, 51, 57);\n    --secondary-button-text-color: var(--google-blue-300);\n    --small-link-color: var(--google-blue-300);\n    --text-color: var(--google-gray-500);\n  }\n}\n</style>\n  <style>/* Copyright 2014 The Chromium Authors. All rights reserved.\n   Use of this source code is governed by a BSD-style license that can be\n   found in the LICENSE file. */\n\nbutton {\n  border: 0;\n  border-radius: 4px;\n  box-sizing: border-box;\n  color: var(--primary-button-text-color);\n  cursor: pointer;\n  float: right;\n  font-size: .875em;\n  margin: 0;\n  padding: 8px 16px;\n  transition: box-shadow 150ms cubic-bezier(0.4, 0, 0.2, 1);\n  user-select: none;\n}\n\n[dir=\'rtl\'] button {\n  float: left;\n}\n\n.bad-clock button,\n.captive-portal button,\n.https-only button,\n.insecure-form button,\n.lookalike-url button,\n.main-frame-blocked button,\n.neterror button,\n.pdf button,\n.ssl button,\n.safe-browsing-billing button {\n  background: var(--primary-button-fill-color);\n}\n\nbutton:active {\n  background: var(--primary-button-fill-color-active);\n  outline: 0;\n}\n\n#debugging {\n  display: inline;\n  overflow: auto;\n}\n\n.debugging-content {\n  line-height: 1em;\n  margin-bottom: 0;\n  margin-top: 1em;\n}\n\n.debugging-content-fixed-width {\n  display: block;\n  font-family: monospace;\n  font-size: 1.2em;\n  margin-top: 0.5em;\n}\n\n.debugging-title {\n  font-weight: bold;\n}\n\n#details {\n  margin: 0 0 50px;\n}\n\n#details p:not(:first-of-type) {\n  margin-top: 20px;\n}\n\n.secondary-button:active {\n  border-color: white;\n  box-shadow: 0 1px 2px 0 rgba(60, 64, 67, .3),\n      0 2px 6px 2px rgba(60, 64, 67, .15);\n}\n\n.secondary-button:hover {\n  background: var(--secondary-button-hover-fill-color);\n  border-color: var(--secondary-button-hover-border-color);\n  text-decoration: none;\n}\n\n.error-code {\n  color: var(--error-code-color);\n  font-size: .8em;\n  margin-top: 12px;\n  text-transform: uppercase;\n}\n\n#error-debugging-info {\n  font-size: 0.8em;\n}\n\nh1 {\n  color: var(--heading-color);\n  font-size: 1.6em;\n  font-weight: normal;\n  line-height: 1.25em;\n  margin-bottom: 16px;\n}\n\nh2 {\n  font-size: 1.2em;\n  font-weight: normal;\n}\n\n.icon {\n  height: 72px;\n  margin: 0 0 40px;\n  width: 72px;\n}\n\ninput[type=checkbox] {\n  opacity: 0;\n}\n\ninput[type=checkbox]:focus ~ .checkbox::after {\n  outline: -webkit-focus-ring-color auto 5px;\n}\n\n.interstitial-wrapper {\n  box-sizing: border-box;\n  font-size: 1em;\n  line-height: 1.6em;\n  margin: 14vh auto 0;\n  max-width: 600px;\n  width: 100%;\n}\n\n#main-message > p {\n  display: inline;\n}\n\n#extended-reporting-opt-in {\n  font-size: .875em;\n  margin-top: 32px;\n}\n\n#extended-reporting-opt-in label {\n  display: grid;\n  grid-template-columns: 1.8em 1fr;\n  position: relative;\n}\n\n#enhanced-protection-message {\n  border-radius: 4px;\n  font-size: 1em;\n  margin-top: 32px;\n  padding: 10px 5px;\n}\n\n#enhanced-protection-message label {\n  display: grid;\n  grid-template-columns: 2.5em 1fr;\n  position: relative;\n}\n\n#enhanced-protection-message div {\n  margin: 0.5em;\n}\n\n#enhanced-protection-message .icon {\n  height: 1.5em;\n  vertical-align: middle;\n  width: 1.5em;\n}\n\n.nav-wrapper {\n  margin-top: 51px;\n}\n\n.nav-wrapper::after {\n  clear: both;\n  content: \'\';\n  display: table;\n  width: 100%;\n}\n\n.small-link {\n  color: var(--small-link-color);\n  font-size: .875em;\n}\n\n.checkboxes {\n  flex: 0 0 24px;\n}\n\n.checkbox {\n  --padding: .9em;\n  background: transparent;\n  display: block;\n  height: 1em;\n  left: -1em;\n  padding-inline-start: var(--padding);\n  position: absolute;\n  right: 0;\n  top: -.5em;\n  width: 1em;\n}\n\n.checkbox::after {\n  border: 1px solid white;\n  border-radius: 2px;\n  content: \'\';\n  height: 1em;\n  left: var(--padding);\n  position: absolute;\n  top: var(--padding);\n  width: 1em;\n}\n\n.checkbox::before {\n  background: transparent;\n  border: 2px solid white;\n  border-inline-end-width: 0;\n  border-top-width: 0;\n  content: \'\';\n  height: .2em;\n  left: calc(.3em + var(--padding));\n  opacity: 0;\n  position: absolute;\n  top: calc(.3em  + var(--padding));\n  transform: rotate(-45deg);\n  width: .5em;\n}\n\ninput[type=checkbox]:checked ~ .checkbox::before {\n  opacity: 1;\n}\n\n#recurrent-error-message {\n  background: #ededed;\n  border-radius: 4px;\n  margin-bottom: 16px;\n  margin-top: 12px;\n  padding: 12px 16px;\n}\n\n.showing-recurrent-error-message #extended-reporting-opt-in {\n  margin-top: 16px;\n}\n\n.showing-recurrent-error-message #enhanced-protection-message {\n  margin-top: 16px;\n}\n\n@media (max-width: 700px) {\n  .interstitial-wrapper {\n    padding: 0 10%;\n  }\n\n  #error-debugging-info {\n    overflow: auto;\n  }\n}\n\n@media (max-width: 420px) {\n  button,\n  [dir=\'rtl\'] button,\n  .small-link {\n    float: none;\n    font-size: .825em;\n    font-weight: 500;\n    margin: 0;\n    width: 100%;\n  }\n\n  button {\n    padding: 16px 24px;\n  }\n\n  #details {\n    margin: 20px 0 20px 0;\n  }\n\n  #details p:not(:first-of-type) {\n    margin-top: 10px;\n  }\n\n  .secondary-button:not(.hidden) {\n    display: block;\n    margin-top: 20px;\n    text-align: center;\n    width: 100%;\n  }\n\n  .interstitial-wrapper {\n    padding: 0 5%;\n  }\n\n  #extended-reporting-opt-in {\n    margin-top: 24px;\n  }\n\n  #enhanced-protection-message {\n    margin-top: 24px;\n  }\n\n  .nav-wrapper {\n    margin-top: 30px;\n  }\n}\n\n/**\n * Mobile specific styling.\n * Navigation buttons are anchored to the bottom of the screen.\n * Details message replaces the top content in its own scrollable area.\n */\n\n@media (max-width: 420px) {\n  .nav-wrapper .secondary-button {\n    border: 0;\n    margin: 16px 0 0;\n    margin-inline-end: 0;\n    padding-bottom: 16px;\n    padding-top: 16px;\n  }\n}\n\n/* Fixed nav. */\n@media (min-width: 240px) and (max-width: 420px) and\n       (min-height: 401px),\n       (min-width: 421px) and (min-height: 240px) and\n       (max-height: 560px) {\n  body .nav-wrapper {\n    background: var(--background-color);\n    bottom: 0;\n    box-shadow: 0 -12px 24px var(--background-color);\n    left: 0;\n    margin: 0 auto;\n    max-width: 736px;\n    padding-inline-end: 24px;\n    padding-inline-start: 24px;\n    position: fixed;\n    right: 0;\n    width: 100%;\n    z-index: 2;\n  }\n\n  .interstitial-wrapper {\n    max-width: 736px;\n  }\n\n  #details,\n  #main-content {\n    padding-bottom: 40px;\n  }\n\n  #details {\n    padding-top: 5.5vh;\n  }\n\n  button.small-link {\n    color: var(--google-blue-600);\n  }\n}\n\n@media (max-width: 420px) and (orientation: portrait),\n       (max-height: 560px) {\n  body {\n    margin: 0 auto;\n  }\n\n  button,\n  [dir=\'rtl\'] button,\n  button.small-link,\n  .nav-wrapper .secondary-button {\n    font-family: Roboto-Regular,Helvetica;\n    font-size: .933em;\n    margin: 6px 0;\n    transform: translatez(0);\n  }\n\n  .nav-wrapper {\n    box-sizing: border-box;\n    padding-bottom: 8px;\n    width: 100%;\n  }\n\n  #details {\n    box-sizing: border-box;\n    height: auto;\n    margin: 0;\n    opacity: 1;\n    transition: opacity 250ms cubic-bezier(0.4, 0, 0.2, 1);\n  }\n\n  #details.hidden,\n  #main-content.hidden {\n    height: 0;\n    opacity: 0;\n    overflow: hidden;\n    padding-bottom: 0;\n    transition: none;\n  }\n\n  h1 {\n    font-size: 1.5em;\n    margin-bottom: 8px;\n  }\n\n  .icon {\n    margin-bottom: 5.69vh;\n  }\n\n  .interstitial-wrapper {\n    box-sizing: border-box;\n    margin: 7vh auto 12px;\n    padding: 0 24px;\n    position: relative;\n  }\n\n  .interstitial-wrapper p {\n    font-size: .95em;\n    line-height: 1.61em;\n    margin-top: 8px;\n  }\n\n  #main-content {\n    margin: 0;\n    transition: opacity 100ms cubic-bezier(0.4, 0, 0.2, 1);\n  }\n\n  .small-link {\n    border: 0;\n  }\n\n  .suggested-left > #control-buttons,\n  .suggested-right > #control-buttons {\n    float: none;\n    margin: 0;\n  }\n}\n\n@media (min-width: 421px) and (min-height: 500px) and (max-height: 560px) {\n  .interstitial-wrapper {\n    margin-top: 10vh;\n  }\n}\n\n@media (min-height: 400px) and (orientation:portrait) {\n  .interstitial-wrapper {\n    margin-bottom: 145px;\n  }\n}\n\n@media (min-height: 299px) {\n  .nav-wrapper {\n    padding-bottom: 16px;\n  }\n}\n\n@media (max-height: 560px) and (min-height: 240px) and (orientation:landscape) {\n  .extended-reporting-has-checkbox #details {\n    padding-bottom: 80px;\n  }\n}\n\n@media (min-height: 500px) and (max-height: 650px) and (max-width: 414px) and\n       (orientation: portrait) {\n  .interstitial-wrapper {\n    margin-top: 7vh;\n  }\n}\n\n@media (min-height: 650px) and (max-width: 414px) and (orientation: portrait) {\n  .interstitial-wrapper {\n    margin-top: 10vh;\n  }\n}\n\n/* Small mobile screens. No fixed nav. */\n@media (max-height: 400px) and (orientation: portrait),\n       (max-height: 239px) and (orientation: landscape),\n       (max-width: 419px) and (max-height: 399px) {\n  .interstitial-wrapper {\n    display: flex;\n    flex-direction: column;\n    margin-bottom: 0;\n  }\n\n  #details {\n    flex: 1 1 auto;\n    order: 0;\n  }\n\n  #main-content {\n    flex: 1 1 auto;\n    order: 0;\n  }\n\n  .nav-wrapper {\n    flex: 0 1 auto;\n    margin-top: 8px;\n    order: 1;\n    padding-inline-end: 0;\n    padding-inline-start: 0;\n    position: relative;\n    width: 100%;\n  }\n\n  button,\n  .nav-wrapper .secondary-button {\n    padding: 16px 24px;\n  }\n\n  button.small-link {\n    color: var(--google-blue-600);\n  }\n}\n\n@media (max-width: 239px) and (orientation: portrait) {\n  .nav-wrapper {\n    padding-inline-end: 0;\n    padding-inline-start: 0;\n  }\n}\n</style>\n  <style>/* Copyright 2013 The Chromium Authors. All rights reserved.\n * Use of this source code is governed by a BSD-style license that can be\n * found in the LICENSE file. */\n\n/* Don\'t use the main frame div when the error is in a subframe. */\nhtml[subframe] #main-frame-error {\n  display: none;\n}\n\n/* Don\'t use the subframe error div when the error is in a main frame. */\nhtml:not([subframe]) #sub-frame-error {\n  display: none;\n}\n\nh1 {\n  margin-top: 0;\n  word-wrap: break-word;\n}\n\nh1 span {\n  font-weight: 500;\n}\n\na {\n  text-decoration: none;\n}\n\n.icon {\n  -webkit-user-select: none;\n  display: inline-block;\n}\n\n.icon-generic {\n  /* Can\'t access chrome://theme/IDR_ERROR_NETWORK_GENERIC from an untrusted\n   * renderer process, so embed the resource manually. */\n  content: -webkit-image-set(\n      url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAEgAAABIAQMAAABvIyEEAAAABlBMVEUAAABTU1OoaSf/AAAAAXRSTlMAQObYZgAAAENJREFUeF7tzbEJACEQRNGBLeAasBCza2lLEGx0CxFGG9hBMDDxRy/72O9FMnIFapGylsu1fgoBdkXfUHLrQgdfrlJN1BdYBjQQm3UAAAAASUVORK5CYII=) 1x,\n      url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAJAAAACQAQMAAADdiHD7AAAABlBMVEUAAABTU1OoaSf/AAAAAXRSTlMAQObYZgAAAFJJREFUeF7t0cENgDAMQ9FwYgxG6WjpaIzCCAxQxVggFuDiCvlLOeRdHR9yzjncHVoq3npu+wQUrUuJHylSTmBaespJyJQoObUeyxDQb3bEm5Au81c0pSCD8HYAAAAASUVORK5CYII=) 2x);\n}\n\n.icon-offline {\n  content: -webkit-image-set(\n      url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAEgAAABIAQMAAABvIyEEAAAABlBMVEUAAABTU1OoaSf/AAAAAXRSTlMAQObYZgAAAGxJREFUeF7tyMEJwkAQRuFf5ipMKxYQiJ3Z2nSwrWwBA0+DQZcdxEOueaePp9+dQZFB7GpUcURSVU66yVNFj6LFICatThZB6r/ko/pbRpUgilY0Cbw5sNmb9txGXUKyuH7eV25x39DtJXUNPQGJtWFV+BT/QAAAAABJRU5ErkJggg==) 1x,\n      url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAJAAAACQBAMAAAAVaP+LAAAAGFBMVEUAAABTU1NNTU1TU1NPT09SUlJSUlJTU1O8B7DEAAAAB3RSTlMAoArVKvVgBuEdKgAAAJ1JREFUeF7t1TEOwyAMQNG0Q6/UE+RMXD9d/tC6womIFSL9P+MnAYOXeTIzMzMzMzMzaz8J9Ri6HoITmuHXhISE8nEh9yxDh55aCEUoTGbbQwjqHwIkRAEiIaG0+0AA9VBMaE89Rogeoww936MQrWdBr4GN/z0IAdQ6nQ/FIpRXDwHcA+JIJcQowQAlFUA0MfQpXLlVQfkzR4igS6ENjknm/wiaGhsAAAAASUVORK5CYII=) 2x);\n  position: relative;\n}\n\n.icon-disabled {\n  content: -webkit-image-set(\n      url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAHAAAABICAMAAAAZF4G5AAAABlBMVEVMaXFTU1OXUj8tAAAAAXRSTlMAQObYZgAAASZJREFUeAHd11Fq7jAMRGGf/W/6PoWB67YMqv5DybwG/CFjRuR8JBw3+ByiRjgV9W/TJ31P0tBfC6+cj1haUFXKHmVJo5wP98WwQ0ZCbfUc6LQ6VuUBz31ikADkLMkDrfUC4rR6QGW+gF6rx7NaHWCj1Y/W6lf4L7utvgBSt3rBFSS/XBMPUILcJINHCBWYUfpWn4NBi1ZfudIc3rf6/NGEvEA+AsYTJozmXemjXeLZAov+mnkN2HfzXpMSVQDnGw++57qNJ4D1xitA2sJ+VAWMygSEaYf2mYPTjZfk2K8wmP7HLIH5Mg4/pP+PEcDzUvDMvYbs/2NWwPO5vBdMZE4EE5UTQLiBFDaUlTDPBRoJ9HdAYIkIo06og3BNXtCzy7zA1aXk5x+tJARq63eAygAAAABJRU5ErkJggg==) 1x,\n      url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAOAAAACQAQMAAAArwfVjAAAABlBMVEVMaXFTU1OXUj8tAAAAAXRSTlMAQObYZgAAAYdJREFUeF7F1EFqwzAUBNARAmVj0FZe5QoBH6BX+dn4GlY2PYNzGx/A0CvkCIJuvIraKJKbgBvzf2g62weDGD7CYggpfFReis4J0ey9EGFIiEQQojFSlA9kSIiqd0KkFjKsewgRbStEN19mxUPTtmW9HQ/h6tyqNQ8NlSMZdzyE6qkoE0trVYGFm0n1WYeBhduzwbwBC7voS+vIxfeMjeaiLxsMMtQNwMPtuew+DjzcTHk8YMfDknEcIUOtf2lVfgVH3K4Xv5PRYAXRVMtItIJ3rfaCIVn9DsTH2NxisAVRex2Hh3hX+/mRUR08bAwPEYsI51ZxWH4Q0SpicQRXeyEaIug48FEdegARfMz/tADVsRciwTAxW308ehmC2gLraC+YCbV3QoTZexa+zegAEW5PhhgYfmbvJgcRqngGByOSXdFJcLk2JeDPEN0kxe1JhIt5FiFA+w+ItMELsUyPF2IaJ4aILqb4FbxPwhImwj6JauKgDUCYaxmYIsd4KXdMjIC9ItB5Bn4BNRwsG0XM2nwAAAAASUVORK5CYII=) 2x);\n  width: 112px;\n}\n\n.hidden {\n  display: none;\n}\n\n#suggestions-list a {\n  color: var(--google-blue-600);\n}\n\n#suggestions-list p {\n  margin-block-end: 0;\n}\n\n#suggestions-list ul {\n  margin-top: 0;\n}\n\n.single-suggestion {\n  list-style-type: none;\n  padding-inline-start: 0;\n}\n\n#error-information-button {\n  content: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij48cGF0aCBmaWxsPSJub25lIiBkPSJNMCAwaDI0djI0SDB6Ii8+PHBhdGggZD0iTTExIDE4aDJ2LTJoLTJ2MnptMS0xNkM2LjQ4IDIgMiA2LjQ4IDIgMTJzNC40OCAxMCAxMCAxMCAxMC00LjQ4IDEwLTEwUzE3LjUyIDIgMTIgMnptMCAxOGMtNC40MSAwLTgtMy41OS04LThzMy41OS04IDgtOCA4IDMuNTkgOCA4LTMuNTkgOC04IDh6bTAtMTRjLTIuMjEgMC00IDEuNzktNCA0aDJjMC0xLjEuOS0yIDItMnMyIC45IDIgMmMwIDItMyAxLjc1LTMgNWgyYzAtMi4yNSAzLTIuNSAzLTUgMC0yLjIxLTEuNzktNC00LTR6Ii8+PC9zdmc+);\n  height: 24px;\n  vertical-align: -.15em;\n  width: 24px;\n}\n\n.use-popup-container#error-information-popup-container\n  #error-information-popup {\n  align-items: center;\n  background-color: var(--popup-container-background-color);\n  display: flex;\n  height: 100%;\n  left: 0;\n  position: fixed;\n  top: 0;\n  width: 100%;\n  z-index: 100;\n}\n\n.use-popup-container#error-information-popup-container\n  #error-information-popup-content > p {\n  margin-bottom: 11px;\n  margin-inline-start: 20px;\n}\n\n.use-popup-container#error-information-popup-container #suggestions-list ul {\n  margin-inline-start: 15px;\n}\n\n.use-popup-container#error-information-popup-container\n  #error-information-popup-box {\n  background-color: var(--background-color);\n  left: 5%;\n  padding-bottom: 15px;\n  padding-top: 15px;\n  position: fixed;\n  width: 90%;\n  z-index: 101;\n}\n\n.use-popup-container#error-information-popup-container div.error-code {\n  margin-inline-start: 20px;\n}\n\n.use-popup-container#error-information-popup-container #suggestions-list p {\n  margin-inline-start: 20px;\n}\n\n:not(.use-popup-container)#error-information-popup-container\n  #error-information-popup-close {\n  display: none;\n}\n\n#error-information-popup-close {\n  margin-bottom: 0;\n  margin-inline-end: 35px;\n  margin-top: 15px;\n  text-align: end;\n}\n\n.link-button {\n  color: rgb(66, 133, 244);\n  display: inline-block;\n  font-weight: bold;\n  text-transform: uppercase;\n}\n\n#sub-frame-error-details {\n\n  color: #8F8F8F;\n\n  /* Not done on mobile for performance reasons. */\n  text-shadow: 0 1px 0 rgba(255,255,255,0.3);\n\n}\n\n[jscontent=hostName],\n[jscontent=failedUrl] {\n  overflow-wrap: break-word;\n}\n\n.secondary-button {\n  background: #d9d9d9;\n  color: #696969;\n  margin-inline-end: 16px;\n}\n\n.snackbar {\n  background: #323232;\n  border-radius: 2px;\n  bottom: 24px;\n  box-sizing: border-box;\n  color: #fff;\n  font-size: .87em;\n  left: 24px;\n  max-width: 568px;\n  min-width: 288px;\n  opacity: 0;\n  padding: 16px 24px 12px;\n  position: fixed;\n  transform: translateY(90px);\n  will-change: opacity, transform;\n  z-index: 999;\n}\n\n.snackbar-show {\n  -webkit-animation:\n    show-snackbar 250ms cubic-bezier(0, 0, 0.2, 1) forwards,\n    hide-snackbar 250ms cubic-bezier(0.4, 0, 1, 1) forwards 5s;\n}\n\n@-webkit-keyframes show-snackbar {\n  100% {\n    opacity: 1;\n    transform: translateY(0);\n  }\n}\n\n@-webkit-keyframes hide-snackbar {\n  0% {\n    opacity: 1;\n    transform: translateY(0);\n  }\n  100% {\n    opacity: 0;\n    transform: translateY(90px);\n  }\n}\n\n.suggestions {\n  margin-top: 18px;\n}\n\n.suggestion-header {\n  font-weight: bold;\n  margin-bottom: 4px;\n}\n\n.suggestion-body {\n  color: #777;\n}\n\n/* Decrease padding at low sizes. */\n@media (max-width: 640px), (max-height: 640px) {\n  h1 {\n    margin: 0 0 15px;\n  }\n  .suggestions {\n    margin-top: 10px;\n  }\n  .suggestion-header {\n    margin-bottom: 0;\n  }\n}\n\n#download-link,\n#download-link-clicked {\n  margin-bottom: 30px;\n  margin-top: 30px;\n}\n\n#download-link-clicked {\n  color: #BBB;\n}\n\n#download-link::before,\n#download-link-clicked::before {\n  content: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxLjJlbSIgaGVpZ2h0PSIxLjJlbSIgdmlld0JveD0iMCAwIDI0IDI0Ij48cGF0aCBkPSJNNSAyMGgxNHYtMkg1bTE0LTloLTRWM0g5djZINWw3IDcgNy03eiIgZmlsbD0iIzQyODVGNCIvPjwvc3ZnPg==);\n  display: inline-block;\n  margin-inline-end: 4px;\n  vertical-align: -webkit-baseline-middle;\n}\n\n#download-link-clicked::before {\n  opacity: 0;\n  width: 0;\n}\n\n#offline-content-list-visibility-card {\n  border: 1px solid white;\n  border-radius: 8px;\n  display: flex;\n  font-size: .8em;\n  justify-content: space-between;\n  line-height: 1;\n}\n\n#offline-content-list.list-hidden #offline-content-list-visibility-card {\n  border-color: rgb(218, 220, 224);\n}\n\n#offline-content-list-visibility-card > div {\n  padding: 1em;\n}\n\n#offline-content-list-title {\n  color: var(--google-gray-700);\n}\n\n#offline-content-list-show-text,\n#offline-content-list-hide-text {\n  color: rgb(66, 133, 244);\n}\n\n/* Hides the "hide" text div when the offline content list is collapsed/hidden\n * and, alternatively, hides the "show" text div when the offline content list\n * is expanded/shown.\n */\n#offline-content-list.list-hidden #offline-content-list-hide-text,\n#offline-content-list:not(.list-hidden) #offline-content-list-show-text {\n  display: none;\n}\n\n/* Controls the animation of the offline content list when it is expanded/shown.\n */\n#offline-content-suggestions {\n  /* Max-height has to be set for the height animation to work. The chosen value\n   * is a little greater than the maximum height the list will have, when all\n   * suggestions have images, so that it is never clamped. This makes so that\n   * when the actual height is smaller then the animation is not as smooth.\n   */\n  max-height: 27em;\n  transition: max-height 200ms ease-in, visibility 0s 200ms,\n              opacity 200ms 200ms linear;\n}\n\n/* Controls the animation of the offline content list when it is\n * collapsed/hidden.\n */\n#offline-content-list.list-hidden #offline-content-suggestions {\n  max-height: 0;\n  opacity: 0;\n  transition: opacity 200ms linear, visibility 0s 200ms,\n              max-height 200ms 200ms ease-out;\n  visibility: hidden;\n}\n\n#offline-content-list {\n  margin-inline-start: -5%;\n  width: 110%;\n}\n\n/* The selectors below adjust the "overflow" of the suggestion cards contents\n * based on the same screen size based strategy used for the main frame, which\n * is applied by the `interstitial-wrapper` class. */\n@media (max-width: 420px)  {\n  #offline-content-list {\n    margin-inline-start: -2.5%;\n    width: 105%;\n  }\n}\n@media (max-width: 420px) and (orientation: portrait),\n       (max-height: 560px) {\n  #offline-content-list {\n    margin-inline-start: -12px;\n    width: calc(100% + 24px);\n  }\n}\n\n.suggestion-with-image .offline-content-suggestion-thumbnail {\n  flex-basis: 8.2em;\n  flex-shrink: 0;\n}\n\n.suggestion-with-image .offline-content-suggestion-thumbnail > img {\n  height: 100%;\n  width: 100%;\n}\n\n.suggestion-with-image #offline-content-list:not(.is-rtl)\n.offline-content-suggestion-thumbnail > img {\n  border-bottom-right-radius: 7px;\n  border-top-right-radius: 7px;\n}\n\n.suggestion-with-image #offline-content-list.is-rtl\n.offline-content-suggestion-thumbnail > img {\n  border-bottom-left-radius: 7px;\n  border-top-left-radius: 7px;\n}\n\n.suggestion-with-icon .offline-content-suggestion-thumbnail {\n  align-items: center;\n  display: flex;\n  justify-content: center;\n  min-height: 4.2em;\n  min-width: 4.2em;\n}\n\n.suggestion-with-icon .offline-content-suggestion-thumbnail > div {\n  align-items: center;\n  background-color: rgb(241, 243, 244);\n  border-radius: 50%;\n  display: flex;\n  height: 2.3em;\n  justify-content: center;\n  width: 2.3em;\n}\n\n.suggestion-with-icon .offline-content-suggestion-thumbnail > div > img {\n  height: 1.45em;\n  width: 1.45em;\n}\n\n.offline-content-suggestion-favicon {\n  height: 1em;\n  margin-inline-end: 0.4em;\n  width: 1.4em;\n}\n\n.offline-content-suggestion-favicon > img {\n  height: 1.4em;\n  width: 1.4em;\n}\n\n.no-favicon .offline-content-suggestion-favicon {\n  display: none;\n}\n\n.image-video {\n  content: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij48cGF0aCBkPSJNMTcgMTAuNVY3YTEgMSAwIDAgMC0xLTFINGExIDEgMCAwIDAtMSAxdjEwYTEgMSAwIDAgMCAxIDFoMTJhMSAxIDAgMCAwIDEtMXYtMy41bDQgNHYtMTFsLTQgNHoiIGZpbGw9IiMzQzQwNDMiLz48L3N2Zz4=);\n}\n\n.image-music-note {\n  content: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij48cGF0aCBkPSJNMTIgM3Y5LjI2Yy0uNS0uMTctMS0uMjYtMS41LS4yNkM4IDEyIDYgMTQgNiAxNi41UzggMjEgMTAuNSAyMXM0LjUtMiA0LjUtNC41VjZoNFYzaC03eiIgZmlsbD0iIzNDNDA0MyIvPjwvc3ZnPg==);\n}\n\n.image-earth {\n  content: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBkPSJNMTIgMmM1LjUyIDAgMTAgNC40OCAxMCAxMHMtNC40OCAxMC0xMCAxMFMyIDE3LjUyIDIgMTIgNi40OCAyIDEyIDJ6TTQgMTJoNC40YzMuNDA3LjAyMiA0LjkyMiAxLjczIDQuNTQzIDUuMTI3SDkuNDg4djIuNDdhOC4wMDQgOC4wMDQgMCAwIDAgMTAuNDk4LTguMDgzQzE5LjMyNyAxMi41MDQgMTguMzMyIDEzIDE3IDEzYy0yLjEzNyAwLTMuMjA2LS45MTYtMy4yMDYtMi43NWgtMy43NDhjLS4yNzQtMi43MjguNjgzLTQuMDkyIDIuODctNC4wOTIgMC0uOTc1LjMyNy0xLjU5Ny44MTEtMS45N0E4LjAwNCA4LjAwNCAwIDAgMCA0IDEyeiIgZmlsbD0iIzNDNDA0MyIvPjwvc3ZnPg==);\n}\n\n.image-file {\n  content: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij48cGF0aCBkPSJNMTMgOVYzLjVMMTguNSA5TTYgMmMtMS4xMSAwLTIgLjg5LTIgMnYxNmEyIDIgMCAwIDAgMiAyaDEyYTIgMiAwIDAgMCAyLTJWOGwtNi02SDZ6IiBmaWxsPSIjM0M0MDQzIi8+PC9zdmc+);\n}\n\n.offline-content-suggestion-texts {\n  display: flex;\n  flex-direction: column;\n  justify-content: space-between;\n  line-height: 1.3;\n  padding: .9em;\n  width: 100%;\n}\n\n.offline-content-suggestion-title {\n  -webkit-box-orient: vertical;\n  -webkit-line-clamp: 3;\n  color: rgb(32, 33, 36);\n  display: -webkit-box;\n  font-size: 1.1em;\n  overflow: hidden;\n  text-overflow: ellipsis;\n}\n\ndiv.offline-content-suggestion {\n  align-items: stretch;\n  border: 1px solid rgb(218, 220, 224);\n  border-radius: 8px;\n  display: flex;\n  justify-content: space-between;\n  margin-bottom: .8em;\n}\n\n.suggestion-with-image {\n  flex-direction: row;\n  height: 8.2em;\n  max-height: 8.2em;\n}\n\n.suggestion-with-icon {\n  flex-direction: row-reverse;\n  height: 4.2em;\n  max-height: 4.2em;\n}\n\n.suggestion-with-icon .offline-content-suggestion-title {\n  -webkit-line-clamp: 1;\n  word-break: break-all;\n}\n\n.suggestion-with-icon .offline-content-suggestion-texts {\n  padding-inline-start: 0;\n}\n\n.offline-content-suggestion-attribution-freshness {\n  color: rgb(95, 99, 104);\n  display: flex;\n  font-size: .8em;\n  line-height: 1.7em;\n}\n\n.offline-content-suggestion-attribution {\n  -webkit-box-orient: vertical;\n  -webkit-line-clamp: 1;\n  display: -webkit-box;\n  flex-shrink: 1;\n  margin-inline-end: 0.3em;\n  overflow: hidden;\n  overflow-wrap: break-word;\n  text-overflow: ellipsis;\n  word-break: break-all;\n}\n\n.no-attribution .offline-content-suggestion-attribution {\n  display: none;\n}\n\n.offline-content-suggestion-freshness::before {\n  content: \'-\';\n  display: inline-block;\n  flex-shrink: 0;\n  margin-inline-end: .1em;\n  margin-inline-start: .1em;\n}\n\n.no-attribution .offline-content-suggestion-freshness::before {\n  display: none;\n}\n\n.offline-content-suggestion-freshness {\n  flex-shrink: 0;\n}\n\n.suggestion-with-image .offline-content-suggestion-pin-spacer {\n  flex-grow: 100;\n  flex-shrink: 1;\n}\n\n.suggestion-with-image .offline-content-suggestion-pin {\n  content: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCI+PGRlZnM+PHBhdGggaWQ9ImEiIGQ9Ik0wIDBoMjR2MjRIMFYweiIvPjwvZGVmcz48Y2xpcFBhdGggaWQ9ImIiPjx1c2UgeGxpbms6aHJlZj0iI2EiIG92ZXJmbG93PSJ2aXNpYmxlIi8+PC9jbGlwUGF0aD48cGF0aCBjbGlwLXBhdGg9InVybCgjYikiIGQ9Ik0xMiAyQzYuNSAyIDIgNi41IDIgMTJzNC41IDEwIDEwIDEwIDEwLTQuNSAxMC0xMFMxNy41IDIgMTIgMnptNSAxNkg3di0yaDEwdjJ6bS02LjctNEw3IDEwLjdsMS40LTEuNCAxLjkgMS45IDUuMy01LjNMMTcgNy4zIDEwLjMgMTR6IiBmaWxsPSIjOUFBMEE2Ii8+PC9zdmc+);\n  flex-shrink: 0;\n  height: 1.4em;\n  margin-inline-start: .4em;\n  width: 1.4em;\n}\n\n/* Controls the animation (and a bit more) of the launch-downloads-home action\n * button when the offline content list is expanded/shown.\n */\n#offline-content-list-action {\n  text-align: center;\n  transition: visibility 0s 200ms, opacity 200ms 200ms linear;\n}\n\n/* Controls the animation of the launch-downloads-home action button when the\n * offline content list is collapsed/hidden.\n */\n#offline-content-list.list-hidden #offline-content-list-action {\n  opacity: 0;\n  transition: opacity 200ms linear, visibility 0s 200ms;\n  visibility: hidden;\n}\n\n#cancel-save-page-button {\n  background-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjI0IiBoZWlnaHQ9IjI0Ij48Y2xpcFBhdGggaWQ9Im1hc2siPjxwYXRoIGQ9Ik0xMiAyQzYuNSAyIDIgNi41IDIgMTJzNC41IDEwIDEwIDEwIDEwLTQuNSAxMC0xMFMxNy41IDIgMTIgMnptNSAxNkg3di0yaDEwdjJ6bS02LjctNEw3IDEwLjdsMS40LTEuNCAxLjkgMS45IDUuMy01LjNMMTcgNy4zIDEwLjMgMTR6IiBmaWxsPSIjOUFBMEE2Ii8+PC9jbGlwUGF0aD48cGF0aCBjbGlwLXBhdGg9InVybCgjbWFzaykiIGZpbGw9IiM5QUEwQTYiIGQ9Ik0wIDBoMjR2MjRIMHoiLz48cGF0aCBjbGlwLXBhdGg9InVybCgjbWFzaykiIGZpbGw9IiMxQTczRTgiIHN0eWxlPSJhbmltYXRpb246b2ZmbGluZUFuaW1hdGlvbiA0cyBpbmZpbml0ZSIgZD0iTTAgMGgyNHYyNEgweiIvPjxzdHlsZT5Aa2V5ZnJhbWVzIG9mZmxpbmVBbmltYXRpb257MCUsMzUle2hlaWdodDowfTYwJXtoZWlnaHQ6MTAwJX05MCV7ZmlsbC1vcGFjaXR5OjF9dG97ZmlsbC1vcGFjaXR5OjB9fTwvc3R5bGU+PC9zdmc+);\n  background-position: right 27px center;\n  background-repeat: no-repeat;\n  border: 1px solid var(--google-gray-300);\n  border-radius: 5px;\n  color: var(--google-gray-700);\n  margin-bottom: 26px;\n  padding-bottom: 16px;\n  padding-inline-end: 88px;\n  padding-inline-start: 16px;\n  padding-top: 16px;\n  text-align: start;\n}\n\nhtml[dir=\'rtl\'] #cancel-save-page-button {\n  background-position: left 27px center;\n}\n\n#save-page-for-later-button {\n  display: flex;\n  justify-content: start;\n}\n\n#save-page-for-later-button a::before {\n  content: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxLjJlbSIgaGVpZ2h0PSIxLjJlbSIgdmlld0JveD0iMCAwIDI0IDI0Ij48cGF0aCBkPSJNNSAyMGgxNHYtMkg1bTE0LTloLTRWM0g5djZINWw3IDcgNy03eiIgZmlsbD0iIzQyODVGNCIvPjwvc3ZnPg==);\n  display: inline-block;\n  margin-inline-end: 4px;\n  vertical-align: -webkit-baseline-middle;\n}\n\n.hidden#save-page-for-later-button {\n  display: none;\n}\n\n/* Don\'t allow overflow when in a subframe. */\nhtml[subframe] body {\n  overflow: hidden;\n}\n\n#sub-frame-error {\n  -webkit-align-items: center;\n  -webkit-flex-flow: column;\n  -webkit-justify-content: center;\n  background-color: #DDD;\n  display: -webkit-flex;\n  height: 100%;\n  left: 0;\n  position: absolute;\n  text-align: center;\n  top: 0;\n  transition: background-color 200ms ease-in-out;\n  width: 100%;\n}\n\n#sub-frame-error:hover {\n  background-color: #EEE;\n}\n\n#sub-frame-error .icon-generic {\n  margin: 0 0 16px;\n}\n\n#sub-frame-error-details {\n  margin: 0 10px;\n  text-align: center;\n  visibility: hidden;\n}\n\n/* Show details only when hovering. */\n#sub-frame-error:hover #sub-frame-error-details {\n  visibility: visible;\n}\n\n/* If the iframe is too small, always hide the error code. */\n/* TODO(mmenke): See if overflow: no-display works better, once supported. */\n@media (max-width: 200px), (max-height: 95px) {\n  #sub-frame-error-details {\n    display: none;\n  }\n}\n\n/* Adjust icon for small embedded frames in apps. */\n@media (max-height: 100px) {\n  #sub-frame-error .icon-generic {\n    height: auto;\n    margin: 0;\n    padding-top: 0;\n    width: 25px;\n  }\n}\n\n/* details-button is special; it\'s a <button> element that looks like a link. */\n#details-button {\n  box-shadow: none;\n  min-width: 0;\n}\n\n/* Styles for platform dependent separation of controls and details button. */\n.suggested-left > #control-buttons,\n.suggested-right > #details-button {\n  float: left;\n}\n\n.suggested-right > #control-buttons,\n.suggested-left > #details-button {\n  float: right;\n}\n\n.suggested-left .secondary-button {\n  margin-inline-end: 0;\n  margin-inline-start: 16px;\n}\n\n#details-button.singular {\n  float: none;\n}\n\n/* download-button shows both icon and text. */\n#download-button {\n  padding-bottom: 4px;\n  padding-top: 4px;\n  position: relative;\n}\n\n#download-button::before {\n  background: -webkit-image-set(\n      url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAQAAABKfvVzAAAAO0lEQVQ4y2NgGArgPxIY1YChsOE/LtBAmpYG0mxpIOSDBpKUo2lpIDZxNJCkHKqlYZAla3RAHQ1DFgAARRroHyLNTwwAAAAASUVORK5CYII=) 1x,\n      url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAwCAQAAAD9CzEMAAAAZElEQVRYw+3Ruw3AMAwDUY3OzZUmRRD4E9iim9wNwAdbEURHyk4AAAAATiCVK8lLyPsKeT9K3lsownnunfkPxO78hKiYHxBV8x2icr5BVM+/CMf8g3DN34Rzns6ViwHUAUQ/6wIAd5Km7l6c8AAAAABJRU5ErkJggg==) 2x)\n    no-repeat;\n  content: \'\';\n  display: inline-block;\n  height: 24px;\n  margin-inline-end: 4px;\n  margin-inline-start: -4px;\n  vertical-align: middle;\n  width: 24px;\n}\n\n#download-button:disabled {\n  background: rgb(180, 206, 249);\n  color: rgb(255, 255, 255);\n}\n\n#buttons::after {\n  clear: both;\n  content: \'\';\n  display: block;\n  width: 100%;\n}\n\n/* Offline page */\nhtml[dir=\'rtl\'] .runner-container,\nhtml[dir=\'rtl\'].offline .icon-offline {\n  transform: scaleX(-1);\n}\n\n.offline {\n  transition: filter 1.5s cubic-bezier(0.65, 0.05, 0.36, 1),\n              background-color 1.5s cubic-bezier(0.65, 0.05, 0.36, 1);\n\n  will-change: filter, background-color;\n\n}\n\n.offline body {\n  transition: background-color 1.5s cubic-bezier(0.65, 0.05, 0.36, 1);\n}\n\n.offline #main-message > p {\n  display: none;\n}\n\n.offline.inverted {\n  background-color: #fff;\n  filter: invert(1);\n}\n\n.offline.inverted body {\n  background-color: #fff;\n}\n\n.offline .interstitial-wrapper {\n  color: var(--text-color);\n  font-size: 1em;\n  line-height: 1.55;\n  margin: 0 auto;\n  max-width: 600px;\n  padding-top: 100px;\n  position: relative;\n  width: 100%;\n}\n\n.offline .runner-container {\n  direction: ltr;\n  height: 150px;\n  max-width: 600px;\n  overflow: hidden;\n  position: absolute;\n  top: 35px;\n  width: 44px;\n}\n\n.offline .runner-container:focus {\n  outline: none;\n}\n\n.offline .runner-container:focus-visible {\n  outline: 3px solid var(--google-blue-300);\n}\n\n.offline .runner-canvas {\n  height: 150px;\n  max-width: 600px;\n  opacity: 1;\n  overflow: hidden;\n  position: absolute;\n  top: 0;\n  z-index: 10;\n}\n\n.offline .controller {\n  height: 100vh;\n  left: 0;\n  position: absolute;\n  top: 0;\n  width: 100vw;\n  z-index: 9;\n}\n\n#offline-resources {\n  display: none;\n}\n\n#offline-instruction {\n  image-rendering: pixelated;\n  left: 0;\n  margin: auto;\n  position: absolute;\n  right: 0;\n  top: 60px;\n  width: fit-content;\n}\n\n.offline-runner-live-region {\n  bottom: 0;\n  clip-path: polygon(0 0, 0 0, 0 0);\n  color: var(--background-color);\n  display: block;\n  font-size: xx-small;\n  overflow: hidden;\n  position: absolute;\n  text-align: center;\n  transition: color 1.5s cubic-bezier(0.65, 0.05, 0.36, 1);\n  user-select: none;\n}\n\n/* Custom toggle */\n.slow-speed-option {\n  align-items: center;\n  background: var(--google-gray-50);\n  border-radius: 24px/50%;\n  bottom: 0;\n  color: var(--error-code-color);\n  display: inline-flex;\n  font-size: 1em;\n  left: 0;\n  line-height: 1.1em;\n  margin: 5px auto;\n  padding: 2px 12px 3px 20px;\n  position: absolute;\n  right: 0;\n  width: max-content;\n  z-index: 999;\n}\n\n.slow-speed-option.hidden {\n  display: none;\n}\n\n.slow-speed-option [type=checkbox] {\n  opacity: 0;\n  pointer-events: none;\n  position: absolute;\n}\n\n.slow-speed-option .slow-speed-toggle {\n  cursor: pointer;\n  margin-inline-start: 8px;\n  padding: 8px 4px;\n  position: relative;\n}\n\n.slow-speed-option [type=checkbox]:disabled ~ .slow-speed-toggle {\n  cursor: default;\n}\n\n.slow-speed-option-label [type=checkbox] {\n  opacity: 0;\n  pointer-events: none;\n  position: absolute;\n}\n\n.slow-speed-option .slow-speed-toggle::before,\n.slow-speed-option .slow-speed-toggle::after {\n  content: \'\';\n  display: block;\n  margin: 0 3px;\n  transition: all 100ms cubic-bezier(0.4, 0, 1, 1);\n}\n\n.slow-speed-option .slow-speed-toggle::before {\n  background: rgb(189,193,198);\n  border-radius: 0.65em;\n  height: 0.9em;\n  width: 2em;\n}\n\n.slow-speed-option .slow-speed-toggle::after {\n  background: #fff;\n  border-radius: 50%;\n  box-shadow: 0 1px 3px 0 rgb(0 0 0 / 40%);\n  height: 1.2em;\n  position: absolute;\n  top: 51%;\n  transform: translate(-20%, -50%);\n  width: 1.1em;\n}\n\n.slow-speed-option [type=checkbox]:focus + .slow-speed-toggle {\n  box-shadow: 0 0 8px rgb(94, 158, 214);\n  outline: 1px solid rgb(93, 157, 213);\n}\n\n.slow-speed-option [type=checkbox]:checked + .slow-speed-toggle::before {\n  background: var(--google-blue-600);\n  opacity: 0.5;\n}\n\n.slow-speed-option [type=checkbox]:checked + .slow-speed-toggle::after {\n  background: var(--google-blue-600);\n  transform: translate(calc(2em - 90%), -50%);\n}\n\n.slow-speed-option [type=checkbox]:checked:disabled +\n  .slow-speed-toggle::before {\n  background: rgb(189,193,198);\n}\n\n.slow-speed-option [type=checkbox]:checked:disabled +\n  .slow-speed-toggle::after {\n  background: var(--google-gray-50);\n}\n\n@media (max-width: 420px) {\n  #download-button {\n    padding-bottom: 12px;\n    padding-top: 12px;\n  }\n\n  .suggested-left > #control-buttons,\n  .suggested-right > #control-buttons {\n    float: none;\n  }\n\n  .snackbar {\n    border-radius: 0;\n    bottom: 0;\n    left: 0;\n    width: 100%;\n  }\n}\n\n@media (max-height: 350px) {\n  h1 {\n    margin: 0 0 15px;\n  }\n\n  .icon-offline {\n    margin: 0 0 10px;\n  }\n\n  .interstitial-wrapper {\n    margin-top: 5%;\n  }\n\n  .nav-wrapper {\n    margin-top: 30px;\n  }\n}\n\n@media (min-width: 420px) and (max-width: 736px) and\n       (min-height: 240px) and (max-height: 420px) and\n       (orientation:landscape) {\n  .interstitial-wrapper {\n    margin-bottom: 100px;\n  }\n}\n\n@media (max-width: 360px) and (max-height: 480px) {\n  .offline .interstitial-wrapper {\n    padding-top: 60px;\n  }\n\n  .offline .runner-container {\n    top: 8px;\n  }\n}\n\n@media (min-height: 240px) and (orientation: landscape) {\n  .offline .interstitial-wrapper {\n    margin-bottom: 90px;\n  }\n\n  .icon-offline {\n    margin-bottom: 20px;\n  }\n}\n\n@media (max-height: 320px) and (orientation: landscape) {\n  .icon-offline {\n    margin-bottom: 0;\n  }\n\n  .offline .runner-container {\n    top: 10px;\n  }\n}\n\n@media (max-width: 240px) {\n  button {\n    padding-inline-end: 12px;\n    padding-inline-start: 12px;\n  }\n\n  .interstitial-wrapper {\n    overflow: inherit;\n    padding: 0 8px;\n  }\n}\n\n@media (max-width: 120px) {\n  button {\n    width: auto;\n  }\n}\n\n.arcade-mode,\n.arcade-mode .runner-container,\n.arcade-mode .runner-canvas {\n  image-rendering: pixelated;\n  max-width: 100%;\n  overflow: hidden;\n}\n\n.arcade-mode #buttons,\n.arcade-mode #main-content {\n  opacity: 0;\n  overflow: hidden;\n}\n\n.arcade-mode .interstitial-wrapper {\n  height: 100vh;\n  max-width: 100%;\n  overflow: hidden;\n}\n\n.arcade-mode .runner-container {\n  left: 0;\n  margin: auto;\n  right: 0;\n  transform-origin: top center;\n  transition: transform 250ms cubic-bezier(0.4, 0, 1, 1) 400ms;\n  z-index: 2;\n}\n\n@media (prefers-color-scheme: dark) {\n  .icon {\n    filter: invert(1);\n  }\n\n  .offline .runner-canvas {\n    filter: invert(1);\n  }\n\n  .offline.inverted {\n    background-color: var(--background-color);\n    filter: invert(0);\n  }\n\n  .offline.inverted body {\n    background-color: #fff;\n  }\n\n  .offline.inverted .offline-runner-live-region {\n    color: #fff;\n  }\n\n  #suggestions-list a {\n    color: var(--link-color);\n  }\n\n  #error-information-button {\n    filter: invert(0.6);\n  }\n\n  .slow-speed-option {\n    background: var(--google-gray-800);\n    color: var(--google-gray-100);\n  }\n\n  .slow-speed-option .slow-speed-toggle::before,\n  .slow-speed-option [type=checkbox]:checked:disabled +\n    .slow-speed-toggle::before {\n     background: rgb(189,193,198);\n  }\n\n  .slow-speed-option [type=checkbox]:checked + .slow-speed-toggle::after,\n  .slow-speed-option [type=checkbox]:checked + .slow-speed-toggle::before {\n    background: var(--google-blue-300);\n  }\n}\n</style>\n  <script>// Copyright 2013 The Chromium Authors. All rights reserved.\n// Use of this source code is governed by a BSD-style license that can be\n// found in the LICENSE file.\n\n/**\n * @typedef {{\n *   downloadButtonClick: function(),\n *   reloadButtonClick: function(string),\n *   detailsButtonClick: function(),\n *   diagnoseErrorsButtonClick: function(),\n *   trackEasterEgg: function(),\n *   updateEasterEggHighScore: function(number),\n *   resetEasterEggHighScore: function(),\n *   launchOfflineItem: function(string, string),\n *   savePageForLater: function(),\n *   cancelSavePage: function(),\n *   listVisibilityChange: function(boolean),\n * }}\n */\n// eslint-disable-next-line no-var\nvar errorPageController;\n\nconst HIDDEN_CLASS = \'hidden\';\n\n// Decodes a UTF16 string that is encoded as base64.\nfunction decodeUTF16Base64ToString(encoded_text) {\n  const data = atob(encoded_text);\n  let result = \'\';\n  for (let i = 0; i < data.length; i += 2) {\n    result +=\n        String.fromCharCode(data.charCodeAt(i) * 256 + data.charCodeAt(i + 1));\n  }\n  return result;\n}\n\nfunction toggleHelpBox() {\n  const helpBoxOuter = document.getElementById(\'details\');\n  helpBoxOuter.classList.toggle(HIDDEN_CLASS);\n  const detailsButton = document.getElementById(\'details-button\');\n  if (helpBoxOuter.classList.contains(HIDDEN_CLASS)) {\n    /** @suppress {missingProperties} */\n    detailsButton.innerText = detailsButton.detailsText;\n  } else {\n    /** @suppress {missingProperties} */\n    detailsButton.innerText = detailsButton.hideDetailsText;\n  }\n\n  // Details appears over the main content on small screens.\n  if (mobileNav) {\n    document.getElementById(\'main-content\').classList.toggle(HIDDEN_CLASS);\n    const runnerContainer = document.querySelector(\'.runner-container\');\n    if (runnerContainer) {\n      runnerContainer.classList.toggle(HIDDEN_CLASS);\n    }\n  }\n}\n\nfunction diagnoseErrors() {\n  if (window.errorPageController) {\n    errorPageController.diagnoseErrorsButtonClick();\n  }\n}\n\n// Subframes use a different layout but the same html file.  This is to make it\n// easier to support platforms that load the error page via different\n// mechanisms (Currently just iOS). We also use the subframe style for portals\n// as they are embedded like subframes and can\'t be interacted with by the user.\nlet isSubFrame = false;\nif (window.top.location !== window.location || window.portalHost) {\n  document.documentElement.setAttribute(\'subframe\', \'\');\n  isSubFrame = true;\n}\n\n// Re-renders the error page using |strings| as the dictionary of values.\n// Used by NetErrorTabHelper to update DNS error pages with probe results.\nfunction updateForDnsProbe(strings) {\n  const context = new JsEvalContext(strings);\n  jstProcess(context, document.getElementById(\'t\'));\n  onDocumentLoadOrUpdate();\n}\n\n// Adds an icon class to the list and removes classes previously set.\nfunction updateIconClass(newClass) {\n  const frameSelector = isSubFrame ? \'#sub-frame-error\' : \'#main-frame-error\';\n  const iconEl = document.querySelector(frameSelector + \' .icon\');\n\n  if (iconEl.classList.contains(newClass)) {\n    return;\n  }\n\n  iconEl.className = \'icon \' + newClass;\n}\n\n// Implements button clicks.  This function is needed during the transition\n// between implementing these in trunk chromium and implementing them in iOS.\nfunction reloadButtonClick(url) {\n  if (window.errorPageController) {\n    // \n\n    // \n    errorPageController.reloadButtonClick();\n    // \n  } else {\n    window.location = url;\n  }\n}\n\nfunction downloadButtonClick() {\n  if (window.errorPageController) {\n    errorPageController.downloadButtonClick();\n    const downloadButton = document.getElementById(\'download-button\');\n    downloadButton.disabled = true;\n    /** @suppress {missingProperties} */\n    downloadButton.textContent = downloadButton.disabledText;\n\n    document.getElementById(\'download-link-wrapper\')\n        .classList.add(HIDDEN_CLASS);\n    document.getElementById(\'download-link-clicked-wrapper\')\n        .classList.remove(HIDDEN_CLASS);\n  }\n}\n\nfunction detailsButtonClick() {\n  if (window.errorPageController) {\n    errorPageController.detailsButtonClick();\n  }\n}\n\nlet primaryControlOnLeft = true;\n// clang-format off\n// \n// clang-format on\nprimaryControlOnLeft = false;\n// \n\nfunction setAutoFetchState(scheduled, can_schedule) {\n  document.getElementById(\'cancel-save-page-button\')\n      .classList.toggle(HIDDEN_CLASS, !scheduled);\n  document.getElementById(\'save-page-for-later-button\')\n      .classList.toggle(HIDDEN_CLASS, scheduled || !can_schedule);\n}\n\nfunction savePageLaterClick() {\n  errorPageController.savePageForLater();\n  // savePageForLater will eventually trigger a call to setAutoFetchState() when\n  // it completes.\n}\n\nfunction cancelSavePageClick() {\n  errorPageController.cancelSavePage();\n  // setAutoFetchState is not called in response to cancelSavePage(), so do it\n  // now.\n  setAutoFetchState(false, true);\n}\n\nfunction toggleErrorInformationPopup() {\n  document.getElementById(\'error-information-popup-container\')\n      .classList.toggle(HIDDEN_CLASS);\n}\n\nfunction launchOfflineItem(itemID, name_space) {\n  errorPageController.launchOfflineItem(itemID, name_space);\n}\n\nfunction launchDownloadsPage() {\n  errorPageController.launchDownloadsPage();\n}\n\nfunction getIconForSuggestedItem(item) {\n  // Note: |item.content_type| contains the enum values from\n  // chrome::mojom::AvailableContentType.\n  switch (item.content_type) {\n    case 1:  // kVideo\n      return \'image-video\';\n    case 2:  // kAudio\n      return \'image-music-note\';\n    case 0:  // kPrefetchedPage\n    case 3:  // kOtherPage\n      return \'image-earth\';\n  }\n  return \'image-file\';\n}\n\nfunction getSuggestedContentDiv(item, index) {\n  // Note: See AvailableContentToValue in available_offline_content_helper.cc\n  // for the data contained in an |item|.\n  // TODO(carlosk): Present |snippet_base64| when that content becomes\n  // available.\n  let thumbnail = \'\';\n  const extraContainerClasses = [];\n  // html_inline.py will try to replace src attributes with data URIs using a\n  // simple regex. The following is obfuscated slightly to avoid that.\n  const source = \'src\';\n  if (item.thumbnail_data_uri) {\n    extraContainerClasses.push(\'suggestion-with-image\');\n    thumbnail = `<img ${source}="${item.thumbnail_data_uri}">`;\n  } else {\n    extraContainerClasses.push(\'suggestion-with-icon\');\n    const iconClass = getIconForSuggestedItem(item);\n    thumbnail = `<div><img class="${iconClass}"></div>`;\n  }\n\n  let favicon = \'\';\n  if (item.favicon_data_uri) {\n    favicon = `<img ${source}="${item.favicon_data_uri}">`;\n  } else {\n    extraContainerClasses.push(\'no-favicon\');\n  }\n\n  if (!item.attribution_base64) {\n    extraContainerClasses.push(\'no-attribution\');\n  }\n\n  return `\n  <div class="offline-content-suggestion ${extraContainerClasses.join(\' \')}"\n    onclick="launchOfflineItem(\'${item.ID}\', \'${item.name_space}\')">\n      <div class="offline-content-suggestion-texts">\n        <div id="offline-content-suggestion-title-${index}"\n             class="offline-content-suggestion-title">\n        </div>\n        <div class="offline-content-suggestion-attribution-freshness">\n          <div id="offline-content-suggestion-favicon-${index}"\n               class="offline-content-suggestion-favicon">\n            ${favicon}\n          </div>\n          <div id="offline-content-suggestion-attribution-${index}"\n               class="offline-content-suggestion-attribution">\n          </div>\n          <div class="offline-content-suggestion-freshness">\n            ${item.date_modified}\n          </div>\n          <div class="offline-content-suggestion-pin-spacer"></div>\n          <div class="offline-content-suggestion-pin"></div>\n        </div>\n      </div>\n      <div class="offline-content-suggestion-thumbnail">\n        ${thumbnail}\n      </div>\n  </div>`;\n}\n\n/**\n * @typedef {{\n *   ID: string,\n *   name_space: string,\n *   title_base64: string,\n *   snippet_base64: string,\n *   date_modified: string,\n *   attribution_base64: string,\n *   thumbnail_data_uri: string,\n *   favicon_data_uri: string,\n *   content_type: number,\n * }}\n */\nlet AvailableOfflineContent;\n\n// Populates a list of suggested offline content.\n// Note: For security reasons all content downloaded from the web is considered\n// unsafe and must be securely handled to be presented on the dino page. Images\n// have already been safely re-encoded but textual content -- like title and\n// attribution -- must be properly handled here.\n// @param {boolean} isShown\n// @param {Array<AvailableOfflineContent>} suggestions\nfunction offlineContentAvailable(isShown, suggestions) {\n  if (!suggestions || !loadTimeData.valueExists(\'offlineContentList\')) {\n    return;\n  }\n\n  const suggestionsHTML = [];\n  for (let index = 0; index < suggestions.length; index++) {\n    suggestionsHTML.push(getSuggestedContentDiv(suggestions[index], index));\n  }\n\n  document.getElementById(\'offline-content-suggestions\').innerHTML =\n      suggestionsHTML.join(\'\\n\');\n\n  // Sets textual web content using |textContent| to make sure it\'s handled as\n  // plain text.\n  for (let index = 0; index < suggestions.length; index++) {\n    document.getElementById(`offline-content-suggestion-title-${index}`)\n        .textContent =\n        decodeUTF16Base64ToString(suggestions[index].title_base64);\n    document.getElementById(`offline-content-suggestion-attribution-${index}`)\n        .textContent =\n        decodeUTF16Base64ToString(suggestions[index].attribution_base64);\n  }\n\n  const contentListElement = document.getElementById(\'offline-content-list\');\n  if (document.dir === \'rtl\') {\n    contentListElement.classList.add(\'is-rtl\');\n  }\n  contentListElement.hidden = false;\n  // The list is configured as hidden by default. Show it if needed.\n  if (isShown) {\n    toggleOfflineContentListVisibility(false);\n  }\n}\n\nfunction toggleOfflineContentListVisibility(updatePref) {\n  if (!loadTimeData.valueExists(\'offlineContentList\')) {\n    return;\n  }\n\n  const contentListElement = document.getElementById(\'offline-content-list\');\n  const isVisible = !contentListElement.classList.toggle(\'list-hidden\');\n\n  if (updatePref && window.errorPageController) {\n    errorPageController.listVisibilityChanged(isVisible);\n  }\n}\n\n// Called on document load, and from updateForDnsProbe().\nfunction onDocumentLoadOrUpdate() {\n  const downloadButtonVisible = loadTimeData.valueExists(\'downloadButton\') &&\n      loadTimeData.getValue(\'downloadButton\').msg;\n  const detailsButton = document.getElementById(\'details-button\');\n\n  // If offline content suggestions will be visible, the usual buttons will not\n  // be presented.\n  const offlineContentVisible =\n      loadTimeData.valueExists(\'suggestedOfflineContentPresentation\');\n  if (offlineContentVisible) {\n    document.querySelector(\'.nav-wrapper\').classList.add(HIDDEN_CLASS);\n    detailsButton.classList.add(HIDDEN_CLASS);\n\n    document.getElementById(\'download-link\').hidden = !downloadButtonVisible;\n    document.getElementById(\'download-links-wrapper\')\n        .classList.remove(HIDDEN_CLASS);\n    document.getElementById(\'error-information-popup-container\')\n        .classList.add(\'use-popup-container\', HIDDEN_CLASS);\n    document.getElementById(\'error-information-button\')\n        .classList.remove(HIDDEN_CLASS);\n  }\n\n  const attemptAutoFetch = loadTimeData.valueExists(\'attemptAutoFetch\') &&\n      loadTimeData.getValue(\'attemptAutoFetch\');\n\n  const reloadButtonVisible = loadTimeData.valueExists(\'reloadButton\') &&\n      loadTimeData.getValue(\'reloadButton\').msg;\n\n  const reloadButton = document.getElementById(\'reload-button\');\n  const downloadButton = document.getElementById(\'download-button\');\n  if (reloadButton.style.display === \'none\' &&\n      downloadButton.style.display === \'none\') {\n    detailsButton.classList.add(\'singular\');\n  }\n\n  // Show or hide control buttons.\n  const controlButtonDiv = document.getElementById(\'control-buttons\');\n  controlButtonDiv.hidden =\n      offlineContentVisible || !(reloadButtonVisible || downloadButtonVisible);\n\n  const iconClass = loadTimeData.valueExists(\'iconClass\') &&\n      loadTimeData.getValue(\'iconClass\');\n\n  updateIconClass(iconClass);\n\n  if (!isSubFrame && iconClass === \'icon-offline\') {\n    document.documentElement.classList.add(\'offline\');\n    new Runner(\'.interstitial-wrapper\');\n  }\n}\n\nfunction onDocumentLoad() {\n  // Sets up the proper button layout for the current platform.\n  const buttonsDiv = document.getElementById(\'buttons\');\n  if (primaryControlOnLeft) {\n    buttonsDiv.classList.add(\'suggested-left\');\n  } else {\n    buttonsDiv.classList.add(\'suggested-right\');\n  }\n\n  onDocumentLoadOrUpdate();\n}\n\ndocument.addEventListener(\'DOMContentLoaded\', onDocumentLoad);\n</script>\n  <script>// Copyright 2015 The Chromium Authors. All rights reserved.\n// Use of this source code is governed by a BSD-style license that can be\n// found in the LICENSE file.\n\nlet mobileNav = false;\n\n/**\n * For small screen mobile the navigation buttons are moved\n * below the advanced text.\n */\nfunction onResize() {\n  const helpOuterBox = document.querySelector(\'#details\');\n  const mainContent = document.querySelector(\'#main-content\');\n  const mediaQuery = \'(min-width: 240px) and (max-width: 420px) and \' +\n      \'(min-height: 401px), \' +\n      \'(max-height: 560px) and (min-height: 240px) and \' +\n      \'(min-width: 421px)\';\n\n  const detailsHidden = helpOuterBox.classList.contains(HIDDEN_CLASS);\n  const runnerContainer = document.querySelector(\'.runner-container\');\n\n  // Check for change in nav status.\n  if (mobileNav !== window.matchMedia(mediaQuery).matches) {\n    mobileNav = !mobileNav;\n\n    // Handle showing the top content / details sections according to state.\n    if (mobileNav) {\n      mainContent.classList.toggle(HIDDEN_CLASS, !detailsHidden);\n      helpOuterBox.classList.toggle(HIDDEN_CLASS, detailsHidden);\n      if (runnerContainer) {\n        runnerContainer.classList.toggle(HIDDEN_CLASS, !detailsHidden);\n      }\n    } else if (!detailsHidden) {\n      // Non mobile nav with visible details.\n      mainContent.classList.remove(HIDDEN_CLASS);\n      helpOuterBox.classList.remove(HIDDEN_CLASS);\n      if (runnerContainer) {\n        runnerContainer.classList.remove(HIDDEN_CLASS);\n      }\n    }\n  }\n}\n\nfunction setupMobileNav() {\n  window.addEventListener(\'resize\', onResize);\n  onResize();\n}\n\ndocument.addEventListener(\'DOMContentLoaded\', setupMobileNav);\n</script>\n  <script>// Copyright (c) 2014 The Chromium Authors. All rights reserved.\n// Use of this source code is governed by a BSD-style license that can be\n// found in the LICENSE file.\n\n/**\n * T-Rex runner.\n * @param {string} outerContainerId Outer containing element id.\n * @param {!Object=} opt_config\n * @constructor\n * @implements {EventListener}\n * @export\n */\nfunction Runner(outerContainerId, opt_config) {\n  // Singleton\n  if (Runner.instance_) {\n    return Runner.instance_;\n  }\n  Runner.instance_ = this;\n\n  this.outerContainerEl = document.querySelector(outerContainerId);\n  this.containerEl = null;\n  this.snackbarEl = null;\n  // A div to intercept touch events. Only set while (playing && useTouch).\n  this.touchController = null;\n\n  this.config = opt_config || Object.assign(Runner.config, Runner.normalConfig);\n  // Logical dimensions of the container.\n  this.dimensions = Runner.defaultDimensions;\n\n  this.gameType = null;\n  Runner.spriteDefinition = Runner.spriteDefinitionByType[\'original\'];\n\n  this.altGameImageSprite = null;\n  this.altGameModeActive = false;\n  this.altGameModeFlashTimer = null;\n  this.fadeInTimer = 0;\n\n  this.canvas = null;\n  this.canvasCtx = null;\n\n  this.tRex = null;\n\n  this.distanceMeter = null;\n  this.distanceRan = 0;\n\n  this.highestScore = 0;\n  this.syncHighestScore = false;\n\n  this.time = 0;\n  this.runningTime = 0;\n  this.msPerFrame = 1000 / FPS;\n  this.currentSpeed = this.config.SPEED;\n  Runner.slowDown = false;\n\n  this.obstacles = [];\n\n  this.activated = false; // Whether the easter egg has been activated.\n  this.playing = false; // Whether the game is currently in play state.\n  this.crashed = false;\n  this.paused = false;\n  this.inverted = false;\n  this.invertTimer = 0;\n  this.resizeTimerId_ = null;\n\n  this.playCount = 0;\n\n  // Sound FX.\n  this.audioBuffer = null;\n\n  /** @type {Object} */\n  this.soundFx = {};\n  this.generatedSoundFx = null;\n\n  // Global web audio context for playing sounds.\n  this.audioContext = null;\n\n  // Images.\n  this.images = {};\n  this.imagesLoaded = 0;\n\n  // Gamepad state.\n  this.pollingGamepads = false;\n  this.gamepadIndex = undefined;\n  this.previousGamepad = null;\n\n  if (this.isDisabled()) {\n    this.setupDisabledRunner();\n  } else {\n    if (Runner.isAltGameModeEnabled()) {\n      this.initAltGameType();\n      Runner.gameType = this.gameType;\n    }\n    this.loadImages();\n\n    window[\'initializeEasterEggHighScore\'] =\n        this.initializeHighScore.bind(this);\n  }\n}\n\n/**\n * Default game width.\n * @const\n */\nconst DEFAULT_WIDTH = 600;\n\n/**\n * Frames per second.\n * @const\n */\nconst FPS = 60;\n\n/** @const */\nconst IS_HIDPI = window.devicePixelRatio > 1;\n\n/** @const */\nconst IS_IOS = /CriOS/.test(window.navigator.userAgent);\n\n/** @const */\nconst IS_MOBILE = /Android/.test(window.navigator.userAgent) || IS_IOS;\n\n/** @const */\nconst IS_RTL = document.querySelector(\'html\').dir == \'rtl\';\n\n/** @const */\nconst ARCADE_MODE_URL = \'chrome://dino/\';\n\n/** @const */\nconst RESOURCE_POSTFIX = \'offline-resources-\';\n\n/** @const */\nconst A11Y_STRINGS = {\n  ariaLabel: \'dinoGameA11yAriaLabel\',\n  description: \'dinoGameA11yDescription\',\n  gameOver: \'dinoGameA11yGameOver\',\n  highScore: \'dinoGameA11yHighScore\',\n  jump: \'dinoGameA11yJump\',\n  started: \'dinoGameA11yStartGame\',\n  speedLabel: \'dinoGameA11ySpeedToggle\',\n};\n\n/**\n * Default game configuration.\n * Shared config for all  versions of the game. Additional parameters are\n * defined in Runner.normalConfig and Runner.slowConfig.\n */\nRunner.config = {\n  AUDIOCUE_PROXIMITY_THRESHOLD: 190,\n  AUDIOCUE_PROXIMITY_THRESHOLD_MOBILE_A11Y: 250,\n  BG_CLOUD_SPEED: 0.2,\n  BOTTOM_PAD: 10,\n  // Scroll Y threshold at which the game can be activated.\n  CANVAS_IN_VIEW_OFFSET: -10,\n  CLEAR_TIME: 3000,\n  CLOUD_FREQUENCY: 0.5,\n  FADE_DURATION: 1,\n  FLASH_DURATION: 1000,\n  GAMEOVER_CLEAR_TIME: 1200,\n  INITIAL_JUMP_VELOCITY: 12,\n  INVERT_FADE_DURATION: 12000,\n  MAX_BLINK_COUNT: 3,\n  MAX_CLOUDS: 6,\n  MAX_OBSTACLE_LENGTH: 3,\n  MAX_OBSTACLE_DUPLICATION: 2,\n  RESOURCE_TEMPLATE_ID: \'audio-resources\',\n  SPEED: 6,\n  SPEED_DROP_COEFFICIENT: 3,\n  ARCADE_MODE_INITIAL_TOP_POSITION: 35,\n  ARCADE_MODE_TOP_POSITION_PERCENT: 0.1,\n};\n\nRunner.normalConfig = {\n  ACCELERATION: 0.001,\n  AUDIOCUE_PROXIMITY_THRESHOLD: 190,\n  AUDIOCUE_PROXIMITY_THRESHOLD_MOBILE_A11Y: 250,\n  GAP_COEFFICIENT: 0.6,\n  INVERT_DISTANCE: 700,\n  MAX_SPEED: 13,\n  MOBILE_SPEED_COEFFICIENT: 1.2,\n  SPEED: 6,\n};\n\n\nRunner.slowConfig = {\n  ACCELERATION: 0.0005,\n  AUDIOCUE_PROXIMITY_THRESHOLD: 170,\n  AUDIOCUE_PROXIMITY_THRESHOLD_MOBILE_A11Y: 220,\n  GAP_COEFFICIENT: 0.3,\n  INVERT_DISTANCE: 350,\n  MAX_SPEED: 9,\n  MOBILE_SPEED_COEFFICIENT: 1.5,\n  SPEED: 4.2,\n};\n\n\n/**\n * Default dimensions.\n */\nRunner.defaultDimensions = {\n  WIDTH: DEFAULT_WIDTH,\n  HEIGHT: 150,\n};\n\n\n/**\n * CSS class names.\n * @enum {string}\n */\nRunner.classes = {\n  ARCADE_MODE: \'arcade-mode\',\n  CANVAS: \'runner-canvas\',\n  CONTAINER: \'runner-container\',\n  CRASHED: \'crashed\',\n  ICON: \'icon-offline\',\n  INVERTED: \'inverted\',\n  SNACKBAR: \'snackbar\',\n  SNACKBAR_SHOW: \'snackbar-show\',\n  TOUCH_CONTROLLER: \'controller\',\n};\n\n\n/**\n * Sound FX. Reference to the ID of the audio tag on interstitial page.\n * @enum {string}\n */\nRunner.sounds = {\n  BUTTON_PRESS: \'offline-sound-press\',\n  HIT: \'offline-sound-hit\',\n  SCORE: \'offline-sound-reached\',\n};\n\n\n/**\n * Key code mapping.\n * @enum {Object}\n */\nRunner.keycodes = {\n  JUMP: {\'38\': 1, \'32\': 1},  // Up, spacebar\n  DUCK: {\'40\': 1},           // Down\n  RESTART: {\'13\': 1},        // Enter\n};\n\n\n/**\n * Runner event names.\n * @enum {string}\n */\nRunner.events = {\n  ANIM_END: \'webkitAnimationEnd\',\n  CLICK: \'click\',\n  KEYDOWN: \'keydown\',\n  KEYUP: \'keyup\',\n  POINTERDOWN: \'pointerdown\',\n  POINTERUP: \'pointerup\',\n  RESIZE: \'resize\',\n  TOUCHEND: \'touchend\',\n  TOUCHSTART: \'touchstart\',\n  VISIBILITY: \'visibilitychange\',\n  BLUR: \'blur\',\n  FOCUS: \'focus\',\n  LOAD: \'load\',\n  GAMEPADCONNECTED: \'gamepadconnected\',\n};\n\nRunner.prototype = {\n  /**\n   * Initialize alternative game type.\n   */\n  initAltGameType() {\n    if (GAME_TYPE.length > 0) {\n      this.gameType = loadTimeData && loadTimeData.valueExists(\'altGameType\') ?\n          GAME_TYPE[parseInt(loadTimeData.getValue(\'altGameType\'), 10) - 1] :\n          \'\';\n    }\n  },\n\n  /**\n   * Whether the easter egg has been disabled. CrOS enterprise enrolled devices.\n   * @return {boolean}\n   */\n  isDisabled() {\n    return loadTimeData && loadTimeData.valueExists(\'disabledEasterEgg\');\n  },\n\n  /**\n   * For disabled instances, set up a snackbar with the disabled message.\n   */\n  setupDisabledRunner() {\n    this.containerEl = document.createElement(\'div\');\n    this.containerEl.className = Runner.classes.SNACKBAR;\n    this.containerEl.textContent = loadTimeData.getValue(\'disabledEasterEgg\');\n    this.outerContainerEl.appendChild(this.containerEl);\n\n    // Show notification when the activation key is pressed.\n    document.addEventListener(Runner.events.KEYDOWN, function(e) {\n      if (Runner.keycodes.JUMP[e.keyCode]) {\n        this.containerEl.classList.add(Runner.classes.SNACKBAR_SHOW);\n        document.querySelector(\'.icon\').classList.add(\'icon-disabled\');\n      }\n    }.bind(this));\n  },\n\n  /**\n   * Setting individual settings for debugging.\n   * @param {string} setting\n   * @param {number|string} value\n   */\n  updateConfigSetting(setting, value) {\n    if (setting in this.config && value !== undefined) {\n      this.config[setting] = value;\n\n      switch (setting) {\n        case \'GRAVITY\':\n        case \'MIN_JUMP_HEIGHT\':\n        case \'SPEED_DROP_COEFFICIENT\':\n          this.tRex.config[setting] = value;\n          break;\n        case \'INITIAL_JUMP_VELOCITY\':\n          this.tRex.setJumpVelocity(value);\n          break;\n        case \'SPEED\':\n          this.setSpeed(/** @type {number} */ (value));\n          break;\n      }\n    }\n  },\n\n  /**\n   * Creates an on page image element from the base 64 encoded string source.\n   * @param {string} resourceName Name in data object,\n   * @return {HTMLImageElement} The created element.\n   */\n  createImageElement(resourceName) {\n    const imgSrc = loadTimeData && loadTimeData.valueExists(resourceName) ?\n        loadTimeData.getString(resourceName) :\n        null;\n\n    if (imgSrc) {\n      const el =\n          /** @type {HTMLImageElement} */ (document.createElement(\'img\'));\n      el.id = resourceName;\n      el.src = imgSrc;\n      document.getElementById(\'offline-resources\').appendChild(el);\n      return el;\n    }\n    return null;\n  },\n\n  /**\n   * Cache the appropriate image sprite from the page and get the sprite sheet\n   * definition.\n   */\n  loadImages() {\n    let scale = \'1x\';\n    this.spriteDef = Runner.spriteDefinition.LDPI;\n    if (IS_HIDPI) {\n      scale = \'2x\';\n      this.spriteDef = Runner.spriteDefinition.HDPI;\n    }\n\n    Runner.imageSprite = /** @type {HTMLImageElement} */\n        (document.getElementById(RESOURCE_POSTFIX + scale));\n\n    if (this.gameType) {\n      Runner.altGameImageSprite = /** @type {HTMLImageElement} */\n          (this.createImageElement(\'altGameSpecificImage\' + scale));\n      Runner.altCommonImageSprite = /** @type {HTMLImageElement} */\n          (this.createImageElement(\'altGameCommonImage\' + scale));\n    }\n    Runner.origImageSprite = Runner.imageSprite;\n\n    // Disable the alt game mode if the sprites can\'t be loaded.\n    if (!Runner.altGameImageSprite || !Runner.altCommonImageSprite) {\n      Runner.isAltGameModeEnabled = () => false;\n      this.altGameModeActive = false;\n    }\n\n    if (Runner.imageSprite.complete) {\n      this.init();\n    } else {\n      // If the images are not yet loaded, add a listener.\n      Runner.imageSprite.addEventListener(Runner.events.LOAD,\n          this.init.bind(this));\n    }\n  },\n\n  /**\n   * Load and decode base 64 encoded sounds.\n   */\n  loadSounds() {\n    if (!IS_IOS) {\n      this.audioContext = new AudioContext();\n\n      const resourceTemplate =\n          document.getElementById(this.config.RESOURCE_TEMPLATE_ID).content;\n\n      for (const sound in Runner.sounds) {\n        let soundSrc =\n            resourceTemplate.getElementById(Runner.sounds[sound]).src;\n        soundSrc = soundSrc.substr(soundSrc.indexOf(\',\') + 1);\n        const buffer = decodeBase64ToArrayBuffer(soundSrc);\n\n        // Async, so no guarantee of order in array.\n        this.audioContext.decodeAudioData(buffer, function(index, audioData) {\n            this.soundFx[index] = audioData;\n          }.bind(this, sound));\n      }\n    }\n  },\n\n  /**\n   * Sets the game speed. Adjust the speed accordingly if on a smaller screen.\n   * @param {number=} opt_speed\n   */\n  setSpeed(opt_speed) {\n    const speed = opt_speed || this.currentSpeed;\n\n    // Reduce the speed on smaller mobile screens.\n    if (this.dimensions.WIDTH < DEFAULT_WIDTH) {\n      const mobileSpeed = Runner.slowDown ? speed :\n                                            speed * this.dimensions.WIDTH /\n              DEFAULT_WIDTH * this.config.MOBILE_SPEED_COEFFICIENT;\n      this.currentSpeed = mobileSpeed > speed ? speed : mobileSpeed;\n    } else if (opt_speed) {\n      this.currentSpeed = opt_speed;\n    }\n  },\n\n  /**\n   * Game initialiser.\n   */\n  init() {\n    // Hide the static icon.\n    document.querySelector(\'.\' + Runner.classes.ICON).style.visibility =\n        \'hidden\';\n\n    this.adjustDimensions();\n    this.setSpeed();\n\n    const ariaLabel = getA11yString(A11Y_STRINGS.ariaLabel);\n    this.containerEl = document.createElement(\'div\');\n    this.containerEl.setAttribute(\'role\', IS_MOBILE ? \'button\' : \'application\');\n    this.containerEl.setAttribute(\'tabindex\', \'0\');\n    this.containerEl.setAttribute(\'title\', ariaLabel);\n\n    this.containerEl.className = Runner.classes.CONTAINER;\n\n    // Player canvas container.\n    this.canvas = createCanvas(this.containerEl, this.dimensions.WIDTH,\n        this.dimensions.HEIGHT);\n\n    // Live region for game status updates.\n    this.a11yStatusEl = document.createElement(\'span\');\n    this.a11yStatusEl.className = \'offline-runner-live-region\';\n    this.a11yStatusEl.setAttribute(\'aria-live\', \'assertive\');\n    this.a11yStatusEl.textContent = \'\';\n    Runner.a11yStatusEl = this.a11yStatusEl;\n\n    // Add checkbox to slow down the game.\n    this.slowSpeedCheckboxLabel = document.createElement(\'label\');\n    this.slowSpeedCheckboxLabel.className = \'slow-speed-option hidden\';\n    this.slowSpeedCheckboxLabel.textContent =\n        getA11yString(A11Y_STRINGS.speedLabel);\n\n    this.slowSpeedCheckbox = document.createElement(\'input\');\n    this.slowSpeedCheckbox.setAttribute(\'type\', \'checkbox\');\n    this.slowSpeedCheckbox.setAttribute(\n        \'title\', getA11yString(A11Y_STRINGS.speedLabel));\n    this.slowSpeedCheckbox.setAttribute(\'tabindex\', \'0\');\n    this.slowSpeedCheckbox.setAttribute(\'checked\', \'checked\');\n\n    this.slowSpeedToggleEl = document.createElement(\'span\');\n    this.slowSpeedToggleEl.className = \'slow-speed-toggle\';\n\n    this.slowSpeedCheckboxLabel.appendChild(this.slowSpeedCheckbox);\n    this.slowSpeedCheckboxLabel.appendChild(this.slowSpeedToggleEl);\n\n    if (IS_IOS) {\n      this.outerContainerEl.appendChild(this.a11yStatusEl);\n    } else {\n      this.containerEl.appendChild(this.a11yStatusEl);\n    }\n\n    announcePhrase(getA11yString(A11Y_STRINGS.description));\n\n    this.generatedSoundFx = new GeneratedSoundFx();\n\n    this.canvasCtx =\n        /** @type {CanvasRenderingContext2D} */ (this.canvas.getContext(\'2d\'));\n    this.canvasCtx.fillStyle = \'#f7f7f7\';\n    this.canvasCtx.fill();\n    Runner.updateCanvasScaling(this.canvas);\n\n    // Horizon contains clouds, obstacles and the ground.\n    this.horizon = new Horizon(this.canvas, this.spriteDef, this.dimensions,\n        this.config.GAP_COEFFICIENT);\n\n    // Distance meter\n    this.distanceMeter = new DistanceMeter(this.canvas,\n          this.spriteDef.TEXT_SPRITE, this.dimensions.WIDTH);\n\n    // Draw t-rex\n    this.tRex = new Trex(this.canvas, this.spriteDef.TREX);\n\n    this.outerContainerEl.appendChild(this.containerEl);\n    this.outerContainerEl.appendChild(this.slowSpeedCheckboxLabel);\n\n    this.startListening();\n    this.update();\n\n    window.addEventListener(Runner.events.RESIZE,\n        this.debounceResize.bind(this));\n\n    // Handle dark mode\n    const darkModeMediaQuery =\n        window.matchMedia(\'(prefers-color-scheme: dark)\');\n    this.isDarkMode = darkModeMediaQuery && darkModeMediaQuery.matches;\n    darkModeMediaQuery.addListener((e) => {\n      this.isDarkMode = e.matches;\n    });\n  },\n\n  /**\n   * Create the touch controller. A div that covers whole screen.\n   */\n  createTouchController() {\n    this.touchController = document.createElement(\'div\');\n    this.touchController.className = Runner.classes.TOUCH_CONTROLLER;\n    this.touchController.addEventListener(Runner.events.TOUCHSTART, this);\n    this.touchController.addEventListener(Runner.events.TOUCHEND, this);\n    this.outerContainerEl.appendChild(this.touchController);\n  },\n\n  /**\n   * Debounce the resize event.\n   */\n  debounceResize() {\n    if (!this.resizeTimerId_) {\n      this.resizeTimerId_ =\n          setInterval(this.adjustDimensions.bind(this), 250);\n    }\n  },\n\n  /**\n   * Adjust game space dimensions on resize.\n   */\n  adjustDimensions() {\n    clearInterval(this.resizeTimerId_);\n    this.resizeTimerId_ = null;\n\n    const boxStyles = window.getComputedStyle(this.outerContainerEl);\n    const padding = Number(boxStyles.paddingLeft.substr(0,\n        boxStyles.paddingLeft.length - 2));\n\n    this.dimensions.WIDTH = this.outerContainerEl.offsetWidth - padding * 2;\n    if (this.isArcadeMode()) {\n      this.dimensions.WIDTH = Math.min(DEFAULT_WIDTH, this.dimensions.WIDTH);\n      if (this.activated) {\n        this.setArcadeModeContainerScale();\n      }\n    }\n\n    // Redraw the elements back onto the canvas.\n    if (this.canvas) {\n      this.canvas.width = this.dimensions.WIDTH;\n      this.canvas.height = this.dimensions.HEIGHT;\n\n      Runner.updateCanvasScaling(this.canvas);\n\n      this.distanceMeter.calcXPos(this.dimensions.WIDTH);\n      this.clearCanvas();\n      this.horizon.update(0, 0, true);\n      this.tRex.update(0);\n\n      // Outer container and distance meter.\n      if (this.playing || this.crashed || this.paused) {\n        this.containerEl.style.width = this.dimensions.WIDTH + \'px\';\n        this.containerEl.style.height = this.dimensions.HEIGHT + \'px\';\n        this.distanceMeter.update(0, Math.ceil(this.distanceRan));\n        this.stop();\n      } else {\n        this.tRex.draw(0, 0);\n      }\n\n      // Game over panel.\n      if (this.crashed && this.gameOverPanel) {\n        this.gameOverPanel.updateDimensions(this.dimensions.WIDTH);\n        this.gameOverPanel.draw(this.altGameModeActive, this.tRex);\n      }\n    }\n  },\n\n  /**\n   * Play the game intro.\n   * Canvas container width expands out to the full width.\n   */\n  playIntro() {\n    if (!this.activated && !this.crashed) {\n      this.playingIntro = true;\n      this.tRex.playingIntro = true;\n\n      // CSS animation definition.\n      const keyframes = \'@-webkit-keyframes intro { \' +\n            \'from { width:\' + Trex.config.WIDTH + \'px }\' +\n            \'to { width: \' + this.dimensions.WIDTH + \'px }\' +\n          \'}\';\n      document.styleSheets[0].insertRule(keyframes, 0);\n\n      this.containerEl.addEventListener(Runner.events.ANIM_END,\n          this.startGame.bind(this));\n\n      this.containerEl.style.webkitAnimation = \'intro .4s ease-out 1 both\';\n      this.containerEl.style.width = this.dimensions.WIDTH + \'px\';\n\n      this.setPlayStatus(true);\n      this.activated = true;\n    } else if (this.crashed) {\n      this.restart();\n    }\n  },\n\n\n  /**\n   * Update the game status to started.\n   */\n  startGame() {\n    if (this.isArcadeMode()) {\n      this.setArcadeMode();\n    }\n    this.toggleSpeed();\n    this.runningTime = 0;\n    this.playingIntro = false;\n    this.tRex.playingIntro = false;\n    this.containerEl.style.webkitAnimation = \'\';\n    this.playCount++;\n    this.generatedSoundFx.background();\n    announcePhrase(getA11yString(A11Y_STRINGS.started));\n\n    if (Runner.audioCues) {\n      this.containerEl.setAttribute(\'title\', getA11yString(A11Y_STRINGS.jump));\n    }\n\n    // Handle tabbing off the page. Pause the current game.\n    document.addEventListener(Runner.events.VISIBILITY,\n          this.onVisibilityChange.bind(this));\n\n    window.addEventListener(Runner.events.BLUR,\n          this.onVisibilityChange.bind(this));\n\n    window.addEventListener(Runner.events.FOCUS,\n          this.onVisibilityChange.bind(this));\n  },\n\n  clearCanvas() {\n    this.canvasCtx.clearRect(0, 0, this.dimensions.WIDTH,\n        this.dimensions.HEIGHT);\n  },\n\n  /**\n   * Checks whether the canvas area is in the viewport of the browser\n   * through the current scroll position.\n   * @return boolean.\n   */\n  isCanvasInView() {\n    return this.containerEl.getBoundingClientRect().top >\n        Runner.config.CANVAS_IN_VIEW_OFFSET;\n  },\n\n  /**\n   * Enable the alt game mode. Switching out the sprites.\n   */\n  enableAltGameMode() {\n    Runner.imageSprite = Runner.altGameImageSprite;\n    Runner.spriteDefinition = Runner.spriteDefinitionByType[Runner.gameType];\n\n    if (IS_HIDPI) {\n      this.spriteDef = Runner.spriteDefinition.HDPI;\n    } else {\n      this.spriteDef = Runner.spriteDefinition.LDPI;\n    }\n\n    this.altGameModeActive = true;\n    this.tRex.enableAltGameMode(this.spriteDef.TREX);\n    this.horizon.enableAltGameMode(this.spriteDef);\n    this.generatedSoundFx.background();\n  },\n\n  /**\n   * Update the game frame and schedules the next one.\n   */\n  update() {\n    this.updatePending = false;\n\n    const now = getTimeStamp();\n    let deltaTime = now - (this.time || now);\n\n    // Flashing when switching game modes.\n    if (this.altGameModeFlashTimer < 0 || this.altGameModeFlashTimer === 0) {\n      this.altGameModeFlashTimer = null;\n      this.tRex.setFlashing(false);\n      this.enableAltGameMode();\n    } else if (this.altGameModeFlashTimer > 0) {\n      this.altGameModeFlashTimer -= deltaTime;\n      this.tRex.update(deltaTime);\n      deltaTime = 0;\n    }\n\n    this.time = now;\n\n    if (this.playing) {\n      this.clearCanvas();\n\n      // Additional fade in - Prevents jump when switching sprites\n      if (this.altGameModeActive &&\n          this.fadeInTimer <= this.config.FADE_DURATION) {\n        this.fadeInTimer += deltaTime / 1000;\n        this.canvasCtx.globalAlpha = this.fadeInTimer;\n      } else {\n        this.canvasCtx.globalAlpha = 1;\n      }\n\n      if (this.tRex.jumping) {\n        this.tRex.updateJump(deltaTime);\n      }\n\n      this.runningTime += deltaTime;\n      const hasObstacles = this.runningTime > this.config.CLEAR_TIME;\n\n      // First jump triggers the intro.\n      if (this.tRex.jumpCount === 1 && !this.playingIntro) {\n        this.playIntro();\n      }\n\n      // The horizon doesn\'t move until the intro is over.\n      if (this.playingIntro) {\n        this.horizon.update(0, this.currentSpeed, hasObstacles);\n      } else if (!this.crashed) {\n        const showNightMode = this.isDarkMode ^ this.inverted;\n        deltaTime = !this.activated ? 0 : deltaTime;\n        this.horizon.update(\n            deltaTime, this.currentSpeed, hasObstacles, showNightMode);\n      }\n\n      // Check for collisions.\n      let collision = hasObstacles &&\n          checkForCollision(this.horizon.obstacles[0], this.tRex);\n\n      // For a11y, audio cues.\n      if (Runner.audioCues && hasObstacles) {\n        const jumpObstacle =\n            this.horizon.obstacles[0].typeConfig.type != \'COLLECTABLE\';\n\n        if (!this.horizon.obstacles[0].jumpAlerted) {\n          const threshold = Runner.isMobileMouseInput ?\n              Runner.config.AUDIOCUE_PROXIMITY_THRESHOLD_MOBILE_A11Y :\n              Runner.config.AUDIOCUE_PROXIMITY_THRESHOLD;\n          const adjProximityThreshold = threshold +\n              (threshold * Math.log10(this.currentSpeed / Runner.config.SPEED));\n\n          if (this.horizon.obstacles[0].xPos < adjProximityThreshold) {\n            if (jumpObstacle) {\n              this.generatedSoundFx.jump();\n            }\n            this.horizon.obstacles[0].jumpAlerted = true;\n          }\n        }\n      }\n\n      // Activated alt game mode.\n      if (Runner.isAltGameModeEnabled() && collision &&\n          this.horizon.obstacles[0].typeConfig.type == \'COLLECTABLE\') {\n        this.horizon.removeFirstObstacle();\n        this.tRex.setFlashing(true);\n        collision = false;\n        this.altGameModeFlashTimer = this.config.FLASH_DURATION;\n        this.runningTime = 0;\n        this.generatedSoundFx.collect();\n      }\n\n      if (!collision) {\n        this.distanceRan += this.currentSpeed * deltaTime / this.msPerFrame;\n\n        if (this.currentSpeed < this.config.MAX_SPEED) {\n          this.currentSpeed += this.config.ACCELERATION;\n        }\n      } else {\n        this.gameOver();\n      }\n\n      const playAchievementSound = this.distanceMeter.update(deltaTime,\n          Math.ceil(this.distanceRan));\n\n      if (!Runner.audioCues && playAchievementSound) {\n        this.playSound(this.soundFx.SCORE);\n      }\n\n      // Night mode.\n      if (!Runner.isAltGameModeEnabled()) {\n        if (this.invertTimer > this.config.INVERT_FADE_DURATION) {\n          this.invertTimer = 0;\n          this.invertTrigger = false;\n          this.invert(false);\n        } else if (this.invertTimer) {\n          this.invertTimer += deltaTime;\n        } else {\n          const actualDistance =\n              this.distanceMeter.getActualDistance(Math.ceil(this.distanceRan));\n\n          if (actualDistance > 0) {\n            this.invertTrigger =\n                !(actualDistance % this.config.INVERT_DISTANCE);\n\n            if (this.invertTrigger && this.invertTimer === 0) {\n              this.invertTimer += deltaTime;\n              this.invert(false);\n            }\n          }\n        }\n      }\n    }\n\n    if (this.playing || (!this.activated &&\n        this.tRex.blinkCount < Runner.config.MAX_BLINK_COUNT)) {\n      this.tRex.update(deltaTime);\n      this.scheduleNextUpdate();\n    }\n  },\n\n  /**\n   * Event handler.\n   * @param {Event} e\n   */\n  handleEvent(e) {\n    return (function(evtType, events) {\n      switch (evtType) {\n        case events.KEYDOWN:\n        case events.TOUCHSTART:\n        case events.POINTERDOWN:\n          this.onKeyDown(e);\n          break;\n        case events.KEYUP:\n        case events.TOUCHEND:\n        case events.POINTERUP:\n          this.onKeyUp(e);\n          break;\n        case events.GAMEPADCONNECTED:\n          this.onGamepadConnected(e);\n          break;\n      }\n    }.bind(this))(e.type, Runner.events);\n  },\n\n  /**\n   * Initialize audio cues if activated by focus on the canvas element.\n   * @param {Event} e\n   */\n  handleCanvasKeyPress(e) {\n    if (!this.activated && !Runner.audioCues) {\n      this.toggleSpeed();\n      Runner.audioCues = true;\n      this.generatedSoundFx.init();\n      Runner.generatedSoundFx = this.generatedSoundFx;\n      Runner.config.CLEAR_TIME *= 1.2;\n    } else if (e.keyCode && Runner.keycodes.JUMP[e.keyCode]) {\n      this.onKeyDown(e);\n    }\n  },\n\n  /**\n   * Prevent space key press from scrolling.\n   * @param {Event} e\n   */\n  preventScrolling(e) {\n    if (e.keyCode === 32) {\n      e.preventDefault();\n    }\n  },\n\n  /**\n   * Toggle speed setting if toggle is shown.\n   */\n  toggleSpeed() {\n    if (Runner.audioCues) {\n      const speedChange = Runner.slowDown != this.slowSpeedCheckbox.checked;\n\n      if (speedChange) {\n        Runner.slowDown = this.slowSpeedCheckbox.checked;\n        const updatedConfig =\n            Runner.slowDown ? Runner.slowConfig : Runner.normalConfig;\n\n        Runner.config = Object.assign(Runner.config, updatedConfig);\n        this.currentSpeed = updatedConfig.SPEED;\n        this.tRex.enableSlowConfig();\n        this.horizon.adjustObstacleSpeed();\n      }\n      if (this.playing) {\n        this.disableSpeedToggle(true);\n      }\n    }\n  },\n\n  /**\n   * Show the speed toggle.\n   * From focus event or when audio cues are activated.\n   * @param {Event=} e\n   */\n  showSpeedToggle(e) {\n    const isFocusEvent = e && e.type == \'focus\';\n    if (Runner.audioCues || isFocusEvent) {\n      this.slowSpeedCheckboxLabel.classList.toggle(\n          HIDDEN_CLASS, isFocusEvent ? false : !this.crashed);\n    }\n  },\n\n  /**\n   * Disable the speed toggle.\n   * @param {boolean} disable\n   */\n  disableSpeedToggle(disable) {\n    if (disable) {\n      this.slowSpeedCheckbox.setAttribute(\'disabled\', \'disabled\');\n    } else {\n      this.slowSpeedCheckbox.removeAttribute(\'disabled\');\n    }\n  },\n\n  /**\n   * Bind relevant key / mouse / touch listeners.\n   */\n  startListening() {\n    // A11y keyboard / screen reader activation.\n    this.containerEl.addEventListener(\n        Runner.events.KEYDOWN, this.handleCanvasKeyPress.bind(this));\n    if (!IS_MOBILE) {\n      this.containerEl.addEventListener(\n          Runner.events.FOCUS, this.showSpeedToggle.bind(this));\n    }\n    this.canvas.addEventListener(\n        Runner.events.KEYDOWN, this.preventScrolling.bind(this));\n    this.canvas.addEventListener(\n        Runner.events.KEYUP, this.preventScrolling.bind(this));\n\n    // Keys.\n    document.addEventListener(Runner.events.KEYDOWN, this);\n    document.addEventListener(Runner.events.KEYUP, this);\n\n    // Touch / pointer.\n    this.containerEl.addEventListener(Runner.events.TOUCHSTART, this);\n    document.addEventListener(Runner.events.POINTERDOWN, this);\n    document.addEventListener(Runner.events.POINTERUP, this);\n\n    if (this.isArcadeMode()) {\n      // Gamepad\n      window.addEventListener(Runner.events.GAMEPADCONNECTED, this);\n    }\n  },\n\n  /**\n   * Remove all listeners.\n   */\n  stopListening() {\n    document.removeEventListener(Runner.events.KEYDOWN, this);\n    document.removeEventListener(Runner.events.KEYUP, this);\n\n    if (this.touchController) {\n      this.touchController.removeEventListener(Runner.events.TOUCHSTART, this);\n      this.touchController.removeEventListener(Runner.events.TOUCHEND, this);\n    }\n\n    this.containerEl.removeEventListener(Runner.events.TOUCHSTART, this);\n    document.removeEventListener(Runner.events.POINTERDOWN, this);\n    document.removeEventListener(Runner.events.POINTERUP, this);\n\n    if (this.isArcadeMode()) {\n      window.removeEventListener(Runner.events.GAMEPADCONNECTED, this);\n    }\n  },\n\n  /**\n   * Process keydown.\n   * @param {Event} e\n   */\n  onKeyDown(e) {\n    // Prevent native page scrolling whilst tapping on mobile.\n    if (IS_MOBILE && this.playing) {\n      e.preventDefault();\n    }\n\n    if (this.isCanvasInView()) {\n      // Allow toggling of speed toggle.\n      if (Runner.keycodes.JUMP[e.keyCode] &&\n          e.target == this.slowSpeedCheckbox) {\n        return;\n      }\n\n      if (!this.crashed && !this.paused) {\n        // For a11y, screen reader activation.\n        const isMobileMouseInput = IS_MOBILE &&\n                e.type === Runner.events.POINTERDOWN &&\n                e.pointerType == \'mouse\' && e.target == this.containerEl ||\n            (IS_IOS && e.pointerType == \'touch\' &&\n             document.activeElement == this.containerEl);\n\n        if (Runner.keycodes.JUMP[e.keyCode] ||\n            e.type === Runner.events.TOUCHSTART || isMobileMouseInput ||\n            (Runner.keycodes.DUCK[e.keyCode] && this.altGameModeActive)) {\n          e.preventDefault();\n          // Starting the game for the first time.\n          if (!this.playing) {\n            // Started by touch so create a touch controller.\n            if (!this.touchController && e.type === Runner.events.TOUCHSTART) {\n              this.createTouchController();\n            }\n\n            if (isMobileMouseInput) {\n              this.handleCanvasKeyPress(e);\n            }\n            this.loadSounds();\n            this.setPlayStatus(true);\n            this.update();\n            if (window.errorPageController) {\n              errorPageController.trackEasterEgg();\n            }\n          }\n          // Start jump.\n          if (!this.tRex.jumping && !this.tRex.ducking) {\n            if (Runner.audioCues) {\n              this.generatedSoundFx.cancelFootSteps();\n            } else {\n              this.playSound(this.soundFx.BUTTON_PRESS);\n            }\n            this.tRex.startJump(this.currentSpeed);\n          }\n          // Ducking is disabled on alt game modes.\n        } else if (\n            !this.altGameModeActive && this.playing &&\n            Runner.keycodes.DUCK[e.keyCode]) {\n          e.preventDefault();\n          if (this.tRex.jumping) {\n            // Speed drop, activated only when jump key is not pressed.\n            this.tRex.setSpeedDrop();\n          } else if (!this.tRex.jumping && !this.tRex.ducking) {\n            // Duck.\n            this.tRex.setDuck(true);\n          }\n        }\n      }\n    }\n  },\n\n  /**\n   * Process key up.\n   * @param {Event} e\n   */\n  onKeyUp(e) {\n    const keyCode = String(e.keyCode);\n    const isjumpKey = Runner.keycodes.JUMP[keyCode] ||\n        e.type === Runner.events.TOUCHEND || e.type === Runner.events.POINTERUP;\n\n    if (this.isRunning() && isjumpKey) {\n      this.tRex.endJump();\n    } else if (Runner.keycodes.DUCK[keyCode]) {\n      this.tRex.speedDrop = false;\n      this.tRex.setDuck(false);\n    } else if (this.crashed) {\n      // Check that enough time has elapsed before allowing jump key to restart.\n      const deltaTime = getTimeStamp() - this.time;\n\n      if (this.isCanvasInView() &&\n          (Runner.keycodes.RESTART[keyCode] || this.isLeftClickOnCanvas(e) ||\n          (deltaTime >= this.config.GAMEOVER_CLEAR_TIME &&\n          Runner.keycodes.JUMP[keyCode]))) {\n        this.handleGameOverClicks(e);\n      }\n    } else if (this.paused && isjumpKey) {\n      // Reset the jump state\n      this.tRex.reset();\n      this.play();\n    }\n  },\n\n  /**\n   * Process gamepad connected event.\n   * @param {Event} e\n   */\n  onGamepadConnected(e) {\n    if (!this.pollingGamepads) {\n      this.pollGamepadState();\n    }\n  },\n\n  /**\n   * rAF loop for gamepad polling.\n   */\n  pollGamepadState() {\n    const gamepads = navigator.getGamepads();\n    this.pollActiveGamepad(gamepads);\n\n    this.pollingGamepads = true;\n    requestAnimationFrame(this.pollGamepadState.bind(this));\n  },\n\n  /**\n   * Polls for a gamepad with the jump button pressed. If one is found this\n   * becomes the "active" gamepad and all others are ignored.\n   * @param {!Array<Gamepad>} gamepads\n   */\n  pollForActiveGamepad(gamepads) {\n    for (let i = 0; i < gamepads.length; ++i) {\n      if (gamepads[i] && gamepads[i].buttons.length > 0 &&\n          gamepads[i].buttons[0].pressed) {\n        this.gamepadIndex = i;\n        this.pollActiveGamepad(gamepads);\n        return;\n      }\n    }\n  },\n\n  /**\n   * Polls the chosen gamepad for button presses and generates KeyboardEvents\n   * to integrate with the rest of the game logic.\n   * @param {!Array<Gamepad>} gamepads\n   */\n  pollActiveGamepad(gamepads) {\n    if (this.gamepadIndex === undefined) {\n      this.pollForActiveGamepad(gamepads);\n      return;\n    }\n\n    const gamepad = gamepads[this.gamepadIndex];\n    if (!gamepad) {\n      this.gamepadIndex = undefined;\n      this.pollForActiveGamepad(gamepads);\n      return;\n    }\n\n    // The gamepad specification defines the typical mapping of physical buttons\n    // to button indicies: https://w3c.github.io/gamepad/#remapping\n    this.pollGamepadButton(gamepad, 0, 38);  // Jump\n    if (gamepad.buttons.length >= 2) {\n      this.pollGamepadButton(gamepad, 1, 40);  // Duck\n    }\n    if (gamepad.buttons.length >= 10) {\n      this.pollGamepadButton(gamepad, 9, 13);  // Restart\n    }\n\n    this.previousGamepad = gamepad;\n  },\n\n  /**\n   * Generates a key event based on a gamepad button.\n   * @param {!Gamepad} gamepad\n   * @param {number} buttonIndex\n   * @param {number} keyCode\n   */\n  pollGamepadButton(gamepad, buttonIndex, keyCode) {\n    const state = gamepad.buttons[buttonIndex].pressed;\n    let previousState = false;\n    if (this.previousGamepad) {\n      previousState = this.previousGamepad.buttons[buttonIndex].pressed;\n    }\n    // Generate key events on the rising and falling edge of a button press.\n    if (state !== previousState) {\n      const e = new KeyboardEvent(state ? Runner.events.KEYDOWN\n                                      : Runner.events.KEYUP,\n                                { keyCode: keyCode });\n      document.dispatchEvent(e);\n    }\n  },\n\n  /**\n   * Handle interactions on the game over screen state.\n   * A user is able to tap the high score twice to reset it.\n   * @param {Event} e\n   */\n  handleGameOverClicks(e) {\n    if (e.target != this.slowSpeedCheckbox) {\n      e.preventDefault();\n      if (this.distanceMeter.hasClickedOnHighScore(e) && this.highestScore) {\n        if (this.distanceMeter.isHighScoreFlashing()) {\n          // Subsequent click, reset the high score.\n          this.saveHighScore(0, true);\n          this.distanceMeter.resetHighScore();\n        } else {\n          // First click, flash the high score.\n          this.distanceMeter.startHighScoreFlashing();\n        }\n      } else {\n        this.distanceMeter.cancelHighScoreFlashing();\n        this.restart();\n      }\n    }\n  },\n\n  /**\n   * Returns whether the event was a left click on canvas.\n   * On Windows right click is registered as a click.\n   * @param {Event} e\n   * @return {boolean}\n   */\n  isLeftClickOnCanvas(e) {\n    return e.button != null && e.button < 2 &&\n        e.type === Runner.events.POINTERUP &&\n        (e.target === this.canvas ||\n         (IS_MOBILE && Runner.audioCues && e.target === this.containerEl));\n  },\n\n  /**\n   * RequestAnimationFrame wrapper.\n   */\n  scheduleNextUpdate() {\n    if (!this.updatePending) {\n      this.updatePending = true;\n      this.raqId = requestAnimationFrame(this.update.bind(this));\n    }\n  },\n\n  /**\n   * Whether the game is running.\n   * @return {boolean}\n   */\n  isRunning() {\n    return !!this.raqId;\n  },\n\n  /**\n   * Set the initial high score as stored in the user\'s profile.\n   * @param {number} highScore\n   */\n  initializeHighScore(highScore) {\n    this.syncHighestScore = true;\n    highScore = Math.ceil(highScore);\n    if (highScore < this.highestScore) {\n      if (window.errorPageController) {\n        errorPageController.updateEasterEggHighScore(this.highestScore);\n      }\n      return;\n    }\n    this.highestScore = highScore;\n    this.distanceMeter.setHighScore(this.highestScore);\n  },\n\n  /**\n   * Sets the current high score and saves to the profile if available.\n   * @param {number} distanceRan Total distance ran.\n   * @param {boolean=} opt_resetScore Whether to reset the score.\n   */\n  saveHighScore(distanceRan, opt_resetScore) {\n    this.highestScore = Math.ceil(distanceRan);\n    this.distanceMeter.setHighScore(this.highestScore);\n\n    // Store the new high score in the profile.\n    if (this.syncHighestScore && window.errorPageController) {\n      if (opt_resetScore) {\n        errorPageController.resetEasterEggHighScore();\n      } else {\n        errorPageController.updateEasterEggHighScore(this.highestScore);\n      }\n    }\n  },\n\n  /**\n   * Game over state.\n   */\n  gameOver() {\n    this.playSound(this.soundFx.HIT);\n    vibrate(200);\n\n    this.stop();\n    this.crashed = true;\n    this.distanceMeter.achievement = false;\n\n    this.tRex.update(100, Trex.status.CRASHED);\n\n    // Game over panel.\n    if (!this.gameOverPanel) {\n      const origSpriteDef = IS_HIDPI ?\n          Runner.spriteDefinitionByType.original.HDPI :\n          Runner.spriteDefinitionByType.original.LDPI;\n\n      if (this.canvas) {\n        if (Runner.isAltGameModeEnabled) {\n          this.gameOverPanel = new GameOverPanel(\n              this.canvas, origSpriteDef.TEXT_SPRITE, origSpriteDef.RESTART,\n              this.dimensions, origSpriteDef.ALT_GAME_END,\n              this.altGameModeActive);\n        } else {\n          this.gameOverPanel = new GameOverPanel(\n              this.canvas, origSpriteDef.TEXT_SPRITE, origSpriteDef.RESTART,\n              this.dimensions);\n        }\n      }\n    }\n\n    this.gameOverPanel.draw(this.altGameModeActive, this.tRex);\n\n    // Update the high score.\n    if (this.distanceRan > this.highestScore) {\n      this.saveHighScore(this.distanceRan);\n    }\n\n    // Reset the time clock.\n    this.time = getTimeStamp();\n\n    if (Runner.audioCues) {\n      this.generatedSoundFx.stopAll();\n      announcePhrase(\n          getA11yString(A11Y_STRINGS.gameOver)\n              .replace(\n                  \'$1\',\n                  this.distanceMeter.getActualDistance(this.distanceRan)\n                      .toString()) +\n          \' \' +\n          getA11yString(A11Y_STRINGS.highScore)\n              .replace(\n                  \'$1\',\n\n                  this.distanceMeter.getActualDistance(this.highestScore)\n                      .toString()));\n      this.containerEl.setAttribute(\n          \'title\', getA11yString(A11Y_STRINGS.ariaLabel));\n    }\n    this.showSpeedToggle();\n    this.disableSpeedToggle(false);\n  },\n\n  stop() {\n    this.setPlayStatus(false);\n    this.paused = true;\n    cancelAnimationFrame(this.raqId);\n    this.raqId = 0;\n    this.generatedSoundFx.stopAll();\n  },\n\n  play() {\n    if (!this.crashed) {\n      this.setPlayStatus(true);\n      this.paused = false;\n      this.tRex.update(0, Trex.status.RUNNING);\n      this.time = getTimeStamp();\n      this.update();\n      this.generatedSoundFx.background();\n    }\n  },\n\n  restart() {\n    if (!this.raqId) {\n      this.playCount++;\n      this.runningTime = 0;\n      this.setPlayStatus(true);\n      this.toggleSpeed();\n      this.paused = false;\n      this.crashed = false;\n      this.distanceRan = 0;\n      this.setSpeed(this.config.SPEED);\n      this.time = getTimeStamp();\n      this.containerEl.classList.remove(Runner.classes.CRASHED);\n      this.clearCanvas();\n      this.distanceMeter.reset();\n      this.horizon.reset();\n      this.tRex.reset();\n      this.playSound(this.soundFx.BUTTON_PRESS);\n      this.invert(true);\n      this.flashTimer = null;\n      this.update();\n      this.gameOverPanel.reset();\n      this.generatedSoundFx.background();\n      this.containerEl.setAttribute(\'title\', getA11yString(A11Y_STRINGS.jump));\n      announcePhrase(getA11yString(A11Y_STRINGS.started));\n    }\n  },\n\n  setPlayStatus(isPlaying) {\n    if (this.touchController) {\n      this.touchController.classList.toggle(HIDDEN_CLASS, !isPlaying);\n    }\n    this.playing = isPlaying;\n  },\n\n  /**\n   * Whether the game should go into arcade mode.\n   * @return {boolean}\n   */\n  isArcadeMode() {\n    // In RTL languages the title is wrapped with the left to right mark\n    // control characters &#x202A; and &#x202C but are invisible.\n    return IS_RTL ? document.title.indexOf(ARCADE_MODE_URL) == 1 :\n                    document.title === ARCADE_MODE_URL;\n  },\n\n  /**\n   * Hides offline messaging for a fullscreen game only experience.\n   */\n  setArcadeMode() {\n    document.body.classList.add(Runner.classes.ARCADE_MODE);\n    this.setArcadeModeContainerScale();\n  },\n\n  /**\n   * Sets the scaling for arcade mode.\n   */\n  setArcadeModeContainerScale() {\n    const windowHeight = window.innerHeight;\n    const scaleHeight = windowHeight / this.dimensions.HEIGHT;\n    const scaleWidth = window.innerWidth / this.dimensions.WIDTH;\n    const scale = Math.max(1, Math.min(scaleHeight, scaleWidth));\n    const scaledCanvasHeight = this.dimensions.HEIGHT * scale;\n    // Positions the game container at 10% of the available vertical window\n    // height minus the game container height.\n    const translateY = Math.ceil(Math.max(0, (windowHeight - scaledCanvasHeight -\n        Runner.config.ARCADE_MODE_INITIAL_TOP_POSITION) *\n        Runner.config.ARCADE_MODE_TOP_POSITION_PERCENT)) *\n        window.devicePixelRatio;\n\n    const cssScale = IS_RTL ? -scale + \',\' + scale : scale;\n    this.containerEl.style.transform =\n        \'scale(\' + cssScale + \') translateY(\' + translateY + \'px)\';\n  },\n\n  /**\n   * Pause the game if the tab is not in focus.\n   */\n  onVisibilityChange(e) {\n    if (document.hidden || document.webkitHidden || e.type === \'blur\' ||\n        document.visibilityState !== \'visible\') {\n      this.stop();\n    } else if (!this.crashed) {\n      this.tRex.reset();\n      this.play();\n    }\n  },\n\n  /**\n   * Play a sound.\n   * @param {AudioBuffer} soundBuffer\n   */\n  playSound(soundBuffer) {\n    if (soundBuffer) {\n      const sourceNode = this.audioContext.createBufferSource();\n      sourceNode.buffer = soundBuffer;\n      sourceNode.connect(this.audioContext.destination);\n      sourceNode.start(0);\n    }\n  },\n\n  /**\n   * Inverts the current page / canvas colors.\n   * @param {boolean} reset Whether to reset colors.\n   */\n  invert(reset) {\n    const htmlEl = document.firstElementChild;\n\n    if (reset) {\n      htmlEl.classList.toggle(Runner.classes.INVERTED,\n          false);\n      this.invertTimer = 0;\n      this.inverted = false;\n    } else {\n      this.inverted = htmlEl.classList.toggle(\n          Runner.classes.INVERTED, this.invertTrigger);\n    }\n  },\n};\n\n\n/**\n * Updates the canvas size taking into\n * account the backing store pixel ratio and\n * the device pixel ratio.\n *\n * See article by Paul Lewis:\n * http://www.html5rocks.com/en/tutorials/canvas/hidpi/\n *\n * @param {HTMLCanvasElement} canvas\n * @param {number=} opt_width\n * @param {number=} opt_height\n * @return {boolean} Whether the canvas was scaled.\n */\nRunner.updateCanvasScaling = function(canvas, opt_width, opt_height) {\n  const context =\n      /** @type {CanvasRenderingContext2D} */ (canvas.getContext(\'2d\'));\n\n  // Query the various pixel ratios\n  const devicePixelRatio = Math.floor(window.devicePixelRatio) || 1;\n  /** @suppress {missingProperties} */\n  const backingStoreRatio =\n      Math.floor(context.webkitBackingStorePixelRatio) || 1;\n  const ratio = devicePixelRatio / backingStoreRatio;\n\n  // Upscale the canvas if the two ratios don\'t match\n  if (devicePixelRatio !== backingStoreRatio) {\n    const oldWidth = opt_width || canvas.width;\n    const oldHeight = opt_height || canvas.height;\n\n    canvas.width = oldWidth * ratio;\n    canvas.height = oldHeight * ratio;\n\n    canvas.style.width = oldWidth + \'px\';\n    canvas.style.height = oldHeight + \'px\';\n\n    // Scale the context to counter the fact that we\'ve manually scaled\n    // our canvas element.\n    context.scale(ratio, ratio);\n    return true;\n  } else if (devicePixelRatio === 1) {\n    // Reset the canvas width / height. Fixes scaling bug when the page is\n    // zoomed and the devicePixelRatio changes accordingly.\n    canvas.style.width = canvas.width + \'px\';\n    canvas.style.height = canvas.height + \'px\';\n  }\n  return false;\n};\n\n\n/**\n * Whether events are enabled.\n * @return {boolean}\n */\nRunner.isAltGameModeEnabled = function() {\n  return loadTimeData && loadTimeData.valueExists(\'enableAltGameMode\');\n};\n\n\n/**\n * Generated sound FX class for audio cues.\n * @constructor\n */\nfunction GeneratedSoundFx() {\n  this.audioCues = false;\n  this.context = null;\n  this.panner = null;\n}\n\nGeneratedSoundFx.prototype = {\n  init() {\n    this.audioCues = true;\n    if (!this.context) {\n      // iOS only supports the webkit version.\n      this.context = window.webkitAudioContext ? new webkitAudioContext() :\n                                                 new AudioContext();\n      if (IS_IOS) {\n        this.context.onstatechange = (function() {\n                                       if (this.context.state != \'running\') {\n                                         this.context.resume();\n                                       }\n                                     }).bind(this);\n        this.context.resume();\n      }\n      this.panner = this.context.createStereoPanner ?\n          this.context.createStereoPanner() :\n          null;\n    }\n  },\n\n  stopAll() {\n    this.cancelFootSteps();\n  },\n\n  /**\n   * Play oscillators at certain frequency and for a certain time.\n   * @param {number} frequency\n   * @param {number} startTime\n   * @param {number} duration\n   * @param {?number=} opt_vol\n   * @param {number=} opt_pan\n   */\n  playNote(frequency, startTime, duration, opt_vol, opt_pan) {\n    const osc1 = this.context.createOscillator();\n    const osc2 = this.context.createOscillator();\n    const volume = this.context.createGain();\n\n    // Set oscillator wave type\n    osc1.type = \'triangle\';\n    osc2.type = \'triangle\';\n    volume.gain.value = 0.1;\n\n    // Set up node routing\n    if (this.panner) {\n      this.panner.pan.value = opt_pan || 0;\n      osc1.connect(volume).connect(this.panner);\n      osc2.connect(volume).connect(this.panner);\n      this.panner.connect(this.context.destination);\n    } else {\n      osc1.connect(volume);\n      osc2.connect(volume);\n      volume.connect(this.context.destination);\n    }\n\n    // Detune oscillators for chorus effect\n    osc1.frequency.value = frequency + 1;\n    osc2.frequency.value = frequency - 2;\n\n    // Fade out\n    volume.gain.setValueAtTime(opt_vol || 0.01, startTime + duration - 0.05);\n    volume.gain.linearRampToValueAtTime(0.00001, startTime + duration);\n\n    // Start oscillators\n    osc1.start(startTime);\n    osc2.start(startTime);\n    // Stop oscillators\n    osc1.stop(startTime + duration);\n    osc2.stop(startTime + duration);\n  },\n\n  background() {\n    if (this.audioCues) {\n      const now = this.context.currentTime;\n      this.playNote(493.883, now, 0.116);\n      this.playNote(659.255, now + 0.116, 0.232);\n      this.loopFootSteps();\n    }\n  },\n\n  loopFootSteps() {\n    if (this.audioCues && !this.bgSoundIntervalId) {\n      this.bgSoundIntervalId = setInterval(function() {\n        this.playNote(73.42, this.context.currentTime, 0.05, 0.16);\n        this.playNote(69.30, this.context.currentTime + 0.116, 0.116, 0.16);\n      }.bind(this), 280);\n    }\n  },\n\n  cancelFootSteps() {\n    if (this.audioCues && this.bgSoundIntervalId) {\n      clearInterval(this.bgSoundIntervalId);\n      this.bgSoundIntervalId = null;\n      this.playNote(103.83, this.context.currentTime, 0.232, 0.02);\n      this.playNote(116.54, this.context.currentTime + 0.116, 0.232, 0.02);\n    }\n  },\n\n  collect() {\n    if (this.audioCues) {\n      this.cancelFootSteps();\n      const now = this.context.currentTime;\n      this.playNote(830.61, now, 0.116);\n      this.playNote(1318.51, now + 0.116, 0.232);\n    }\n  },\n\n  jump() {\n    if (this.audioCues) {\n      const now = this.context.currentTime;\n      this.playNote(659.25, now, 0.116, 0.3, -0.6);\n      this.playNote(880, now + 0.116, 0.232, 0.3, -0.6);\n    }\n  },\n};\n\n\n/**\n * Speak a phrase using Speech Synthesis API for a11y.\n * @param {string} phrase Sentence to speak.\n */\nfunction speakPhrase(phrase) {\n  if (\'speechSynthesis\' in window) {\n    const msg = new SpeechSynthesisUtterance(phrase);\n    const voices = window.speechSynthesis.getVoices();\n    msg.text = phrase;\n    speechSynthesis.speak(msg);\n  }\n}\n\n\n/**\n * For screen readers make an announcement to the live region.\n * @param {string} phrase Sentence to speak.\n */\nfunction announcePhrase(phrase) {\n  if (Runner.a11yStatusEl) {\n    Runner.a11yStatusEl.textContent = \'\';\n    Runner.a11yStatusEl.textContent = phrase;\n  }\n}\n\n\n/**\n * Returns a string from loadTimeData data object.\n * @param {string} stringName\n * @return {string}\n */\nfunction getA11yString(stringName) {\n  return loadTimeData && loadTimeData.valueExists(stringName) ?\n      loadTimeData.getString(stringName) :\n      \'\';\n}\n\n\n/**\n * Get random number.\n * @param {number} min\n * @param {number} max\n */\nfunction getRandomNum(min, max) {\n  return Math.floor(Math.random() * (max - min + 1)) + min;\n}\n\n\n/**\n * Vibrate on mobile devices.\n * @param {number} duration Duration of the vibration in milliseconds.\n */\nfunction vibrate(duration) {\n  if (IS_MOBILE && window.navigator.vibrate) {\n    window.navigator.vibrate(duration);\n  }\n}\n\n\n/**\n * Create canvas element.\n * @param {Element} container Element to append canvas to.\n * @param {number} width\n * @param {number} height\n * @param {string=} opt_classname\n * @return {HTMLCanvasElement}\n */\nfunction createCanvas(container, width, height, opt_classname) {\n  const canvas =\n      /** @type {!HTMLCanvasElement} */ (document.createElement(\'canvas\'));\n  canvas.className = opt_classname ? Runner.classes.CANVAS + \' \' +\n      opt_classname : Runner.classes.CANVAS;\n  canvas.width = width;\n  canvas.height = height;\n  container.appendChild(canvas);\n\n  return canvas;\n}\n\n\n/**\n * Decodes the base 64 audio to ArrayBuffer used by Web Audio.\n * @param {string} base64String\n */\nfunction decodeBase64ToArrayBuffer(base64String) {\n  const len = (base64String.length / 4) * 3;\n  const str = atob(base64String);\n  const arrayBuffer = new ArrayBuffer(len);\n  const bytes = new Uint8Array(arrayBuffer);\n\n  for (let i = 0; i < len; i++) {\n    bytes[i] = str.charCodeAt(i);\n  }\n  return bytes.buffer;\n}\n\n\n/**\n * Return the current timestamp.\n * @return {number}\n */\nfunction getTimeStamp() {\n  return IS_IOS ? new Date().getTime() : performance.now();\n}\n\n\n//******************************************************************************\n\n\n/**\n * Game over panel.\n * @param {!HTMLCanvasElement} canvas\n * @param {Object} textImgPos\n * @param {Object} restartImgPos\n * @param {!Object} dimensions Canvas dimensions.\n * @param {Object=} opt_altGameEndImgPos\n * @param {boolean=} opt_altGameActive\n * @constructor\n */\nfunction GameOverPanel(\n    canvas, textImgPos, restartImgPos, dimensions, opt_altGameEndImgPos,\n    opt_altGameActive) {\n  this.canvas = canvas;\n  this.canvasCtx =\n      /** @type {CanvasRenderingContext2D} */ (canvas.getContext(\'2d\'));\n  this.canvasDimensions = dimensions;\n  this.textImgPos = textImgPos;\n  this.restartImgPos = restartImgPos;\n  this.altGameEndImgPos = opt_altGameEndImgPos;\n  this.altGameModeActive = opt_altGameActive;\n\n  // Retry animation.\n  this.frameTimeStamp = 0;\n  this.animTimer = 0;\n  this.currentFrame = 0;\n\n  this.gameOverRafId = null;\n\n  this.flashTimer = 0;\n  this.flashCounter = 0;\n  this.originalText = true;\n}\n\nGameOverPanel.RESTART_ANIM_DURATION = 875;\nGameOverPanel.LOGO_PAUSE_DURATION = 875;\nGameOverPanel.FLASH_ITERATIONS = 5;\n\n/**\n * Animation frames spec.\n */\nGameOverPanel.animConfig = {\n  frames: [0, 36, 72, 108, 144, 180, 216, 252],\n  msPerFrame: GameOverPanel.RESTART_ANIM_DURATION / 8,\n};\n\n/**\n * Dimensions used in the panel.\n * @enum {number}\n */\nGameOverPanel.dimensions = {\n  TEXT_X: 0,\n  TEXT_Y: 13,\n  TEXT_WIDTH: 191,\n  TEXT_HEIGHT: 11,\n  RESTART_WIDTH: 36,\n  RESTART_HEIGHT: 32,\n};\n\n\nGameOverPanel.prototype = {\n  /**\n   * Update the panel dimensions.\n   * @param {number} width New canvas width.\n   * @param {number} opt_height Optional new canvas height.\n   */\n  updateDimensions(width, opt_height) {\n    this.canvasDimensions.WIDTH = width;\n    if (opt_height) {\n      this.canvasDimensions.HEIGHT = opt_height;\n    }\n    this.currentFrame = GameOverPanel.animConfig.frames.length - 1;\n  },\n\n  drawGameOverText(dimensions, opt_useAltText) {\n    const centerX = this.canvasDimensions.WIDTH / 2;\n    let textSourceX = dimensions.TEXT_X;\n    let textSourceY = dimensions.TEXT_Y;\n    let textSourceWidth = dimensions.TEXT_WIDTH;\n    let textSourceHeight = dimensions.TEXT_HEIGHT;\n\n    const textTargetX = Math.round(centerX - (dimensions.TEXT_WIDTH / 2));\n    const textTargetY = Math.round((this.canvasDimensions.HEIGHT - 25) / 3);\n    const textTargetWidth = dimensions.TEXT_WIDTH;\n    const textTargetHeight = dimensions.TEXT_HEIGHT;\n\n    if (IS_HIDPI) {\n      textSourceY *= 2;\n      textSourceX *= 2;\n      textSourceWidth *= 2;\n      textSourceHeight *= 2;\n    }\n\n    if (!opt_useAltText) {\n      textSourceX += this.textImgPos.x;\n      textSourceY += this.textImgPos.y;\n    }\n\n    const spriteSource =\n        opt_useAltText ? Runner.altCommonImageSprite : Runner.origImageSprite;\n\n    this.canvasCtx.save();\n\n    if (IS_RTL) {\n      this.canvasCtx.translate(this.canvasDimensions.WIDTH, 0);\n      this.canvasCtx.scale(-1, 1);\n    }\n\n    // Game over text from sprite.\n    this.canvasCtx.drawImage(\n        spriteSource, textSourceX, textSourceY, textSourceWidth,\n        textSourceHeight, textTargetX, textTargetY, textTargetWidth,\n        textTargetHeight);\n\n    this.canvasCtx.restore();\n  },\n\n  /**\n   * Draw additional adornments for alternative game types.\n   */\n  drawAltGameElements(tRex) {\n    // Additional adornments.\n    if (this.altGameModeActive && Runner.spriteDefinition.ALT_GAME_END_CONFIG) {\n      const altGameEndConfig = Runner.spriteDefinition.ALT_GAME_END_CONFIG;\n\n      let altGameEndSourceWidth = altGameEndConfig.WIDTH;\n      let altGameEndSourceHeight = altGameEndConfig.HEIGHT;\n      const altGameEndTargetX = tRex.xPos + altGameEndConfig.X_OFFSET;\n      const altGameEndTargetY = tRex.yPos + altGameEndConfig.Y_OFFSET;\n\n      if (IS_HIDPI) {\n        altGameEndSourceWidth *= 2;\n        altGameEndSourceHeight *= 2;\n      }\n\n      this.canvasCtx.drawImage(\n          Runner.altCommonImageSprite, this.altGameEndImgPos.x,\n          this.altGameEndImgPos.y, altGameEndSourceWidth,\n          altGameEndSourceHeight, altGameEndTargetX, altGameEndTargetY,\n          altGameEndConfig.WIDTH, altGameEndConfig.HEIGHT);\n    }\n  },\n\n  /**\n   * Draw restart button.\n   */\n  drawRestartButton() {\n    const dimensions = GameOverPanel.dimensions;\n    let framePosX = GameOverPanel.animConfig.frames[this.currentFrame];\n    let restartSourceWidth = dimensions.RESTART_WIDTH;\n    let restartSourceHeight = dimensions.RESTART_HEIGHT;\n    const restartTargetX =\n        (this.canvasDimensions.WIDTH / 2) - (dimensions.RESTART_WIDTH / 2);\n    const restartTargetY = this.canvasDimensions.HEIGHT / 2;\n\n    if (IS_HIDPI) {\n      restartSourceWidth *= 2;\n      restartSourceHeight *= 2;\n      framePosX *= 2;\n    }\n\n    this.canvasCtx.save();\n\n    if (IS_RTL) {\n      this.canvasCtx.translate(this.canvasDimensions.WIDTH, 0);\n      this.canvasCtx.scale(-1, 1);\n    }\n\n    this.canvasCtx.drawImage(\n        Runner.origImageSprite, this.restartImgPos.x + framePosX,\n        this.restartImgPos.y, restartSourceWidth, restartSourceHeight,\n        restartTargetX, restartTargetY, dimensions.RESTART_WIDTH,\n        dimensions.RESTART_HEIGHT);\n    this.canvasCtx.restore();\n  },\n\n\n  /**\n   * Draw the panel.\n   * @param {boolean} opt_altGameModeActive\n   * @param {!Trex} opt_tRex\n   */\n  draw(opt_altGameModeActive, opt_tRex) {\n    if (opt_altGameModeActive) {\n      this.altGameModeActive = opt_altGameModeActive;\n    }\n\n    this.drawGameOverText(GameOverPanel.dimensions, false);\n    this.drawRestartButton();\n    this.drawAltGameElements(opt_tRex);\n    this.update();\n  },\n\n  /**\n   * Update animation frames.\n   */\n  update() {\n    const now = getTimeStamp();\n    const deltaTime = now - (this.frameTimeStamp || now);\n\n    this.frameTimeStamp = now;\n    this.animTimer += deltaTime;\n    this.flashTimer += deltaTime;\n\n    // Restart Button\n    if (this.currentFrame == 0 &&\n        this.animTimer > GameOverPanel.LOGO_PAUSE_DURATION) {\n      this.animTimer = 0;\n      this.currentFrame++;\n      this.drawRestartButton();\n    } else if (\n        this.currentFrame > 0 &&\n        this.currentFrame < GameOverPanel.animConfig.frames.length) {\n      if (this.animTimer >= GameOverPanel.animConfig.msPerFrame) {\n        this.currentFrame++;\n        this.drawRestartButton();\n      }\n    } else if (\n        !this.altGameModeActive &&\n        this.currentFrame == GameOverPanel.animConfig.frames.length) {\n      this.reset();\n      return;\n    }\n\n    // Game over text\n    if (this.altGameModeActive &&\n        Runner.spriteDefinitionByType.original.ALT_GAME_OVER_TEXT_CONFIG) {\n      const altTextConfig =\n          Runner.spriteDefinitionByType.original.ALT_GAME_OVER_TEXT_CONFIG;\n\n      if (this.flashCounter < GameOverPanel.FLASH_ITERATIONS &&\n          this.flashTimer > altTextConfig.FLASH_DURATION) {\n        this.flashTimer = 0;\n        this.originalText = !this.originalText;\n\n        this.clearGameOverTextBounds();\n        if (this.originalText) {\n          this.drawGameOverText(GameOverPanel.dimensions, false);\n          this.flashCounter++;\n        } else {\n          this.drawGameOverText(altTextConfig, true);\n        }\n      } else if (this.flashCounter >= GameOverPanel.FLASH_ITERATIONS) {\n        this.reset();\n        return;\n      }\n    }\n\n    this.gameOverRafId = requestAnimationFrame(this.update.bind(this));\n  },\n\n  /**\n   * Clear game over text.\n   */\n  clearGameOverTextBounds() {\n    this.canvasCtx.save();\n\n    this.canvasCtx.clearRect(\n        Math.round(\n            this.canvasDimensions.WIDTH / 2 -\n            (GameOverPanel.dimensions.TEXT_WIDTH / 2)),\n        Math.round((this.canvasDimensions.HEIGHT - 25) / 3),\n        GameOverPanel.dimensions.TEXT_WIDTH,\n        GameOverPanel.dimensions.TEXT_HEIGHT + 4);\n    this.canvasCtx.restore();\n  },\n\n  reset() {\n    if (this.gameOverRafId) {\n      cancelAnimationFrame(this.gameOverRafId);\n      this.gameOverRafId = null;\n    }\n    this.animTimer = 0;\n    this.frameTimeStamp = 0;\n    this.currentFrame = 0;\n    this.flashTimer = 0;\n    this.flashCounter = 0;\n    this.originalText = true;\n  },\n};\n\n\n//******************************************************************************\n\n/**\n * Check for a collision.\n * @param {!Obstacle} obstacle\n * @param {!Trex} tRex T-rex object.\n * @param {CanvasRenderingContext2D=} opt_canvasCtx Optional canvas context for\n *    drawing collision boxes.\n * @return {Array<CollisionBox>|undefined}\n */\nfunction checkForCollision(obstacle, tRex, opt_canvasCtx) {\n  const obstacleBoxXPos = Runner.defaultDimensions.WIDTH + obstacle.xPos;\n\n  // Adjustments are made to the bounding box as there is a 1 pixel white\n  // border around the t-rex and obstacles.\n  const tRexBox = new CollisionBox(\n      tRex.xPos + 1,\n      tRex.yPos + 1,\n      tRex.config.WIDTH - 2,\n      tRex.config.HEIGHT - 2);\n\n  const obstacleBox = new CollisionBox(\n      obstacle.xPos + 1,\n      obstacle.yPos + 1,\n      obstacle.typeConfig.width * obstacle.size - 2,\n      obstacle.typeConfig.height - 2);\n\n  // Debug outer box\n  if (opt_canvasCtx) {\n    drawCollisionBoxes(opt_canvasCtx, tRexBox, obstacleBox);\n  }\n\n  // Simple outer bounds check.\n  if (boxCompare(tRexBox, obstacleBox)) {\n    const collisionBoxes = obstacle.collisionBoxes;\n    let tRexCollisionBoxes = [];\n\n    if (Runner.isAltGameModeEnabled()) {\n      tRexCollisionBoxes = Runner.spriteDefinition.TREX.COLLISION_BOXES;\n    } else {\n      tRexCollisionBoxes = tRex.ducking ? Trex.collisionBoxes.DUCKING :\n                                          Trex.collisionBoxes.RUNNING;\n    }\n\n    // Detailed axis aligned box check.\n    for (let t = 0; t < tRexCollisionBoxes.length; t++) {\n      for (let i = 0; i < collisionBoxes.length; i++) {\n        // Adjust the box to actual positions.\n        const adjTrexBox =\n            createAdjustedCollisionBox(tRexCollisionBoxes[t], tRexBox);\n        const adjObstacleBox =\n            createAdjustedCollisionBox(collisionBoxes[i], obstacleBox);\n        const crashed = boxCompare(adjTrexBox, adjObstacleBox);\n\n        // Draw boxes for debug.\n        if (opt_canvasCtx) {\n          drawCollisionBoxes(opt_canvasCtx, adjTrexBox, adjObstacleBox);\n        }\n\n        if (crashed) {\n          return [adjTrexBox, adjObstacleBox];\n        }\n      }\n    }\n  }\n}\n\n\n/**\n * Adjust the collision box.\n * @param {!CollisionBox} box The original box.\n * @param {!CollisionBox} adjustment Adjustment box.\n * @return {CollisionBox} The adjusted collision box object.\n */\nfunction createAdjustedCollisionBox(box, adjustment) {\n  return new CollisionBox(\n      box.x + adjustment.x,\n      box.y + adjustment.y,\n      box.width,\n      box.height);\n}\n\n\n/**\n * Draw the collision boxes for debug.\n */\nfunction drawCollisionBoxes(canvasCtx, tRexBox, obstacleBox) {\n  canvasCtx.save();\n  canvasCtx.strokeStyle = \'#f00\';\n  canvasCtx.strokeRect(tRexBox.x, tRexBox.y, tRexBox.width, tRexBox.height);\n\n  canvasCtx.strokeStyle = \'#0f0\';\n  canvasCtx.strokeRect(obstacleBox.x, obstacleBox.y,\n      obstacleBox.width, obstacleBox.height);\n  canvasCtx.restore();\n}\n\n\n/**\n * Compare two collision boxes for a collision.\n * @param {CollisionBox} tRexBox\n * @param {CollisionBox} obstacleBox\n * @return {boolean} Whether the boxes intersected.\n */\nfunction boxCompare(tRexBox, obstacleBox) {\n  let crashed = false;\n  const tRexBoxX = tRexBox.x;\n  const tRexBoxY = tRexBox.y;\n\n  const obstacleBoxX = obstacleBox.x;\n  const obstacleBoxY = obstacleBox.y;\n\n  // Axis-Aligned Bounding Box method.\n  if (tRexBox.x < obstacleBoxX + obstacleBox.width &&\n      tRexBox.x + tRexBox.width > obstacleBoxX &&\n      tRexBox.y < obstacleBox.y + obstacleBox.height &&\n      tRexBox.height + tRexBox.y > obstacleBox.y) {\n    crashed = true;\n  }\n\n  return crashed;\n}\n\n\n//******************************************************************************\n\n/**\n * Collision box object.\n * @param {number} x X position.\n * @param {number} y Y Position.\n * @param {number} w Width.\n * @param {number} h Height.\n * @constructor\n */\nfunction CollisionBox(x, y, w, h) {\n  this.x = x;\n  this.y = y;\n  this.width = w;\n  this.height = h;\n}\n\n\n//******************************************************************************\n\n/**\n * Obstacle.\n * @param {CanvasRenderingContext2D} canvasCtx\n * @param {ObstacleType} type\n * @param {Object} spriteImgPos Obstacle position in sprite.\n * @param {Object} dimensions\n * @param {number} gapCoefficient Mutipler in determining the gap.\n * @param {number} speed\n * @param {number=} opt_xOffset\n * @param {boolean=} opt_isAltGameMode\n * @constructor\n */\nfunction Obstacle(\n    canvasCtx, type, spriteImgPos, dimensions, gapCoefficient, speed,\n    opt_xOffset, opt_isAltGameMode) {\n  this.canvasCtx = canvasCtx;\n  this.spritePos = spriteImgPos;\n  this.typeConfig = type;\n  this.gapCoefficient = Runner.slowDown ? gapCoefficient * 2 : gapCoefficient;\n  this.size = getRandomNum(1, Obstacle.MAX_OBSTACLE_LENGTH);\n  this.dimensions = dimensions;\n  this.remove = false;\n  this.xPos = dimensions.WIDTH + (opt_xOffset || 0);\n  this.yPos = 0;\n  this.width = 0;\n  this.collisionBoxes = [];\n  this.gap = 0;\n  this.speedOffset = 0;\n  this.altGameModeActive = opt_isAltGameMode;\n  this.imageSprite = this.typeConfig.type == \'COLLECTABLE\' ?\n      Runner.altCommonImageSprite :\n      this.altGameModeActive ? Runner.altGameImageSprite : Runner.imageSprite;\n\n  // For animated obstacles.\n  this.currentFrame = 0;\n  this.timer = 0;\n\n  this.init(speed);\n}\n\n/**\n * Coefficient for calculating the maximum gap.\n */\nObstacle.MAX_GAP_COEFFICIENT = 1.5;\n\n/**\n * Maximum obstacle grouping count.\n */\nObstacle.MAX_OBSTACLE_LENGTH = 3;\n\n\nObstacle.prototype = {\n  /**\n   * Initialise the DOM for the obstacle.\n   * @param {number} speed\n   */\n  init(speed) {\n    this.cloneCollisionBoxes();\n\n    // Only allow sizing if we\'re at the right speed.\n    if (this.size > 1 && this.typeConfig.multipleSpeed > speed) {\n      this.size = 1;\n    }\n\n    this.width = this.typeConfig.width * this.size;\n\n    // Check if obstacle can be positioned at various heights.\n    if (Array.isArray(this.typeConfig.yPos)) {\n      const yPosConfig =\n          IS_MOBILE ? this.typeConfig.yPosMobile : this.typeConfig.yPos;\n      this.yPos = yPosConfig[getRandomNum(0, yPosConfig.length - 1)];\n    } else {\n      this.yPos = this.typeConfig.yPos;\n    }\n\n    this.draw();\n\n    // Make collision box adjustments,\n    // Central box is adjusted to the size as one box.\n    //      ____        ______        ________\n    //    _|   |-|    _|     |-|    _|       |-|\n    //   | |<->| |   | |<--->| |   | |<----->| |\n    //   | | 1 | |   | |  2  | |   | |   3   | |\n    //   |_|___|_|   |_|_____|_|   |_|_______|_|\n    //\n    if (this.size > 1) {\n      this.collisionBoxes[1].width = this.width - this.collisionBoxes[0].width -\n          this.collisionBoxes[2].width;\n      this.collisionBoxes[2].x = this.width - this.collisionBoxes[2].width;\n    }\n\n    // For obstacles that go at a different speed from the horizon.\n    if (this.typeConfig.speedOffset) {\n      this.speedOffset = Math.random() > 0.5 ? this.typeConfig.speedOffset :\n                                               -this.typeConfig.speedOffset;\n    }\n\n    this.gap = this.getGap(this.gapCoefficient, speed);\n\n    // Increase gap for audio cues enabled.\n    if (Runner.audioCues) {\n      this.gap *= 2;\n    }\n  },\n\n  /**\n   * Draw and crop based on size.\n   */\n  draw() {\n    let sourceWidth = this.typeConfig.width;\n    let sourceHeight = this.typeConfig.height;\n\n    if (IS_HIDPI) {\n      sourceWidth = sourceWidth * 2;\n      sourceHeight = sourceHeight * 2;\n    }\n\n    // X position in sprite.\n    let sourceX =\n        (sourceWidth * this.size) * (0.5 * (this.size - 1)) + this.spritePos.x;\n\n    // Animation frames.\n    if (this.currentFrame > 0) {\n      sourceX += sourceWidth * this.currentFrame;\n    }\n\n    this.canvasCtx.drawImage(\n        this.imageSprite, sourceX, this.spritePos.y, sourceWidth * this.size,\n        sourceHeight, this.xPos, this.yPos, this.typeConfig.width * this.size,\n        this.typeConfig.height);\n  },\n\n  /**\n   * Obstacle frame update.\n   * @param {number} deltaTime\n   * @param {number} speed\n   */\n  update(deltaTime, speed) {\n    if (!this.remove) {\n      if (this.typeConfig.speedOffset) {\n        speed += this.speedOffset;\n      }\n      this.xPos -= Math.floor((speed * FPS / 1000) * deltaTime);\n\n      // Update frame\n      if (this.typeConfig.numFrames) {\n        this.timer += deltaTime;\n        if (this.timer >= this.typeConfig.frameRate) {\n          this.currentFrame =\n              this.currentFrame === this.typeConfig.numFrames - 1 ?\n              0 :\n              this.currentFrame + 1;\n          this.timer = 0;\n        }\n      }\n      this.draw();\n\n      if (!this.isVisible()) {\n        this.remove = true;\n      }\n    }\n  },\n\n  /**\n   * Calculate a random gap size.\n   * - Minimum gap gets wider as speed increses\n   * @param {number} gapCoefficient\n   * @param {number} speed\n   * @return {number} The gap size.\n   */\n  getGap(gapCoefficient, speed) {\n    const minGap = Math.round(\n        this.width * speed + this.typeConfig.minGap * gapCoefficient);\n    const maxGap = Math.round(minGap * Obstacle.MAX_GAP_COEFFICIENT);\n    return getRandomNum(minGap, maxGap);\n  },\n\n  /**\n   * Check if obstacle is visible.\n   * @return {boolean} Whether the obstacle is in the game area.\n   */\n  isVisible() {\n    return this.xPos + this.width > 0;\n  },\n\n  /**\n   * Make a copy of the collision boxes, since these will change based on\n   * obstacle type and size.\n   */\n  cloneCollisionBoxes() {\n    const collisionBoxes = this.typeConfig.collisionBoxes;\n\n    for (let i = collisionBoxes.length - 1; i >= 0; i--) {\n      this.collisionBoxes[i] = new CollisionBox(\n          collisionBoxes[i].x, collisionBoxes[i].y, collisionBoxes[i].width,\n          collisionBoxes[i].height);\n    }\n  },\n};\n\n\n//******************************************************************************\n/**\n * T-rex game character.\n * @param {HTMLCanvasElement} canvas\n * @param {Object} spritePos Positioning within image sprite.\n * @constructor\n */\nfunction Trex(canvas, spritePos) {\n  this.canvas = canvas;\n  this.canvasCtx =\n      /** @type {CanvasRenderingContext2D} */ (canvas.getContext(\'2d\'));\n  this.spritePos = spritePos;\n  this.xPos = 0;\n  this.yPos = 0;\n  this.xInitialPos = 0;\n  // Position when on the ground.\n  this.groundYPos = 0;\n  this.currentFrame = 0;\n  this.currentAnimFrames = [];\n  this.blinkDelay = 0;\n  this.blinkCount = 0;\n  this.animStartTime = 0;\n  this.timer = 0;\n  this.msPerFrame = 1000 / FPS;\n  this.config = Object.assign(Trex.config, Trex.normalJumpConfig);\n  // Current status.\n  this.status = Trex.status.WAITING;\n  this.jumping = false;\n  this.ducking = false;\n  this.jumpVelocity = 0;\n  this.reachedMinHeight = false;\n  this.speedDrop = false;\n  this.jumpCount = 0;\n  this.jumpspotX = 0;\n  this.altGameModeEnabled = false;\n  this.flashing = false;\n\n  this.init();\n}\n\n\n/**\n * T-rex player config.\n */\nTrex.config = {\n  DROP_VELOCITY: -5,\n  FLASH_OFF: 175,\n  FLASH_ON: 100,\n  HEIGHT: 47,\n  HEIGHT_DUCK: 25,\n  INTRO_DURATION: 1500,\n  SPEED_DROP_COEFFICIENT: 3,\n  SPRITE_WIDTH: 262,\n  START_X_POS: 50,\n  WIDTH: 44,\n  WIDTH_DUCK: 59,\n};\n\nTrex.slowJumpConfig = {\n  GRAVITY: 0.25,\n  MAX_JUMP_HEIGHT: 50,\n  MIN_JUMP_HEIGHT: 45,\n  INITIAL_JUMP_VELOCITY: -20,\n};\n\nTrex.normalJumpConfig = {\n  GRAVITY: 0.6,\n  MAX_JUMP_HEIGHT: 30,\n  MIN_JUMP_HEIGHT: 30,\n  INITIAL_JUMP_VELOCITY: -10,\n};\n\n/**\n * Used in collision detection.\n * @enum {Array<CollisionBox>}\n */\nTrex.collisionBoxes = {\n  DUCKING: [new CollisionBox(1, 18, 55, 25)],\n  RUNNING: [\n    new CollisionBox(22, 0, 17, 16),\n    new CollisionBox(1, 18, 30, 9),\n    new CollisionBox(10, 35, 14, 8),\n    new CollisionBox(1, 24, 29, 5),\n    new CollisionBox(5, 30, 21, 4),\n    new CollisionBox(9, 34, 15, 4),\n  ],\n};\n\n\n/**\n * Animation states.\n * @enum {string}\n */\nTrex.status = {\n  CRASHED: \'CRASHED\',\n  DUCKING: \'DUCKING\',\n  JUMPING: \'JUMPING\',\n  RUNNING: \'RUNNING\',\n  WAITING: \'WAITING\',\n};\n\n/**\n * Blinking coefficient.\n * @const\n */\nTrex.BLINK_TIMING = 7000;\n\n\n/**\n * Animation config for different states.\n * @enum {Object}\n */\nTrex.animFrames = {\n  WAITING: {\n    frames: [44, 0],\n    msPerFrame: 1000 / 3,\n  },\n  RUNNING: {\n    frames: [88, 132],\n    msPerFrame: 1000 / 12,\n  },\n  CRASHED: {\n    frames: [220],\n    msPerFrame: 1000 / 60,\n  },\n  JUMPING: {\n    frames: [0],\n    msPerFrame: 1000 / 60,\n  },\n  DUCKING: {\n    frames: [264, 323],\n    msPerFrame: 1000 / 8,\n  },\n};\n\n\nTrex.prototype = {\n  /**\n   * T-rex player initaliser.\n   * Sets the t-rex to blink at random intervals.\n   */\n  init() {\n    this.groundYPos = Runner.defaultDimensions.HEIGHT - this.config.HEIGHT -\n        Runner.config.BOTTOM_PAD;\n    this.yPos = this.groundYPos;\n    this.minJumpHeight = this.groundYPos - this.config.MIN_JUMP_HEIGHT;\n\n    this.draw(0, 0);\n    this.update(0, Trex.status.WAITING);\n  },\n\n  /**\n   * Assign the appropriate jump parameters based on the game speed.\n   */\n  enableSlowConfig: function() {\n    const jumpConfig =\n        Runner.slowDown ? Trex.slowJumpConfig : Trex.normalJumpConfig;\n    Trex.config = Object.assign(Trex.config, jumpConfig);\n\n    this.adjustAltGameConfigForSlowSpeed();\n  },\n\n  /**\n   * Enables the alternative game. Redefines the dino config.\n   * @param {Object} spritePos New positioning within image sprite.\n   */\n  enableAltGameMode: function(spritePos) {\n    this.altGameModeEnabled = true;\n    this.spritePos = spritePos;\n    const spriteDefinition = Runner.spriteDefinition[\'TREX\'];\n\n    // Update animation frames.\n    Trex.animFrames.RUNNING.frames =\n        [spriteDefinition.RUNNING_1.x, spriteDefinition.RUNNING_2.x];\n    Trex.animFrames.CRASHED.frames = [spriteDefinition.CRASHED.x];\n\n    if (typeof spriteDefinition.JUMPING.x == \'object\') {\n      Trex.animFrames.JUMPING.frames = spriteDefinition.JUMPING.x;\n    } else {\n      Trex.animFrames.JUMPING.frames = [spriteDefinition.JUMPING.x];\n    }\n\n    Trex.animFrames.DUCKING.frames =\n        [spriteDefinition.RUNNING_1.x, spriteDefinition.RUNNING_2.x];\n\n    // Update Trex config\n    Trex.config.GRAVITY = spriteDefinition.GRAVITY || Trex.config.GRAVITY;\n    Trex.config.HEIGHT = spriteDefinition.RUNNING_1.h,\n    Trex.config.INITIAL_JUMP_VELOCITY = spriteDefinition.INITIAL_JUMP_VELOCITY;\n    Trex.config.MAX_JUMP_HEIGHT = spriteDefinition.MAX_JUMP_HEIGHT;\n    Trex.config.MIN_JUMP_HEIGHT = spriteDefinition.MIN_JUMP_HEIGHT;\n    Trex.config.WIDTH = spriteDefinition.RUNNING_1.w;\n    Trex.config.WIDTH_JUMP = spriteDefinition.JUMPING.w;\n    Trex.config.INVERT_JUMP = spriteDefinition.INVERT_JUMP;\n\n    this.adjustAltGameConfigForSlowSpeed(spriteDefinition.GRAVITY);\n    this.config = Trex.config;\n\n    // Adjust bottom horizon placement.\n    this.groundYPos = Runner.defaultDimensions.HEIGHT - this.config.HEIGHT -\n        Runner.spriteDefinition[\'BOTTOM_PAD\'];\n    this.yPos = this.groundYPos;\n    this.reset();\n  },\n\n  /**\n   * Slow speeds adjustments for the alt game modes.\n   * @param {number=} opt_gravityValue\n   */\n  adjustAltGameConfigForSlowSpeed: function(opt_gravityValue) {\n    if (Runner.slowDown) {\n      if (opt_gravityValue) {\n        Trex.config.GRAVITY = opt_gravityValue / 1.5;\n      }\n      Trex.config.MIN_JUMP_HEIGHT *= 1.5;\n      Trex.config.MAX_JUMP_HEIGHT *= 1.5;\n      Trex.config.INITIAL_JUMP_VELOCITY =\n          Trex.config.INITIAL_JUMP_VELOCITY * 1.5;\n    }\n  },\n\n  /**\n   * Setter whether dino is flashing.\n   * @param {boolean} status\n   */\n  setFlashing: function(status) {\n    this.flashing = status;\n  },\n\n  /**\n   * Setter for the jump velocity.\n   * The approriate drop velocity is also set.\n   * @param {number} setting\n   */\n  setJumpVelocity(setting) {\n    this.config.INITIAL_JUMP_VELOCITY = -setting;\n    this.config.DROP_VELOCITY = -setting / 2;\n  },\n\n  /**\n   * Set the animation status.\n   * @param {!number} deltaTime\n   * @param {Trex.status=} opt_status Optional status to switch to.\n   */\n  update(deltaTime, opt_status) {\n    this.timer += deltaTime;\n\n    // Update the status.\n    if (opt_status) {\n      this.status = opt_status;\n      this.currentFrame = 0;\n      this.msPerFrame = Trex.animFrames[opt_status].msPerFrame;\n      this.currentAnimFrames = Trex.animFrames[opt_status].frames;\n\n      if (opt_status === Trex.status.WAITING) {\n        this.animStartTime = getTimeStamp();\n        this.setBlinkDelay();\n      }\n    }\n    // Game intro animation, T-rex moves in from the left.\n    if (this.playingIntro && this.xPos < this.config.START_X_POS) {\n      this.xPos += Math.round((this.config.START_X_POS /\n          this.config.INTRO_DURATION) * deltaTime);\n      this.xInitialPos = this.xPos;\n    }\n\n    if (this.status === Trex.status.WAITING) {\n      this.blink(getTimeStamp());\n    } else {\n      this.draw(this.currentAnimFrames[this.currentFrame], 0);\n    }\n\n    // Update the frame position.\n    if (!this.flashing && this.timer >= this.msPerFrame) {\n      this.currentFrame = this.currentFrame ==\n          this.currentAnimFrames.length - 1 ? 0 : this.currentFrame + 1;\n      this.timer = 0;\n    }\n\n    if (!this.altGameModeEnabled) {\n      // Speed drop becomes duck if the down key is still being pressed.\n      if (this.speedDrop && this.yPos === this.groundYPos) {\n        this.speedDrop = false;\n        this.setDuck(true);\n      }\n    }\n  },\n\n  /**\n   * Draw the t-rex to a particular position.\n   * @param {number} x\n   * @param {number} y\n   */\n  draw(x, y) {\n    let sourceX = x;\n    let sourceY = y;\n    let sourceWidth = this.ducking && this.status !== Trex.status.CRASHED ?\n        this.config.WIDTH_DUCK :\n        this.config.WIDTH;\n    let sourceHeight = this.config.HEIGHT;\n    const outputHeight = sourceHeight;\n\n    let jumpOffset = Runner.spriteDefinition.TREX.JUMPING.xOffset;\n\n    // Width of sprite changes on jump.\n    if (this.altGameModeEnabled && this.jumping &&\n        this.status !== Trex.status.CRASHED) {\n      sourceWidth = this.config.WIDTH_JUMP;\n    }\n\n    if (IS_HIDPI) {\n      sourceX *= 2;\n      sourceY *= 2;\n      sourceWidth *= 2;\n      sourceHeight *= 2;\n      jumpOffset *= 2;\n    }\n\n    // Adjustments for sprite sheet position.\n    sourceX += this.spritePos.x;\n    sourceY += this.spritePos.y;\n\n    // Flashing.\n    if (this.flashing) {\n      if (this.timer < this.config.FLASH_ON) {\n        this.canvasCtx.globalAlpha = 0.5;\n      } else if (this.timer > this.config.FLASH_OFF) {\n        this.timer = 0;\n      }\n    }\n\n    // Ducking.\n    if (!this.altGameModeEnabled && this.ducking &&\n        this.status !== Trex.status.CRASHED) {\n      this.canvasCtx.drawImage(Runner.imageSprite, sourceX, sourceY,\n          sourceWidth, sourceHeight,\n          this.xPos, this.yPos,\n          this.config.WIDTH_DUCK, outputHeight);\n    } else if (\n        this.altGameModeEnabled && this.jumping &&\n        this.status !== Trex.status.CRASHED) {\n      // Jumping with adjustments.\n      this.canvasCtx.drawImage(\n          Runner.imageSprite, sourceX, sourceY, sourceWidth, sourceHeight,\n          this.xPos - jumpOffset, this.yPos, this.config.WIDTH_JUMP,\n          outputHeight);\n    } else {\n      // Crashed whilst ducking. Trex is standing up so needs adjustment.\n      if (this.ducking && this.status === Trex.status.CRASHED) {\n        this.xPos++;\n      }\n      // Standing / running\n      this.canvasCtx.drawImage(Runner.imageSprite, sourceX, sourceY,\n          sourceWidth, sourceHeight,\n          this.xPos, this.yPos,\n          this.config.WIDTH, outputHeight);\n    }\n    this.canvasCtx.globalAlpha = 1;\n  },\n\n  /**\n   * Sets a random time for the blink to happen.\n   */\n  setBlinkDelay() {\n    this.blinkDelay = Math.ceil(Math.random() * Trex.BLINK_TIMING);\n  },\n\n  /**\n   * Make t-rex blink at random intervals.\n   * @param {number} time Current time in milliseconds.\n   */\n  blink(time) {\n    const deltaTime = time - this.animStartTime;\n\n    if (deltaTime >= this.blinkDelay) {\n      this.draw(this.currentAnimFrames[this.currentFrame], 0);\n\n      if (this.currentFrame === 1) {\n        // Set new random delay to blink.\n        this.setBlinkDelay();\n        this.animStartTime = time;\n        this.blinkCount++;\n      }\n    }\n  },\n\n  /**\n   * Initialise a jump.\n   * @param {number} speed\n   */\n  startJump(speed) {\n    if (!this.jumping) {\n      this.update(0, Trex.status.JUMPING);\n      // Tweak the jump velocity based on the speed.\n      this.jumpVelocity = this.config.INITIAL_JUMP_VELOCITY - (speed / 10);\n      this.jumping = true;\n      this.reachedMinHeight = false;\n      this.speedDrop = false;\n\n      if (this.config.INVERT_JUMP) {\n        this.minJumpHeight = this.groundYPos + this.config.MIN_JUMP_HEIGHT;\n      }\n    }\n  },\n\n  /**\n   * Jump is complete, falling down.\n   */\n  endJump() {\n    if (this.reachedMinHeight &&\n        this.jumpVelocity < this.config.DROP_VELOCITY) {\n      this.jumpVelocity = this.config.DROP_VELOCITY;\n    }\n  },\n\n  /**\n   * Update frame for a jump.\n   * @param {number} deltaTime\n   */\n  updateJump(deltaTime) {\n    const msPerFrame = Trex.animFrames[this.status].msPerFrame;\n    const framesElapsed = deltaTime / msPerFrame;\n\n    // Speed drop makes Trex fall faster.\n    if (this.speedDrop) {\n      this.yPos += Math.round(this.jumpVelocity *\n          this.config.SPEED_DROP_COEFFICIENT * framesElapsed);\n    } else if (this.config.INVERT_JUMP) {\n      this.yPos -= Math.round(this.jumpVelocity * framesElapsed);\n    } else {\n      this.yPos += Math.round(this.jumpVelocity * framesElapsed);\n    }\n\n    this.jumpVelocity += this.config.GRAVITY * framesElapsed;\n\n    // Minimum height has been reached.\n    if (this.config.INVERT_JUMP && (this.yPos > this.minJumpHeight) ||\n        !this.config.INVERT_JUMP && (this.yPos < this.minJumpHeight) ||\n        this.speedDrop) {\n      this.reachedMinHeight = true;\n    }\n\n    // Reached max height.\n    if (this.config.INVERT_JUMP && (this.yPos > -this.config.MAX_JUMP_HEIGHT) ||\n        !this.config.INVERT_JUMP && (this.yPos < this.config.MAX_JUMP_HEIGHT) ||\n        this.speedDrop) {\n      this.endJump();\n    }\n\n    // Back down at ground level. Jump completed.\n    if ((this.config.INVERT_JUMP && this.yPos) < this.groundYPos ||\n        (!this.config.INVERT_JUMP && this.yPos) > this.groundYPos) {\n      this.reset();\n      this.jumpCount++;\n\n      if (Runner.audioCues) {\n        Runner.generatedSoundFx.loopFootSteps();\n      }\n    }\n  },\n\n  /**\n   * Set the speed drop. Immediately cancels the current jump.\n   */\n  setSpeedDrop() {\n    this.speedDrop = true;\n    this.jumpVelocity = 1;\n  },\n\n  /**\n   * @param {boolean} isDucking\n   */\n  setDuck(isDucking) {\n    if (isDucking && this.status !== Trex.status.DUCKING) {\n      this.update(0, Trex.status.DUCKING);\n      this.ducking = true;\n    } else if (this.status === Trex.status.DUCKING) {\n      this.update(0, Trex.status.RUNNING);\n      this.ducking = false;\n    }\n  },\n\n  /**\n   * Reset the t-rex to running at start of game.\n   */\n  reset() {\n    this.xPos = this.xInitialPos;\n    this.yPos = this.groundYPos;\n    this.jumpVelocity = 0;\n    this.jumping = false;\n    this.ducking = false;\n    this.update(0, Trex.status.RUNNING);\n    this.midair = false;\n    this.speedDrop = false;\n    this.jumpCount = 0;\n  },\n};\n\n\n//******************************************************************************\n\n/**\n * Handles displaying the distance meter.\n * @param {!HTMLCanvasElement} canvas\n * @param {Object} spritePos Image position in sprite.\n * @param {number} canvasWidth\n * @constructor\n */\nfunction DistanceMeter(canvas, spritePos, canvasWidth) {\n  this.canvas = canvas;\n  this.canvasCtx =\n      /** @type {CanvasRenderingContext2D} */ (canvas.getContext(\'2d\'));\n  this.image = Runner.imageSprite;\n  this.spritePos = spritePos;\n  this.x = 0;\n  this.y = 5;\n\n  this.currentDistance = 0;\n  this.maxScore = 0;\n  this.highScore = \'0\';\n  this.container = null;\n\n  this.digits = [];\n  this.achievement = false;\n  this.defaultString = \'\';\n  this.flashTimer = 0;\n  this.flashIterations = 0;\n  this.invertTrigger = false;\n  this.flashingRafId = null;\n  this.highScoreBounds = {};\n  this.highScoreFlashing = false;\n\n  this.config = DistanceMeter.config;\n  this.maxScoreUnits = this.config.MAX_DISTANCE_UNITS;\n  this.canvasWidth = canvasWidth;\n  this.init(canvasWidth);\n}\n\n\n/**\n * @enum {number}\n */\nDistanceMeter.dimensions = {\n  WIDTH: 10,\n  HEIGHT: 13,\n  DEST_WIDTH: 11,\n};\n\n\n/**\n * Y positioning of the digits in the sprite sheet.\n * X position is always 0.\n * @type {Array<number>}\n */\nDistanceMeter.yPos = [0, 13, 27, 40, 53, 67, 80, 93, 107, 120];\n\n\n/**\n * Distance meter config.\n * @enum {number}\n */\nDistanceMeter.config = {\n  // Number of digits.\n  MAX_DISTANCE_UNITS: 5,\n\n  // Distance that causes achievement animation.\n  ACHIEVEMENT_DISTANCE: 100,\n\n  // Used for conversion from pixel distance to a scaled unit.\n  COEFFICIENT: 0.025,\n\n  // Flash duration in milliseconds.\n  FLASH_DURATION: 1000 / 4,\n\n  // Flash iterations for achievement animation.\n  FLASH_ITERATIONS: 3,\n\n  // Padding around the high score hit area.\n  HIGH_SCORE_HIT_AREA_PADDING: 4,\n};\n\n\nDistanceMeter.prototype = {\n  /**\n   * Initialise the distance meter to \'00000\'.\n   * @param {number} width Canvas width in px.\n   */\n  init(width) {\n    let maxDistanceStr = \'\';\n\n    this.calcXPos(width);\n    this.maxScore = this.maxScoreUnits;\n    for (let i = 0; i < this.maxScoreUnits; i++) {\n      this.draw(i, 0);\n      this.defaultString += \'0\';\n      maxDistanceStr += \'9\';\n    }\n\n    this.maxScore = parseInt(maxDistanceStr, 10);\n  },\n\n  /**\n   * Calculate the xPos in the canvas.\n   * @param {number} canvasWidth\n   */\n  calcXPos(canvasWidth) {\n    this.x = canvasWidth - (DistanceMeter.dimensions.DEST_WIDTH *\n        (this.maxScoreUnits + 1));\n  },\n\n  /**\n   * Draw a digit to canvas.\n   * @param {number} digitPos Position of the digit.\n   * @param {number} value Digit value 0-9.\n   * @param {boolean=} opt_highScore Whether drawing the high score.\n   */\n  draw(digitPos, value, opt_highScore) {\n    let sourceWidth = DistanceMeter.dimensions.WIDTH;\n    let sourceHeight = DistanceMeter.dimensions.HEIGHT;\n    let sourceX = DistanceMeter.dimensions.WIDTH * value;\n    let sourceY = 0;\n\n    const targetX = digitPos * DistanceMeter.dimensions.DEST_WIDTH;\n    const targetY = this.y;\n    const targetWidth = DistanceMeter.dimensions.WIDTH;\n    const targetHeight = DistanceMeter.dimensions.HEIGHT;\n\n    // For high DPI we 2x source values.\n    if (IS_HIDPI) {\n      sourceWidth *= 2;\n      sourceHeight *= 2;\n      sourceX *= 2;\n    }\n\n    sourceX += this.spritePos.x;\n    sourceY += this.spritePos.y;\n\n    this.canvasCtx.save();\n\n    if (IS_RTL) {\n      if (opt_highScore) {\n        this.canvasCtx.translate(\n            this.canvasWidth -\n                (DistanceMeter.dimensions.WIDTH * (this.maxScoreUnits + 3)),\n            this.y);\n      } else {\n        this.canvasCtx.translate(\n            this.canvasWidth - DistanceMeter.dimensions.WIDTH, this.y);\n      }\n      this.canvasCtx.scale(-1, 1);\n    } else {\n      const highScoreX =\n          this.x - (this.maxScoreUnits * 2) * DistanceMeter.dimensions.WIDTH;\n      if (opt_highScore) {\n        this.canvasCtx.translate(highScoreX, this.y);\n      } else {\n        this.canvasCtx.translate(this.x, this.y);\n      }\n    }\n\n    this.canvasCtx.drawImage(\n        this.image,\n        sourceX,\n        sourceY,\n        sourceWidth,\n        sourceHeight,\n        targetX,\n        targetY,\n        targetWidth,\n        targetHeight,\n    );\n\n    this.canvasCtx.restore();\n  },\n\n  /**\n   * Covert pixel distance to a \'real\' distance.\n   * @param {number} distance Pixel distance ran.\n   * @return {number} The \'real\' distance ran.\n   */\n  getActualDistance(distance) {\n    return distance ? Math.round(distance * this.config.COEFFICIENT) : 0;\n  },\n\n  /**\n   * Update the distance meter.\n   * @param {number} distance\n   * @param {number} deltaTime\n   * @return {boolean} Whether the acheivement sound fx should be played.\n   */\n  update(deltaTime, distance) {\n    let paint = true;\n    let playSound = false;\n\n    if (!this.achievement) {\n      distance = this.getActualDistance(distance);\n      // Score has gone beyond the initial digit count.\n      if (distance > this.maxScore && this.maxScoreUnits ==\n        this.config.MAX_DISTANCE_UNITS) {\n        this.maxScoreUnits++;\n        this.maxScore = parseInt(this.maxScore + \'9\', 10);\n      } else {\n        this.distance = 0;\n      }\n\n      if (distance > 0) {\n        // Achievement unlocked.\n        if (distance % this.config.ACHIEVEMENT_DISTANCE === 0) {\n          // Flash score and play sound.\n          this.achievement = true;\n          this.flashTimer = 0;\n          playSound = true;\n        }\n\n        // Create a string representation of the distance with leading 0.\n        const distanceStr = (this.defaultString +\n            distance).substr(-this.maxScoreUnits);\n        this.digits = distanceStr.split(\'\');\n      } else {\n        this.digits = this.defaultString.split(\'\');\n      }\n    } else {\n      // Control flashing of the score on reaching acheivement.\n      if (this.flashIterations <= this.config.FLASH_ITERATIONS) {\n        this.flashTimer += deltaTime;\n\n        if (this.flashTimer < this.config.FLASH_DURATION) {\n          paint = false;\n        } else if (this.flashTimer > this.config.FLASH_DURATION * 2) {\n          this.flashTimer = 0;\n          this.flashIterations++;\n        }\n      } else {\n        this.achievement = false;\n        this.flashIterations = 0;\n        this.flashTimer = 0;\n      }\n    }\n\n    // Draw the digits if not flashing.\n    if (paint) {\n      for (let i = this.digits.length - 1; i >= 0; i--) {\n        this.draw(i, parseInt(this.digits[i], 10));\n      }\n    }\n\n    this.drawHighScore();\n    return playSound;\n  },\n\n  /**\n   * Draw the high score.\n   */\n  drawHighScore() {\n    if (parseInt(this.highScore, 10) > 0) {\n      this.canvasCtx.save();\n      this.canvasCtx.globalAlpha = .8;\n      for (let i = this.highScore.length - 1; i >= 0; i--) {\n        this.draw(i, parseInt(this.highScore[i], 10), true);\n      }\n      this.canvasCtx.restore();\n    }\n  },\n\n  /**\n   * Set the highscore as a array string.\n   * Position of char in the sprite: H - 10, I - 11.\n   * @param {number} distance Distance ran in pixels.\n   */\n  setHighScore(distance) {\n    distance = this.getActualDistance(distance);\n    const highScoreStr = (this.defaultString +\n        distance).substr(-this.maxScoreUnits);\n\n    this.highScore = [\'10\', \'11\', \'\'].concat(highScoreStr.split(\'\'));\n  },\n\n\n  /**\n   * Whether a clicked is in the high score area.\n   * @param {Event} e Event object.\n   * @return {boolean} Whether the click was in the high score bounds.\n   */\n  hasClickedOnHighScore(e) {\n    let x = 0;\n    let y = 0;\n\n    if (e.touches) {\n      // Bounds for touch differ from pointer.\n      const canvasBounds = this.canvas.getBoundingClientRect();\n      x = e.touches[0].clientX - canvasBounds.left;\n      y = e.touches[0].clientY - canvasBounds.top;\n    } else {\n      x = e.offsetX;\n      y = e.offsetY;\n    }\n\n    this.highScoreBounds = this.getHighScoreBounds();\n    return x >= this.highScoreBounds.x && x <=\n        this.highScoreBounds.x + this.highScoreBounds.width &&\n        y >= this.highScoreBounds.y && y <=\n        this.highScoreBounds.y + this.highScoreBounds.height;\n  },\n\n  /**\n   * Get the bounding box for the high score.\n   * @return {Object} Object with x, y, width and height properties.\n   */\n  getHighScoreBounds() {\n    return {\n      x: (this.x - (this.maxScoreUnits * 2) * DistanceMeter.dimensions.WIDTH) -\n          DistanceMeter.config.HIGH_SCORE_HIT_AREA_PADDING,\n      y: this.y,\n      width: DistanceMeter.dimensions.WIDTH * (this.highScore.length + 1) +\n          DistanceMeter.config.HIGH_SCORE_HIT_AREA_PADDING,\n      height: DistanceMeter.dimensions.HEIGHT +\n          (DistanceMeter.config.HIGH_SCORE_HIT_AREA_PADDING * 2),\n    };\n  },\n\n  /**\n   * Animate flashing the high score to indicate ready for resetting.\n   * The flashing stops following this.config.FLASH_ITERATIONS x 2 flashes.\n   */\n  flashHighScore() {\n    const now = getTimeStamp();\n    const deltaTime = now - (this.frameTimeStamp || now);\n    let paint = true;\n    this.frameTimeStamp = now;\n\n    // Reached the max number of flashes.\n    if (this.flashIterations > this.config.FLASH_ITERATIONS * 2) {\n      this.cancelHighScoreFlashing();\n      return;\n    }\n\n    this.flashTimer += deltaTime;\n\n    if (this.flashTimer < this.config.FLASH_DURATION) {\n      paint = false;\n    } else if (this.flashTimer > this.config.FLASH_DURATION * 2) {\n      this.flashTimer = 0;\n      this.flashIterations++;\n    }\n\n    if (paint) {\n      this.drawHighScore();\n    } else {\n      this.clearHighScoreBounds();\n    }\n    // Frame update.\n    this.flashingRafId =\n        requestAnimationFrame(this.flashHighScore.bind(this));\n  },\n\n  /**\n   * Draw empty rectangle over high score.\n   */\n  clearHighScoreBounds() {\n    this.canvasCtx.save();\n    this.canvasCtx.fillStyle = \'#fff\';\n    this.canvasCtx.rect(this.highScoreBounds.x, this.highScoreBounds.y,\n        this.highScoreBounds.width, this.highScoreBounds.height);\n    this.canvasCtx.fill();\n    this.canvasCtx.restore();\n  },\n\n  /**\n   * Starts the flashing of the high score.\n   */\n  startHighScoreFlashing() {\n    this.highScoreFlashing = true;\n    this.flashHighScore();\n  },\n\n  /**\n   * Whether high score is flashing.\n   * @return {boolean}\n   */\n  isHighScoreFlashing() {\n    return this.highScoreFlashing;\n  },\n\n  /**\n   * Stop flashing the high score.\n   */\n  cancelHighScoreFlashing() {\n    if (this.flashingRafId) {\n      cancelAnimationFrame(this.flashingRafId);\n    }\n    this.flashIterations = 0;\n    this.flashTimer = 0;\n    this.highScoreFlashing = false;\n    this.clearHighScoreBounds();\n    this.drawHighScore();\n  },\n\n  /**\n   * Clear the high score.\n   */\n  resetHighScore() {\n    this.setHighScore(0);\n    this.cancelHighScoreFlashing();\n  },\n\n  /**\n   * Reset the distance meter back to \'00000\'.\n   */\n  reset() {\n    this.update(0, 0);\n    this.achievement = false;\n  },\n};\n\n\n//******************************************************************************\n\n/**\n * Cloud background item.\n * Similar to an obstacle object but without collision boxes.\n * @param {HTMLCanvasElement} canvas Canvas element.\n * @param {Object} spritePos Position of image in sprite.\n * @param {number} containerWidth\n * @constructor\n */\nfunction Cloud(canvas, spritePos, containerWidth) {\n  this.canvas = canvas;\n  this.canvasCtx =\n      /** @type {CanvasRenderingContext2D} */ (this.canvas.getContext(\'2d\'));\n  this.spritePos = spritePos;\n  this.containerWidth = containerWidth;\n  this.xPos = containerWidth;\n  this.yPos = 0;\n  this.remove = false;\n  this.gap =\n      getRandomNum(Cloud.config.MIN_CLOUD_GAP, Cloud.config.MAX_CLOUD_GAP);\n\n  this.init();\n}\n\n\n/**\n * Cloud object config.\n * @enum {number}\n */\nCloud.config = {\n  HEIGHT: 14,\n  MAX_CLOUD_GAP: 400,\n  MAX_SKY_LEVEL: 30,\n  MIN_CLOUD_GAP: 100,\n  MIN_SKY_LEVEL: 71,\n  WIDTH: 46,\n};\n\n\nCloud.prototype = {\n  /**\n   * Initialise the cloud. Sets the Cloud height.\n   */\n  init() {\n    this.yPos = getRandomNum(Cloud.config.MAX_SKY_LEVEL,\n        Cloud.config.MIN_SKY_LEVEL);\n    this.draw();\n  },\n\n  /**\n   * Draw the cloud.\n   */\n  draw() {\n    this.canvasCtx.save();\n    let sourceWidth = Cloud.config.WIDTH;\n    let sourceHeight = Cloud.config.HEIGHT;\n    const outputWidth = sourceWidth;\n    const outputHeight = sourceHeight;\n    if (IS_HIDPI) {\n      sourceWidth = sourceWidth * 2;\n      sourceHeight = sourceHeight * 2;\n    }\n\n    this.canvasCtx.drawImage(Runner.imageSprite, this.spritePos.x,\n        this.spritePos.y,\n        sourceWidth, sourceHeight,\n        this.xPos, this.yPos,\n        outputWidth, outputHeight);\n\n    this.canvasCtx.restore();\n  },\n\n  /**\n   * Update the cloud position.\n   * @param {number} speed\n   */\n  update(speed) {\n    if (!this.remove) {\n      this.xPos -= Math.ceil(speed);\n      this.draw();\n\n      // Mark as removeable if no longer in the canvas.\n      if (!this.isVisible()) {\n        this.remove = true;\n      }\n    }\n  },\n\n  /**\n   * Check if the cloud is visible on the stage.\n   * @return {boolean}\n   */\n  isVisible() {\n    return this.xPos + Cloud.config.WIDTH > 0;\n  },\n};\n\n\n/**\n * Background item.\n * Similar to cloud, without random y position.\n * @param {HTMLCanvasElement} canvas Canvas element.\n * @param {Object} spritePos Position of image in sprite.\n * @param {number} containerWidth\n * @param {string} type Element type.\n * @constructor\n */\nfunction BackgroundEl(canvas, spritePos, containerWidth, type) {\n  this.canvas = canvas;\n  this.canvasCtx =\n      /** @type {CanvasRenderingContext2D} */ (this.canvas.getContext(\'2d\'));\n  this.spritePos = spritePos;\n  this.containerWidth = containerWidth;\n  this.xPos = containerWidth;\n  this.yPos = 0;\n  this.remove = false;\n  this.type = type;\n  this.gap =\n      getRandomNum(BackgroundEl.config.MIN_GAP, BackgroundEl.config.MAX_GAP);\n  this.animTimer = 0;\n  this.switchFrames = false;\n\n  this.spriteConfig = {};\n  this.init();\n}\n\n/**\n * Background element object config.\n * Real values assigned when game type changes.\n * @enum {number}\n */\nBackgroundEl.config = {\n  MAX_BG_ELS: 0,\n  MAX_GAP: 0,\n  MIN_GAP: 0,\n  POS: 0,\n  SPEED: 0,\n  Y_POS: 0,\n  MS_PER_FRAME: 0,  // only needed when BACKGROUND_EL.FIXED is true\n};\n\n\nBackgroundEl.prototype = {\n  /**\n   * Initialise the element setting the y position.\n   */\n  init() {\n    this.spriteConfig = Runner.spriteDefinition.BACKGROUND_EL[this.type];\n    if (this.spriteConfig.FIXED) {\n      this.xPos = this.spriteConfig.FIXED_X_POS;\n    }\n    this.yPos = BackgroundEl.config.Y_POS - this.spriteConfig.HEIGHT +\n        this.spriteConfig.OFFSET;\n    this.draw();\n  },\n\n  /**\n   * Draw the element.\n   */\n  draw() {\n    this.canvasCtx.save();\n    let sourceWidth = this.spriteConfig.WIDTH;\n    let sourceHeight = this.spriteConfig.HEIGHT;\n    let sourceX = this.spriteConfig.X_POS;\n    const outputWidth = sourceWidth;\n    const outputHeight = sourceHeight;\n\n    if (IS_HIDPI) {\n      sourceWidth *= 2;\n      sourceHeight *= 2;\n      sourceX *= 2;\n    }\n\n    this.canvasCtx.drawImage(\n        Runner.imageSprite, sourceX, this.spritePos.y, sourceWidth,\n        sourceHeight, this.xPos, this.yPos, outputWidth, outputHeight);\n\n    this.canvasCtx.restore();\n  },\n\n  /**\n   * Update the background element position.\n   * @param {number} speed\n   */\n  update(speed) {\n    if (!this.remove) {\n      if (this.spriteConfig.FIXED) {\n        this.animTimer += speed;\n        if (this.animTimer > BackgroundEl.config.MS_PER_FRAME) {\n          this.animTimer = 0;\n          this.switchFrames = !this.switchFrames;\n        }\n\n        if (this.spriteConfig.FIXED_Y_POS_1 &&\n            this.spriteConfig.FIXED_Y_POS_2) {\n          this.yPos = this.switchFrames ? this.spriteConfig.FIXED_Y_POS_1 :\n                                          this.spriteConfig.FIXED_Y_POS_2;\n        }\n      } else {\n        // Fixed speed, regardless of actual game speed.\n        this.xPos -= BackgroundEl.config.SPEED;\n      }\n      this.draw();\n\n      // Mark as removable if no longer in the canvas.\n      if (!this.isVisible()) {\n        this.remove = true;\n      }\n    }\n  },\n\n  /**\n   * Check if the element is visible on the stage.\n   * @return {boolean}\n   */\n  isVisible() {\n    return this.xPos + this.spriteConfig.WIDTH > 0;\n  },\n};\n\n\n\n//******************************************************************************\n\n/**\n * Nightmode shows a moon and stars on the horizon.\n * @param {HTMLCanvasElement} canvas\n * @param {number} spritePos\n * @param {number} containerWidth\n * @constructor\n */\nfunction NightMode(canvas, spritePos, containerWidth) {\n  this.spritePos = spritePos;\n  this.canvas = canvas;\n  this.canvasCtx =\n      /** @type {CanvasRenderingContext2D} */ (canvas.getContext(\'2d\'));\n  this.xPos = containerWidth - 50;\n  this.yPos = 30;\n  this.currentPhase = 0;\n  this.opacity = 0;\n  this.containerWidth = containerWidth;\n  this.stars = [];\n  this.drawStars = false;\n  this.placeStars();\n}\n\n/**\n * @enum {number}\n */\nNightMode.config = {\n  FADE_SPEED: 0.035,\n  HEIGHT: 40,\n  MOON_SPEED: 0.25,\n  NUM_STARS: 2,\n  STAR_SIZE: 9,\n  STAR_SPEED: 0.3,\n  STAR_MAX_Y: 70,\n  WIDTH: 20,\n};\n\nNightMode.phases = [140, 120, 100, 60, 40, 20, 0];\n\nNightMode.prototype = {\n  /**\n   * Update moving moon, changing phases.\n   * @param {boolean} activated Whether night mode is activated.\n   */\n  update(activated) {\n    // Moon phase.\n    if (activated && this.opacity === 0) {\n      this.currentPhase++;\n\n      if (this.currentPhase >= NightMode.phases.length) {\n        this.currentPhase = 0;\n      }\n    }\n\n    // Fade in / out.\n    if (activated && (this.opacity < 1 || this.opacity === 0)) {\n      this.opacity += NightMode.config.FADE_SPEED;\n    } else if (this.opacity > 0) {\n      this.opacity -= NightMode.config.FADE_SPEED;\n    }\n\n    // Set moon positioning.\n    if (this.opacity > 0) {\n      this.xPos = this.updateXPos(this.xPos, NightMode.config.MOON_SPEED);\n\n      // Update stars.\n      if (this.drawStars) {\n        for (let i = 0; i < NightMode.config.NUM_STARS; i++) {\n          this.stars[i].x =\n              this.updateXPos(this.stars[i].x, NightMode.config.STAR_SPEED);\n        }\n      }\n      this.draw();\n    } else {\n      this.opacity = 0;\n      this.placeStars();\n    }\n    this.drawStars = true;\n  },\n\n  updateXPos(currentPos, speed) {\n    if (currentPos < -NightMode.config.WIDTH) {\n      currentPos = this.containerWidth;\n    } else {\n      currentPos -= speed;\n    }\n    return currentPos;\n  },\n\n  draw() {\n    let moonSourceWidth = this.currentPhase === 3 ? NightMode.config.WIDTH * 2 :\n                                                    NightMode.config.WIDTH;\n    let moonSourceHeight = NightMode.config.HEIGHT;\n    let moonSourceX = this.spritePos.x + NightMode.phases[this.currentPhase];\n    const moonOutputWidth = moonSourceWidth;\n    let starSize = NightMode.config.STAR_SIZE;\n    let starSourceX = Runner.spriteDefinitionByType.original.LDPI.STAR.x;\n\n    if (IS_HIDPI) {\n      moonSourceWidth *= 2;\n      moonSourceHeight *= 2;\n      moonSourceX = this.spritePos.x +\n          (NightMode.phases[this.currentPhase] * 2);\n      starSize *= 2;\n      starSourceX = Runner.spriteDefinitionByType.original.HDPI.STAR.x;\n    }\n\n    this.canvasCtx.save();\n    this.canvasCtx.globalAlpha = this.opacity;\n\n    // Stars.\n    if (this.drawStars) {\n      for (let i = 0; i < NightMode.config.NUM_STARS; i++) {\n        this.canvasCtx.drawImage(\n            Runner.origImageSprite, starSourceX, this.stars[i].sourceY,\n            starSize, starSize, Math.round(this.stars[i].x), this.stars[i].y,\n            NightMode.config.STAR_SIZE, NightMode.config.STAR_SIZE);\n      }\n    }\n\n    // Moon.\n    this.canvasCtx.drawImage(\n        Runner.origImageSprite, moonSourceX, this.spritePos.y, moonSourceWidth,\n        moonSourceHeight, Math.round(this.xPos), this.yPos, moonOutputWidth,\n        NightMode.config.HEIGHT);\n\n    this.canvasCtx.globalAlpha = 1;\n    this.canvasCtx.restore();\n  },\n\n  // Do star placement.\n  placeStars() {\n    const segmentSize = Math.round(this.containerWidth /\n        NightMode.config.NUM_STARS);\n\n    for (let i = 0; i < NightMode.config.NUM_STARS; i++) {\n      this.stars[i] = {};\n      this.stars[i].x = getRandomNum(segmentSize * i, segmentSize * (i + 1));\n      this.stars[i].y = getRandomNum(0, NightMode.config.STAR_MAX_Y);\n\n      if (IS_HIDPI) {\n        this.stars[i].sourceY =\n            Runner.spriteDefinitionByType.original.HDPI.STAR.y +\n            NightMode.config.STAR_SIZE * 2 * i;\n      } else {\n        this.stars[i].sourceY =\n            Runner.spriteDefinitionByType.original.LDPI.STAR.y +\n            NightMode.config.STAR_SIZE * i;\n      }\n    }\n  },\n\n  reset() {\n    this.currentPhase = 0;\n    this.opacity = 0;\n    this.update(false);\n  },\n\n};\n\n\n//******************************************************************************\n\n/**\n * Horizon Line.\n * Consists of two connecting lines. Randomly assigns a flat / bumpy horizon.\n * @param {HTMLCanvasElement} canvas\n * @param {Object} lineConfig Configuration object.\n * @constructor\n */\nfunction HorizonLine(canvas, lineConfig) {\n  let sourceX = lineConfig.SOURCE_X;\n  let sourceY = lineConfig.SOURCE_Y;\n\n  if (IS_HIDPI) {\n    sourceX *= 2;\n    sourceY *= 2;\n  }\n\n  this.spritePos = {x: sourceX, y: sourceY};\n  this.canvas = canvas;\n  this.canvasCtx =\n      /** @type {CanvasRenderingContext2D} */ (canvas.getContext(\'2d\'));\n  this.sourceDimensions = {};\n  this.dimensions = lineConfig;\n\n  this.sourceXPos = [this.spritePos.x, this.spritePos.x +\n      this.dimensions.WIDTH];\n  this.xPos = [];\n  this.yPos = 0;\n  this.bumpThreshold = 0.5;\n\n  this.setSourceDimensions(lineConfig);\n  this.draw();\n}\n\n\n/**\n * Horizon line dimensions.\n * @enum {number}\n */\nHorizonLine.dimensions = {\n  WIDTH: 600,\n  HEIGHT: 12,\n  YPOS: 127,\n};\n\n\nHorizonLine.prototype = {\n  /**\n   * Set the source dimensions of the horizon line.\n   */\n  setSourceDimensions(newDimensions) {\n    for (const dimension in newDimensions) {\n      if (dimension !== \'SOURCE_X\' && dimension !== \'SOURCE_Y\') {\n        if (IS_HIDPI) {\n          if (dimension !== \'YPOS\') {\n            this.sourceDimensions[dimension] = newDimensions[dimension] * 2;\n          }\n        } else {\n          this.sourceDimensions[dimension] = newDimensions[dimension];\n        }\n        this.dimensions[dimension] = newDimensions[dimension];\n      }\n    }\n\n    this.xPos = [0, newDimensions.WIDTH];\n    this.yPos = newDimensions.YPOS;\n  },\n\n  /**\n   * Return the crop x position of a type.\n   */\n  getRandomType() {\n    return Math.random() > this.bumpThreshold ? this.dimensions.WIDTH : 0;\n  },\n\n  /**\n   * Draw the horizon line.\n   */\n  draw() {\n    this.canvasCtx.drawImage(Runner.imageSprite, this.sourceXPos[0],\n        this.spritePos.y,\n        this.sourceDimensions.WIDTH, this.sourceDimensions.HEIGHT,\n        this.xPos[0], this.yPos,\n        this.dimensions.WIDTH, this.dimensions.HEIGHT);\n\n    this.canvasCtx.drawImage(Runner.imageSprite, this.sourceXPos[1],\n        this.spritePos.y,\n        this.sourceDimensions.WIDTH, this.sourceDimensions.HEIGHT,\n        this.xPos[1], this.yPos,\n        this.dimensions.WIDTH, this.dimensions.HEIGHT);\n  },\n\n  /**\n   * Update the x position of an indivdual piece of the line.\n   * @param {number} pos Line position.\n   * @param {number} increment\n   */\n  updateXPos(pos, increment) {\n    const line1 = pos;\n    const line2 = pos === 0 ? 1 : 0;\n\n    this.xPos[line1] -= increment;\n    this.xPos[line2] = this.xPos[line1] + this.dimensions.WIDTH;\n\n    if (this.xPos[line1] <= -this.dimensions.WIDTH) {\n      this.xPos[line1] += this.dimensions.WIDTH * 2;\n      this.xPos[line2] = this.xPos[line1] - this.dimensions.WIDTH;\n      this.sourceXPos[line1] = this.getRandomType() + this.spritePos.x;\n    }\n  },\n\n  /**\n   * Update the horizon line.\n   * @param {number} deltaTime\n   * @param {number} speed\n   */\n  update(deltaTime, speed) {\n    const increment = Math.floor(speed * (FPS / 1000) * deltaTime);\n\n    if (this.xPos[0] <= 0) {\n      this.updateXPos(0, increment);\n    } else {\n      this.updateXPos(1, increment);\n    }\n    this.draw();\n  },\n\n  /**\n   * Reset horizon to the starting position.\n   */\n  reset() {\n    this.xPos[0] = 0;\n    this.xPos[1] = this.dimensions.WIDTH;\n  },\n};\n\n\n//******************************************************************************\n\n/**\n * Horizon background class.\n * @param {HTMLCanvasElement} canvas\n * @param {Object} spritePos Sprite positioning.\n * @param {Object} dimensions Canvas dimensions.\n * @param {number} gapCoefficient\n * @constructor\n */\nfunction Horizon(canvas, spritePos, dimensions, gapCoefficient) {\n  this.canvas = canvas;\n  this.canvasCtx =\n      /** @type {CanvasRenderingContext2D} */ (this.canvas.getContext(\'2d\'));\n  this.config = Horizon.config;\n  this.dimensions = dimensions;\n  this.gapCoefficient = gapCoefficient;\n  this.obstacles = [];\n  this.obstacleHistory = [];\n  this.horizonOffsets = [0, 0];\n  this.cloudFrequency = this.config.CLOUD_FREQUENCY;\n  this.spritePos = spritePos;\n  this.nightMode = null;\n  this.altGameModeActive = false;\n\n  // Cloud\n  this.clouds = [];\n  this.cloudSpeed = this.config.BG_CLOUD_SPEED;\n\n  // Background elements\n  this.backgroundEls = [];\n  this.lastEl = null;\n  this.backgroundSpeed = this.config.BG_CLOUD_SPEED;\n\n  // Horizon\n  this.horizonLine = null;\n  this.horizonLines = [];\n  this.init();\n}\n\n\n/**\n * Horizon config.\n * @enum {number}\n */\nHorizon.config = {\n  BG_CLOUD_SPEED: 0.2,\n  BUMPY_THRESHOLD: .3,\n  CLOUD_FREQUENCY: .5,\n  HORIZON_HEIGHT: 16,\n  MAX_CLOUDS: 6,\n};\n\n\nHorizon.prototype = {\n  /**\n   * Initialise the horizon. Just add the line and a cloud. No obstacles.\n   */\n  init() {\n    Obstacle.types = Runner.spriteDefinitionByType.original.OBSTACLES;\n    this.addCloud();\n    // Multiple Horizon lines\n    for (let i = 0; i < Runner.spriteDefinition.LINES.length; i++) {\n      this.horizonLines.push(\n          new HorizonLine(this.canvas, Runner.spriteDefinition.LINES[i]));\n    }\n\n    this.nightMode = new NightMode(this.canvas, this.spritePos.MOON,\n        this.dimensions.WIDTH);\n  },\n\n  /**\n   * Update obstacle definitions based on the speed of the game.\n   */\n  adjustObstacleSpeed: function() {\n    for (let i = 0; i < Obstacle.types.length; i++) {\n      if (Runner.slowDown) {\n        Obstacle.types[i].multipleSpeed = Obstacle.types[i].multipleSpeed / 2;\n        Obstacle.types[i].minGap *= 1.5;\n        Obstacle.types[i].minSpeed = Obstacle.types[i].minSpeed / 2;\n\n        // Convert variable y position obstacles to fixed.\n        if (typeof (Obstacle.types[i].yPos) == \'object\') {\n          Obstacle.types[i].yPos = Obstacle.types[i].yPos[0];\n          Obstacle.types[i].yPosMobile = Obstacle.types[i].yPos[0];\n        }\n      }\n    }\n  },\n\n  /**\n   * Update sprites to correspond to change in sprite sheet.\n   * @param {number} spritePos\n   */\n  enableAltGameMode: function(spritePos) {\n    // Clear existing horizon objects.\n    this.clouds = [];\n    this.backgroundEls = [];\n\n    this.altGameModeActive = true;\n    this.spritePos = spritePos;\n\n    Obstacle.types = Runner.spriteDefinition.OBSTACLES;\n    this.adjustObstacleSpeed();\n\n    Obstacle.MAX_GAP_COEFFICIENT = Runner.spriteDefinition.MAX_GAP_COEFFICIENT;\n    Obstacle.MAX_OBSTACLE_LENGTH = Runner.spriteDefinition.MAX_OBSTACLE_LENGTH;\n\n    BackgroundEl.config = Runner.spriteDefinition.BACKGROUND_EL_CONFIG;\n\n    this.horizonLines = [];\n    for (let i = 0; i < Runner.spriteDefinition.LINES.length; i++) {\n      this.horizonLines.push(\n          new HorizonLine(this.canvas, Runner.spriteDefinition.LINES[i]));\n    }\n    this.reset();\n  },\n\n  /**\n   * @param {number} deltaTime\n   * @param {number} currentSpeed\n   * @param {boolean} updateObstacles Used as an override to prevent\n   *     the obstacles from being updated / added. This happens in the\n   *     ease in section.\n   * @param {boolean} showNightMode Night mode activated.\n   */\n  update(deltaTime, currentSpeed, updateObstacles, showNightMode) {\n    this.runningTime += deltaTime;\n\n    if (this.altGameModeActive) {\n      this.updateBackgroundEls(deltaTime, currentSpeed);\n    }\n\n    for (let i = 0; i < this.horizonLines.length; i++) {\n      this.horizonLines[i].update(deltaTime, currentSpeed);\n    }\n\n    if (!this.altGameModeActive || Runner.spriteDefinition.HAS_CLOUDS) {\n      this.nightMode.update(showNightMode);\n      this.updateClouds(deltaTime, currentSpeed);\n    }\n\n    if (updateObstacles) {\n      this.updateObstacles(deltaTime, currentSpeed);\n    }\n  },\n\n  /**\n   * Update background element positions. Also handles creating new elements.\n   * @param {number} elSpeed\n   * @param {Array<Object>} bgElArray\n   * @param {number} maxBgEl\n   * @param {Function} bgElAddFunction\n   * @param {number} frequency\n   */\n  updateBackgroundEl(elSpeed, bgElArray, maxBgEl, bgElAddFunction, frequency) {\n    const numElements = bgElArray.length;\n\n    if (numElements) {\n      for (let i = numElements - 1; i >= 0; i--) {\n        bgElArray[i].update(elSpeed);\n      }\n\n      const lastEl = bgElArray[numElements - 1];\n\n      // Check for adding a new element.\n      if (numElements < maxBgEl &&\n          (this.dimensions.WIDTH - lastEl.xPos) > lastEl.gap &&\n          frequency > Math.random()) {\n        bgElAddFunction();\n      }\n    } else {\n      bgElAddFunction();\n    }\n  },\n\n  /**\n   * Update the cloud positions.\n   * @param {number} deltaTime\n   * @param {number} speed\n   */\n  updateClouds(deltaTime, speed) {\n    const elSpeed = this.cloudSpeed / 1000 * deltaTime * speed;\n    this.updateBackgroundEl(\n        elSpeed, this.clouds, this.config.MAX_CLOUDS, this.addCloud.bind(this),\n        this.cloudFrequency);\n\n    // Remove expired elements.\n    this.clouds = this.clouds.filter((obj) => !obj.remove);\n  },\n\n  /**\n   * Update the background element positions.\n   * @param {number} deltaTime\n   * @param {number} speed\n   */\n  updateBackgroundEls(deltaTime, speed) {\n    this.updateBackgroundEl(\n        deltaTime, this.backgroundEls, BackgroundEl.config.MAX_BG_ELS,\n        this.addBackgroundEl.bind(this), this.cloudFrequency);\n\n    // Remove expired elements.\n    this.backgroundEls = this.backgroundEls.filter((obj) => !obj.remove);\n  },\n\n  /**\n   * Update the obstacle positions.\n   * @param {number} deltaTime\n   * @param {number} currentSpeed\n   */\n  updateObstacles(deltaTime, currentSpeed) {\n    const updatedObstacles = this.obstacles.slice(0);\n\n    for (let i = 0; i < this.obstacles.length; i++) {\n      const obstacle = this.obstacles[i];\n      obstacle.update(deltaTime, currentSpeed);\n\n      // Clean up existing obstacles.\n      if (obstacle.remove) {\n        updatedObstacles.shift();\n      }\n    }\n    this.obstacles = updatedObstacles;\n\n    if (this.obstacles.length > 0) {\n      const lastObstacle = this.obstacles[this.obstacles.length - 1];\n\n      if (lastObstacle && !lastObstacle.followingObstacleCreated &&\n          lastObstacle.isVisible() &&\n          (lastObstacle.xPos + lastObstacle.width + lastObstacle.gap) <\n          this.dimensions.WIDTH) {\n        this.addNewObstacle(currentSpeed);\n        lastObstacle.followingObstacleCreated = true;\n      }\n    } else {\n      // Create new obstacles.\n      this.addNewObstacle(currentSpeed);\n    }\n  },\n\n  removeFirstObstacle() {\n    this.obstacles.shift();\n  },\n\n  /**\n   * Add a new obstacle.\n   * @param {number} currentSpeed\n   */\n  addNewObstacle(currentSpeed) {\n    const obstacleCount =\n        Obstacle.types[Obstacle.types.length - 1].type != \'COLLECTABLE\' ||\n            (Runner.isAltGameModeEnabled() && !this.altGameModeActive ||\n             this.altGameModeActive) ?\n        Obstacle.types.length - 1 :\n        Obstacle.types.length - 2;\n    const obstacleTypeIndex =\n        obstacleCount > 0 ? getRandomNum(0, obstacleCount) : 0;\n    const obstacleType = Obstacle.types[obstacleTypeIndex];\n\n    // Check for multiples of the same type of obstacle.\n    // Also check obstacle is available at current speed.\n    if ((obstacleCount > 0 && this.duplicateObstacleCheck(obstacleType.type)) ||\n        currentSpeed < obstacleType.minSpeed) {\n      this.addNewObstacle(currentSpeed);\n    } else {\n      const obstacleSpritePos = this.spritePos[obstacleType.type];\n\n      this.obstacles.push(new Obstacle(\n          this.canvasCtx, obstacleType, obstacleSpritePos, this.dimensions,\n          this.gapCoefficient, currentSpeed, obstacleType.width,\n          this.altGameModeActive));\n\n      this.obstacleHistory.unshift(obstacleType.type);\n\n      if (this.obstacleHistory.length > 1) {\n        this.obstacleHistory.splice(Runner.config.MAX_OBSTACLE_DUPLICATION);\n      }\n    }\n  },\n\n  /**\n   * Returns whether the previous two obstacles are the same as the next one.\n   * Maximum duplication is set in config value MAX_OBSTACLE_DUPLICATION.\n   * @return {boolean}\n   */\n  duplicateObstacleCheck(nextObstacleType) {\n    let duplicateCount = 0;\n\n    for (let i = 0; i < this.obstacleHistory.length; i++) {\n      duplicateCount =\n          this.obstacleHistory[i] === nextObstacleType ? duplicateCount + 1 : 0;\n    }\n    return duplicateCount >= Runner.config.MAX_OBSTACLE_DUPLICATION;\n  },\n\n  /**\n   * Reset the horizon layer.\n   * Remove existing obstacles and reposition the horizon line.\n   */\n  reset() {\n    this.obstacles = [];\n    for (let l = 0; l < this.horizonLines.length; l++) {\n      this.horizonLines[l].reset();\n    }\n\n    this.nightMode.reset();\n  },\n\n  /**\n   * Update the canvas width and scaling.\n   * @param {number} width Canvas width.\n   * @param {number} height Canvas height.\n   */\n  resize(width, height) {\n    this.canvas.width = width;\n    this.canvas.height = height;\n  },\n\n  /**\n   * Add a new cloud to the horizon.\n   */\n  addCloud() {\n    this.clouds.push(new Cloud(this.canvas, this.spritePos.CLOUD,\n        this.dimensions.WIDTH));\n  },\n\n  /**\n   * Add a random background element to the horizon.\n   */\n  addBackgroundEl() {\n    const backgroundElTypes =\n        Object.keys(Runner.spriteDefinition.BACKGROUND_EL);\n\n    if (backgroundElTypes.length > 0) {\n      let index = getRandomNum(0, backgroundElTypes.length - 1);\n      let type = backgroundElTypes[index];\n\n      // Add variation if available.\n      while (type == this.lastEl && backgroundElTypes.length > 1) {\n        index = getRandomNum(0, backgroundElTypes.length - 1);\n        type = backgroundElTypes[index];\n      }\n\n      this.lastEl = type;\n      this.backgroundEls.push(new BackgroundEl(\n          this.canvas, this.spritePos.BACKGROUND_EL, this.dimensions.WIDTH,\n          type));\n    }\n  },\n};\n</script>\n  <script>// Copyright (c) 2021 The Chromium Authors. All rights reserved.\n// Use of this source code is governed by a BSD-style license that can be\n// found in the LICENSE file.\n\n/* @const\n * Add matching sprite definition and config to Runner.spriteDefinitionByType.\n */\nconst GAME_TYPE = [];\n\n/**\n * Obstacle definitions.\n * minGap: minimum pixel space between obstacles.\n * multipleSpeed: Speed at which multiples are allowed.\n * speedOffset: speed faster / slower than the horizon.\n * minSpeed: Minimum speed which the obstacle can make an appearance.\n *\n * @typedef {{\n *   type: string,\n *   width: number,\n *   height: number,\n *   yPos: number,\n *   multipleSpeed: number,\n *   minGap: number,\n *   minSpeed: number,\n *   collisionBoxes: Array<CollisionBox>,\n * }}\n */\nlet ObstacleType;\n\n/**\n * T-Rex runner sprite definitions.\n */\nRunner.spriteDefinitionByType = {\n  original: {\n    LDPI: {\n      BACKGROUND_EL: {x: 86, y: 2},\n      CACTUS_LARGE: {x: 332, y: 2},\n      CACTUS_SMALL: {x: 228, y: 2},\n      OBSTACLE_2: {x: 332, y: 2},\n      OBSTACLE: {x: 228, y: 2},\n      CLOUD: {x: 86, y: 2},\n      HORIZON: {x: 2, y: 54},\n      MOON: {x: 484, y: 2},\n      PTERODACTYL: {x: 134, y: 2},\n      RESTART: {x: 2, y: 68},\n      TEXT_SPRITE: {x: 655, y: 2},\n      TREX: {x: 848, y: 2},\n      STAR: {x: 645, y: 2},\n      COLLECTABLE: {x: 2, y: 2},\n      ALT_GAME_END: {x: 121, y: 2},\n    },\n    HDPI: {\n      BACKGROUND_EL: {x: 166, y: 2},\n      CACTUS_LARGE: {x: 652, y: 2},\n      CACTUS_SMALL: {x: 446, y: 2},\n      OBSTACLE_2: {x: 652, y: 2},\n      OBSTACLE: {x: 446, y: 2},\n      CLOUD: {x: 166, y: 2},\n      HORIZON: {x: 2, y: 104},\n      MOON: {x: 954, y: 2},\n      PTERODACTYL: {x: 260, y: 2},\n      RESTART: {x: 2, y: 130},\n      TEXT_SPRITE: {x: 1294, y: 2},\n      TREX: {x: 1678, y: 2},\n      STAR: {x: 1276, y: 2},\n      COLLECTABLE: {x: 4, y: 4},\n      ALT_GAME_END: {x: 242, y: 4},\n    },\n    MAX_GAP_COEFFICIENT: 1.5,\n    MAX_OBSTACLE_LENGTH: 3,\n    HAS_CLOUDS: 1,\n    BOTTOM_PAD: 10,\n    TREX: {\n      WAITING_1: {x: 44, w: 44, h: 47, xOffset: 0},\n      WAITING_2: {x: 0, w: 44, h: 47, xOffset: 0},\n      RUNNING_1: {x: 88, w: 44, h: 47, xOffset: 0},\n      RUNNING_2: {x: 132, w: 44, h: 47, xOffset: 0},\n      JUMPING: {x: 0, w: 44, h: 47, xOffset: 0},\n      CRASHED: {x: 220, w: 44, h: 47, xOffset: 0},\n      COLLISION_BOXES: [\n        new CollisionBox(22, 0, 17, 16),\n        new CollisionBox(1, 18, 30, 9),\n        new CollisionBox(10, 35, 14, 8),\n        new CollisionBox(1, 24, 29, 5),\n        new CollisionBox(5, 30, 21, 4),\n        new CollisionBox(9, 34, 15, 4),\n      ],\n    },\n    /** @type {Array<ObstacleType>} */\n    OBSTACLES: [\n      {\n        type: \'CACTUS_SMALL\',\n        width: 17,\n        height: 35,\n        yPos: 105,\n        multipleSpeed: 4,\n        minGap: 120,\n        minSpeed: 0,\n        collisionBoxes: [\n          new CollisionBox(0, 7, 5, 27),\n          new CollisionBox(4, 0, 6, 34),\n          new CollisionBox(10, 4, 7, 14),\n        ],\n      },\n      {\n        type: \'CACTUS_LARGE\',\n        width: 25,\n        height: 50,\n        yPos: 90,\n        multipleSpeed: 7,\n        minGap: 120,\n        minSpeed: 0,\n        collisionBoxes: [\n          new CollisionBox(0, 12, 7, 38),\n          new CollisionBox(8, 0, 7, 49),\n          new CollisionBox(13, 10, 10, 38),\n        ],\n      },\n      {\n        type: \'PTERODACTYL\',\n        width: 46,\n        height: 40,\n        yPos: [100, 75, 50],    // Variable height.\n        yPosMobile: [100, 50],  // Variable height mobile.\n        multipleSpeed: 999,\n        minSpeed: 8.5,\n        minGap: 150,\n        collisionBoxes: [\n          new CollisionBox(15, 15, 16, 5),\n          new CollisionBox(18, 21, 24, 6),\n          new CollisionBox(2, 14, 4, 3),\n          new CollisionBox(6, 10, 4, 7),\n          new CollisionBox(10, 8, 6, 9),\n        ],\n        numFrames: 2,\n        frameRate: 1000 / 6,\n        speedOffset: .8,\n      },\n    ],\n    BACKGROUND_EL: {\n      \'CLOUD\': {\n        HEIGHT: 14,\n        MAX_CLOUD_GAP: 400,\n        MAX_SKY_LEVEL: 30,\n        MIN_CLOUD_GAP: 100,\n        MIN_SKY_LEVEL: 71,\n        OFFSET: 4,\n        WIDTH: 46,\n        X_POS: 1,\n        Y_POS: 120,\n      },\n    },\n    BACKGROUND_EL_CONFIG: {\n      MAX_BG_ELS: 1,\n      MAX_GAP: 400,\n      MIN_GAP: 100,\n      POS: 0,\n      SPEED: 0.5,\n      Y_POS: 125,\n    },\n    LINES: [\n      {SOURCE_X: 2, SOURCE_Y: 52, WIDTH: 600, HEIGHT: 12, YPOS: 127},\n    ],\n  },\n};\n</script>\n  \n</head>\n<body id="t" class="neterror" style="font-family: system-ui, sans-serif; font-size: 75%" jstcache="0">\n  <div id="main-frame-error" class="interstitial-wrapper" jstcache="0">\n    <div id="main-content" jstcache="0">\n      <div class="icon icon-generic" jstcache="0"></div>\n      <div id="main-message" jstcache="0">\n        <h1 jstcache="0">\n          <span jsselect="heading" jsvalues=".innerHTML:msg" jstcache="9">This site can’t be reached</span>\n          <a id="error-information-button" class="hidden" onclick="toggleErrorInformationPopup();" jstcache="0"></a>\n        </h1>\n        <p jsselect="summary" jsvalues=".innerHTML:msg" jstcache="1"><strong jscontent="hostName" jstcache="22">openreview.net</strong> took too long to respond.</p>\n        <!--The suggestion list and error code are normally presented inline,\n          in which case error-information-popup-* divs have no effect. When\n          error-information-popup-container has the use-popup-container class, this\n          information is provided in a popup instead.-->\n        <div id="error-information-popup-container" jstcache="0">\n          <div id="error-information-popup" jstcache="0">\n            <div id="error-information-popup-box" jstcache="0">\n              <div id="error-information-popup-content" jstcache="0">\n                <div id="suggestions-list" style="" jsdisplay="(suggestionsSummaryList &amp;&amp; suggestionsSummaryList.length)" jstcache="16">\n                  <p jsvalues=".innerHTML:suggestionsSummaryListHeader" jstcache="18">Try:</p>\n                  <ul jsvalues=".className:suggestionsSummaryList.length == 1 ? \'single-suggestion\' : \'\'" jstcache="19" class="">\n                    <li jsselect="suggestionsSummaryList" jsvalues=".innerHTML:summary" jstcache="21" jsinstance="0">Checking the connection</li><li jsselect="suggestionsSummaryList" jsvalues=".innerHTML:summary" jstcache="21" jsinstance="*1"><a href="#buttons" onclick="toggleHelpBox()" jstcache="0">Checking the proxy and the firewall</a></li>\n                  </ul>\n                </div>\n                <div class="error-code" jscontent="errorCode" jstcache="17">ERR_TIMED_OUT</div>\n                <p id="error-information-popup-close" jstcache="0">\n                  <a class="link-button" jscontent="closeDescriptionPopup" onclick="toggleErrorInformationPopup();" jstcache="20">null</a>\n                </p>\n              </div>\n            </div>\n          </div>\n        </div>\n        <div id="download-links-wrapper" class="hidden" jstcache="0">\n          <div id="download-link-wrapper" jstcache="0">\n            <a id="download-link" class="link-button" onclick="downloadButtonClick()" jsselect="downloadButton" jscontent="msg" jsvalues=".disabledText:disabledMsg" jstcache="6" style="display: none;">\n            </a>\n          </div>\n          <div id="download-link-clicked-wrapper" class="hidden" jstcache="0">\n            <div id="download-link-clicked" class="link-button" jsselect="downloadButton" jscontent="disabledMsg" jstcache="11" style="display: none;">\n            </div>\n          </div>\n        </div>\n        <div id="save-page-for-later-button" class="hidden" jstcache="0">\n          <a class="link-button" onclick="savePageLaterClick()" jsselect="savePageLater" jscontent="savePageMsg" jstcache="10" style="display: none;">\n          </a>\n        </div>\n        <div id="cancel-save-page-button" class="hidden" onclick="cancelSavePageClick()" jsselect="savePageLater" jsvalues=".innerHTML:cancelMsg" jstcache="4" style="display: none;">\n        </div>\n        <div id="offline-content-list" class="list-hidden" hidden="" jstcache="0">\n          <div id="offline-content-list-visibility-card" onclick="toggleOfflineContentListVisibility(true)" jstcache="0">\n            <div id="offline-content-list-title" jsselect="offlineContentList" jscontent="title" jstcache="12" style="display: none;">\n            </div>\n            <div jstcache="0">\n              <div id="offline-content-list-show-text" jsselect="offlineContentList" jscontent="showText" jstcache="14" style="display: none;">\n              </div>\n              <div id="offline-content-list-hide-text" jsselect="offlineContentList" jscontent="hideText" jstcache="15" style="display: none;">\n              </div>\n            </div>\n          </div>\n          <div id="offline-content-suggestions" jstcache="0"></div>\n          <div id="offline-content-list-action" jstcache="0">\n            <a class="link-button" onclick="launchDownloadsPage()" jsselect="offlineContentList" jscontent="actionText" jstcache="13" style="display: none;">\n            </a>\n          </div>\n        </div>\n      </div>\n    </div>\n    <div id="buttons" class="nav-wrapper suggested-right" jstcache="0">\n      <div id="control-buttons" jstcache="0">\n        <button id="reload-button" class="blue-button text-button" onclick="reloadButtonClick(this.url);" jsselect="reloadButton" jsvalues=".url:reloadUrl" jscontent="msg" jstcache="5">Reload</button>\n        <button id="download-button" class="blue-button text-button" onclick="downloadButtonClick()" jsselect="downloadButton" jscontent="msg" jsvalues=".disabledText:disabledMsg" jstcache="6" style="display: none;">\n        </button>\n      </div>\n      <button id="details-button" class="secondary-button text-button small-link" onclick="detailsButtonClick(); toggleHelpBox()" jscontent="details" jsdisplay="(suggestionsDetails &amp;&amp; suggestionsDetails.length > 0) || diagnose" jsvalues=".detailsText:details; .hideDetailsText:hideDetails;" jstcache="2">Details</button>\n    </div>\n    <div id="details" class="hidden" jstcache="0">\n      <div class="suggestions" jsselect="suggestionsDetails" jstcache="3" jsinstance="0">\n        <div class="suggestion-header" jsvalues=".innerHTML:header" jstcache="7">Check your Internet connection</div>\n        <div class="suggestion-body" jsvalues=".innerHTML:body" jstcache="8">Check any cables and reboot any routers, modems, or other network\n    devices you may be using.</div>\n      </div><div class="suggestions" jsselect="suggestionsDetails" jstcache="3" jsinstance="1">\n        <div class="suggestion-header" jsvalues=".innerHTML:header" jstcache="7">Allow Chrome to access the network in your firewall or antivirus\n          settings.</div>\n        <div class="suggestion-body" jsvalues=".innerHTML:body" jstcache="8">If it is already listed as a program allowed to access the network, try\n      removing it from the list and adding it again.</div>\n      </div><div class="suggestions" jsselect="suggestionsDetails" jstcache="3" jsinstance="*2">\n        <div class="suggestion-header" jsvalues=".innerHTML:header" jstcache="7">If you use a proxy server…</div>\n        <div class="suggestion-body" jsvalues=".innerHTML:body" jstcache="8">Check your proxy settings or contact your network administrator to\n      make sure the proxy server is working. If you don\'t believe you should\n      be using a proxy server:\n      Go to\n          Applications &gt; System Preferences &gt; Network &gt; Advanced &gt; Proxies\n          and deselect any proxies that have been selected.</div>\n      </div>\n    </div>\n  </div>\n  <div id="sub-frame-error" jstcache="0">\n    <!-- Show details when hovering over the icon, in case the details are\n         hidden because they\'re too large. -->\n    <div class="icon" jstcache="0"></div>\n    <div id="sub-frame-error-details" jsselect="summary" jsvalues=".innerHTML:msg" jstcache="1"><strong jscontent="hostName" jstcache="22">openreview.net</strong> took too long to respond.</div>\n  </div>\n\n  <div id="offline-resources" jstcache="0">\n    <img id="offline-resources-1x" src="data:image/png;base64,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" jstcache="0">\n    <img id="offline-resources-2x" src="data:image/png;base64,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" jstcache="0">\n    <template id="audio-resources" jstcache="0">\n      <audio id="offline-sound-press" src="data:audio/mpeg;base64,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"></audio>\n      <audio id="offline-sound-hit" src="data:audio/mpeg;base64,T2dnUwACAAAAAAAAAABVDxppAAAAABYzHfUBHgF2b3JiaXMAAAAAAkSsAAD/////AHcBAP////+4AU9nZ1MAAAAAAAAAAAAAVQ8aaQEAAAC9PVXbEEf//////////////////+IDdm9yYmlzNwAAAEFPOyBhb1R1ViBiNSBbMjAwNjEwMjRdIChiYXNlZCBvbiBYaXBoLk9yZydzIGxpYlZvcmJpcykAAAAAAQV2b3JiaXMlQkNWAQBAAAAkcxgqRqVzFoQQGkJQGeMcQs5r7BlCTBGCHDJMW8slc5AhpKBCiFsogdCQVQAAQAAAh0F4FISKQQghhCU9WJKDJz0IIYSIOXgUhGlBCCGEEEIIIYQQQgghhEU5aJKDJ0EIHYTjMDgMg+U4+ByERTlYEIMnQegghA9CuJqDrDkIIYQkNUhQgwY56ByEwiwoioLEMLgWhAQ1KIyC5DDI1IMLQoiag0k1+BqEZ0F4FoRpQQghhCRBSJCDBkHIGIRGQViSgwY5uBSEy0GoGoQqOQgfhCA0ZBUAkAAAoKIoiqIoChAasgoAyAAAEEBRFMdxHMmRHMmxHAsIDVkFAAABAAgAAKBIiqRIjuRIkiRZkiVZkiVZkuaJqizLsizLsizLMhAasgoASAAAUFEMRXEUBwgNWQUAZAAACKA4iqVYiqVoiueIjgiEhqwCAIAAAAQAABA0Q1M8R5REz1RV17Zt27Zt27Zt27Zt27ZtW5ZlGQgNWQUAQAAAENJpZqkGiDADGQZCQ1YBAAgAAIARijDEgNCQVQAAQAAAgBhKDqIJrTnfnOOgWQ6aSrE5HZxItXmSm4q5Oeecc87J5pwxzjnnnKKcWQyaCa0555zEoFkKmgmtOeecJ7F50JoqrTnnnHHO6WCcEcY555wmrXmQmo21OeecBa1pjppLsTnnnEi5eVKbS7U555xzzjnnnHPOOeec6sXpHJwTzjnnnKi9uZab0MU555xPxunenBDOOeecc84555xzzjnnnCA0ZBUAAAQAQBCGjWHcKQjS52ggRhFiGjLpQffoMAkag5xC6tHoaKSUOggllXFSSicIDVkFAAACAEAIIYUUUkghhRRSSCGFFGKIIYYYcsopp6CCSiqpqKKMMssss8wyyyyzzDrsrLMOOwwxxBBDK63EUlNtNdZYa+4555qDtFZaa621UkoppZRSCkJDVgEAIAAABEIGGWSQUUghhRRiiCmnnHIKKqiA0JBVAAAgAIAAAAAAT/Ic0REd0REd0REd0REd0fEczxElURIlURIt0zI101NFVXVl15Z1Wbd9W9iFXfd93fd93fh1YViWZVmWZVmWZVmWZVmWZVmWIDRkFQAAAgAAIIQQQkghhRRSSCnGGHPMOegklBAIDVkFAAACAAgAAABwFEdxHMmRHEmyJEvSJM3SLE/zNE8TPVEURdM0VdEVXVE3bVE2ZdM1XVM2XVVWbVeWbVu2dduXZdv3fd/3fd/3fd/3fd/3fV0HQkNWAQASAAA6kiMpkiIpkuM4jiRJQGjIKgBABgBAAACK4iiO4ziSJEmSJWmSZ3mWqJma6ZmeKqpAaMgqAAAQAEAAAAAAAACKpniKqXiKqHiO6IiSaJmWqKmaK8qm7Lqu67qu67qu67qu67qu67qu67qu67qu67qu67qu67qu67quC4SGrAIAJAAAdCRHciRHUiRFUiRHcoDQkFUAgAwAgAAAHMMxJEVyLMvSNE/zNE8TPdETPdNTRVd0gdCQVQAAIACAAAAAAAAADMmwFMvRHE0SJdVSLVVTLdVSRdVTVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVTdM0TRMIDVkJAJABAKAQW0utxdwJahxi0nLMJHROYhCqsQgiR7W3yjGlHMWeGoiUURJ7qihjiknMMbTQKSet1lI6hRSkmFMKFVIOWiA0ZIUAEJoB4HAcQLIsQLI0AAAAAAAAAJA0DdA8D7A8DwAAAAAAAAAkTQMsTwM0zwMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQNI0QPM8QPM8AAAAAAAAANA8D/BEEfBEEQAAAAAAAAAszwM80QM8UQQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAwNE0QPM8QPM8AAAAAAAAALA8D/BEEfA8EQAAAAAAAAA0zwM8UQQ8UQQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABAAABDgAAAQYCEUGrIiAIgTADA4DjQNmgbPAziWBc+D50EUAY5lwfPgeRBFAAAAAAAAAAAAADTPg6pCVeGqAM3zYKpQVaguAAAAAAAAAAAAAJbnQVWhqnBdgOV5MFWYKlQVAAAAAAAAAAAAAE8UobpQXbgqwDNFuCpcFaoLAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgAABhwAAAIMKEMFBqyIgCIEwBwOIplAQCA4ziWBQAAjuNYFgAAWJYligAAYFmaKAIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAAAGHAAAAgwoQwUGrISAIgCADAoimUBy7IsYFmWBTTNsgCWBtA8gOcBRBEACAAAKHAAAAiwQVNicYBCQ1YCAFEAAAZFsSxNE0WapmmaJoo0TdM0TRR5nqZ5nmlC0zzPNCGKnmeaEEXPM02YpiiqKhBFVRUAAFDgAAAQYIOmxOIAhYasBABCAgAMjmJZnieKoiiKpqmqNE3TPE8URdE0VdVVaZqmeZ4oiqJpqqrq8jxNE0XTFEXTVFXXhaaJommaommqquvC80TRNE1TVVXVdeF5omiapqmqruu6EEVRNE3TVFXXdV0giqZpmqrqurIMRNE0VVVVXVeWgSiapqqqquvKMjBN01RV15VdWQaYpqq6rizLMkBVXdd1ZVm2Aarquq4ry7INcF3XlWVZtm0ArivLsmzbAgAADhwAAAKMoJOMKouw0YQLD0ChISsCgCgAAMAYphRTyjAmIaQQGsYkhBJCJiWVlEqqIKRSUikVhFRSKiWjklJqKVUQUikplQpCKqWVVAAA2IEDANiBhVBoyEoAIA8AgCBGKcYYYwwyphRjzjkHlVKKMeeck4wxxphzzkkpGWPMOeeklIw555xzUkrmnHPOOSmlc84555yUUkrnnHNOSiklhM45J6WU0jnnnBMAAFTgAAAQYKPI5gQjQYWGrAQAUgEADI5jWZqmaZ4nipYkaZrneZ4omqZmSZrmeZ4niqbJ8zxPFEXRNFWV53meKIqiaaoq1xVF0zRNVVVVsiyKpmmaquq6ME3TVFXXdWWYpmmqquu6LmzbVFXVdWUZtq2aqiq7sgxcV3Vl17aB67qu7Nq2AADwBAcAoAIbVkc4KRoLLDRkJQCQAQBAGIOMQgghhRBCCiGElFIICQAAGHAAAAgwoQwUGrISAEgFAACQsdZaa6211kBHKaWUUkqpcIxSSimllFJKKaWUUkoppZRKSimllFJKKaWUUkoppZRSSimllFJKKaWUUkoppZRSSimllFJKKaWUUkoppZRSSimllFJKKaWUUkoppZRSSimllFJKKaWUUkoFAC5VOADoPtiwOsJJ0VhgoSErAYBUAADAGKWYck5CKRVCjDkmIaUWK4QYc05KSjEWzzkHoZTWWiyecw5CKa3FWFTqnJSUWoqtqBQyKSml1mIQwpSUWmultSCEKqnEllprQQhdU2opltiCELa2klKMMQbhg4+xlVhqDD74IFsrMdVaAABmgwMARIINqyOcFI0FFhqyEgAICQAgjFGKMcYYc8455yRjjDHmnHMQQgihZIwx55xzDkIIIZTOOeeccxBCCCGEUkrHnHMOQgghhFBS6pxzEEIIoYQQSiqdcw5CCCGEUkpJpXMQQgihhFBCSSWl1DkIIYQQQikppZRCCCGEEkIoJaWUUgghhBBCKKGklFIKIYRSQgillJRSSimFEEoIpZSSUkkppRJKCSGEUlJJKaUUQggllFJKKimllEoJoYRSSimlpJRSSiGUUEIpBQAAHDgAAAQYQScZVRZhowkXHoBCQ1YCAGQAAJSyUkoorVVAIqUYpNpCR5mDFHOJLHMMWs2lYg4pBq2GyjGlGLQWMgiZUkxKCSV1TCknLcWYSuecpJhzjaVzEAAAAEEAgICQAAADBAUzAMDgAOFzEHQCBEcbAIAgRGaIRMNCcHhQCRARUwFAYoJCLgBUWFykXVxAlwEu6OKuAyEEIQhBLA6ggAQcnHDDE294wg1O0CkqdSAAAAAAAAwA8AAAkFwAERHRzGFkaGxwdHh8gISIjJAIAAAAAAAYAHwAACQlQERENHMYGRobHB0eHyAhIiMkAQCAAAIAAAAAIIAABAQEAAAAAAACAAAABARPZ2dTAATCMAAAAAAAAFUPGmkCAAAAhlAFnjkoHh4dHx4pKHA1KjEqLzIsNDQqMCveHiYpczUpLS4sLSg3MicsLCsqJTIvJi0sKywkMjbgWVlXWUa00CqtQNVCq7QC1aoNVPXg9Xldx3nn5tixvV6vb7TX+hg7cK21QYgAtNJFphRUtpUuMqWgsqrasj2IhOA1F7LFMdFaWzkAtNBFpisIQgtdZLqCIKjqAAa9WePLkKr1MMG1FlwGtNJFTSkIcitd1JSCIKsCAQWISK0Cyzw147T1tAK00kVNKKjQVrqoCQUVqqr412m+VKtZf9h+TDaaztAAtNJFzVQQhFa6qJkKgqAqUGgtuOa2Se5l6jeXGSqnLM9enqnLs5dn6m7TptWUiVUVN4jhUz9//lzx+Xw+X3x8fCQSiWggDAA83UXF6/vpLipe3zsCULWMBE5PMTBMlsv39/f39/f39524nZ13CDgaRFuLYTbaWgyzq22MzEyKolIpst50Z9PGqqJSq8T2++taLf3+oqg6btyouhEjYlxFjXxex1wCBFxcv+PmzG1uc2bKyJFLLlkizZozZ/ZURpZs2TKiWbNnz5rKyJItS0akWbNnzdrIyJJtxmCczpxOATRRhoPimyjDQfEfIFMprQDU3WFYbXZLZZxMhxrGyRh99Uqel55XEk+9efP7I/FU/8Ojew4JNN/rTq6b73Un1x+AVSsCWD2tNqtpGOM4DOM4GV7n5th453cXNGcfAYQKTFEOguKnKAdB8btRLxNBWUrViLoY1/q1er+Q9xkvZM/IjaoRf30xu3HLnr61fu3UBDRZHZdqsjoutQeAVesAxNMTw2rR66X/Ix6/T5tx80+t/D67ipt/q5XfJzTfa03Wzfdak/UeAEpZawlsbharxTBVO1+c2nm/7/f1XR1dY8XaKWMH3aW9xvEFRFEksXgURRKLn7VamSFRVnYXg0C2Zo2MNE3+57u+e3NFlVev1uufX6nU3Lnf9d1j4wE03+sObprvdQc3ewBYFIArAtjdrRaraRivX7x+8VrbHIofG0n6cFwtNFKYBzxXA2j4uRpAw7dJRkSETBkZV1V1o+N0Op1WhmEyDOn36437RbKvl7zz838wgn295Iv8/Ac8UaRIPFGkSHyAzCItAXY3dzGsNueM6VDDOJkOY3QYX008L6vnfZp/3qf559VQL3Xm1SEFNN2fiMA03Z+IwOwBoKplAKY4TbGIec0111x99dXr9XrjZ/nzdSWXBekAHEsWp4ljyeI0sVs2FEGiLFLj7rjxeqG8Pm+tX/uW90b+DX31bVTF/I+Ut+/sM1IA/MyILvUzI7rUbpNqyIBVjSDGVV/Jo/9H6G/jq+5y3Pzb7P74Znf5ffZtApI5/fN5SAcHjIhB5vTP5yEdHDAiBt4oK/WGeqUMMspeTNsGk/H/PziIgCrG1Rijktfreh2vn4DH78WXa25yZkizZc9oM7JmaYeZM6bJOJkOxmE69Hmp/q/k0fvVRLln3H6fXcXNPt78W638Ptlxsytv/pHyW7Pfp1Xc7L5XfqvZb5MdN7vy5p/u8lut/D6t4mb3vfmnVn6bNt9nV3Hzj1d+q9lv02bc7Mqbf6vZb+N23OzKm73u8lOz3+fY3uwqLv1022+THTepN38yf7XyW1aX8YqjACWfDTiAA+BQALTURU0oCFpLXdSEgqAJpAKxrLtzybNt1Go5VeJAASzRnh75Eu3pke8BYNWiCIBVLdgsXMqlXBJijDGW2Sj5lUqlSJFpPN9fAf08318B/ewBUMUiA3h4YGIaooZrfn5+fn5+fn5+fn6mtQYKcQE8WVg5YfJkYeWEyWqblCIiiqKoVGq1WqxWWa3X6/V6vVoty0zrptXq9/u4ccS4GjWKGxcM6ogaNWpUnoDf73Xd3OQml2xZMhJNM7Nmz54zZ/bsWbNmphVJRpYs2bJly5YtS0YSoWlm1uzZc+bMnj17ZloATNNI4PbTNBK4/W5jlJGglFJWI4hR/levXr06RuJ5+fLly6Ln1atXxxD18uXLKnr+V8cI8/M03+vErpvvdWLXewBYxVoC9bBZDcPU3Bevtc399UWNtZH0p4MJZov7AkxThBmYpggzcNVCJqxIRQwiLpNBxxqUt/NvuCqmb2Poa+RftCr7DO3te16HBjzbulL22daVsnsAqKIFwMXVzbCLYdVe9vGovzx9xP7469mk3L05d1+qjyKuPAY8397G2PPtbYztAWDVQgCH09MwTTG+Us67nX1fG5G+0o3YvspGtK+yfBmqAExTJDHQaYokBnrrZZEZkqoa3BjFDJlmGA17PF+qE/GbJd3xm0V38qoYT/aLuTzh6w/ST/j6g/QHYBVgKYHTxcVqGKY5DOM4DNNRO3OXkM0JmAto6AE01xBa5OYaQou8B4BmRssAUNQ0TfP169fv169fvz6XSIZhGIbJixcvXrzIFP7+/3/9evc/wyMAVFM8EEOvpngghr5by8hIsqiqBjXGXx0T4zCdTCfj8PJl1fy83vv7q1fHvEubn5+fnwc84etOrp/wdSfXewBUsRDA5upqMU1DNl+/GNunkTDUGrWzn0BDIC5UUw7CwKspB2HgVzVFSFZ1R9QxU8MkHXvLGV8jKxtjv6J9G0N/MX1fIysbQzTdOlK26daRsnsAWLUGWFxcTQum8Skv93j2KLpfjSeb3fvFmM3xt3L3/mwCPN/2Rvb5tjeyewBULQGmzdM0DMzS3vEVHVu6MVTZGNn3Fe37WjxU2RjqAUxThJGfpggjv1uLDAlVdeOIGNH/1P9Q5/Jxvf49nmyOj74quveLufGb4zzh685unvB1Zzd7AFQAWAhguLpaTFNk8/1i7Ni+Oq5BxQVcGABEVcgFXo+qkAu8vlurZiaoqiNi3N2Z94sXL168ePEiR4wYMWLEiBEjRowYMWLEiBEjAFRVtGm4qqJNw7ceGRkZrGpQNW58OozDOIzDy5dV8/Pz8/Pz8/Pz8/Pz8/Pz8/NlPN/rDr6f73UH33sAVLGUwHRxsxqGaq72+tcvy5LsLLZ5JdBo0BdUU7Qgr6ZoQb4NqKon4PH6zfFknHYYjOqLT9XaWdkYWvQr2vcV7fuK9n3F9AEs3SZSduk2kbJ7AKhqBeDm7maYaujzKS8/0f/UJ/eL7v2ie7/o3rfHk83xBDzdZlLu6TaTcnsAWLUAYHcz1KqivUt7V/ZQZWPoX7TvK9r3a6iyMVSJ6QNMUaSQnaJIIXvrGSkSVTWIihsZpsmYjKJ/8vTxvC6694sxm+PJ5vhbuXu/ADzf6w5+nu91Bz97AFi1lACHm9UwVHPztbbpkiKHJVsy2SAcDURTFhZc0ZSFBdeqNqiKQXwej8dxXrx48eLFixcvXrx4oY3g8/////////+voo3IF3cCRE/xjoLoKd5RsPUCKVN9jt/v8TruMJ1MJ9PJ6E3z8y9fvnz58uXLly+rSp+Z+V+9ejXv7+8eukl9XpcPJED4YJP6vC4fSIDwgWN7vdDrmfT//4PHDfg98ns9/qDHnBxps2RPkuw5ciYZOXPJmSFrllSSNVumJDNLphgno2E6GQ3jUBmPeOn/KP11zY6bfxvfjCu/TSuv/Datustxs0/Njpt9anbc7Nv4yiu/TSuv/Datustxs0/Njpt9aptx82/jm175bVp55bfZ/e5y3OxT24ybfWqbcfNv08orv00rr/w27dfsuNmnthk3+7SVV36bVl75bVqJnUxPzXazT0294mnq2W+TikmmE5LiQb3pAa94mnpFAGxeSf1/jn9mWTgDBjhUUv+f459ZFs6AAQ4AAAAAAIAH/0EYBHEAB6gDzBkAAUxWjEAQk7nWaBZuuKvBN6iqkoMah7sAhnRZ6lFjmllwEgGCAde2zYBzAB5AAH5J/X+Of81ycQZMHI0uqf/P8a9ZLs6AiaMRAAAAAAIAOPgPw0EUEIddhEaDphAAjAhrrgAUlNDwPZKFEPFz2JKV4FqHl6tIxjaQDfQAiJqgZk1GDQgcBuAAfkn9f45/zXLiDBgwuqT+P8e/ZjlxBgwYAQAAAAAAg/8fDBlCDUeGDICqAJAT585AAALkhkHxIHMR3AF8IwmgWZwQhv0DcpcIMeTjToEGKDQAB0CEACgAfkn9f45/LXLiDCiMxpfU/+f41yInzoDCaAwAAAAEg4P/wyANDgAEhDsAujhQcBgAHEakAKBZjwHgANMYAkIDo+L8wDUrrgHpWnPwBBoJGZqDBmBAUAB1QANeOf1/zn53uYQA9ckctMrp/3P2u8slBKhP5qABAAAAAACAIAyCIAiD8DAMwoADzgECAA0wQFMAiMtgo6AATVGAE0gADAQA"></audio>\n      <audio id="offline-sound-reached" src="data:audio/mpeg;base64,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"></audio>\n    </template>\n  </div>\n\n\n<script jstcache="0">(function(){function l(a,b,c){return Function.prototype.call.apply(Array.prototype.slice,arguments)}function m(a,b,c){var e=l(arguments,2);return function(){return b.apply(a,e)}}function n(a,b){var c=new p(b);for(c.h=[a];c.h.length;){var e=c,d=c.h.shift();e.i(d);for(d=d.firstChild;d;d=d.nextSibling)1==d.nodeType&&e.h.push(d)}}function p(a){this.i=a}function q(a){a.style.display=""}function r(a){a.style.display="none"};var t=/\\s*;\\s*/;function u(a,b){this.l.apply(this,arguments)}u.prototype.l=function(a,b){this.a||(this.a={});if(b){var c=this.a,e=b.a;for(d in e)c[d]=e[d]}else{var d=this.a;e=v;for(c in e)d[c]=e[c]}this.a.$this=a;this.a.$context=this;this.f="undefined"!=typeof a&&null!=a?a:"";b||(this.a.$top=this.f)};var v={$default:null},w=[];function x(a){for(var b in a.a)delete a.a[b];a.f=null;w.push(a)}function y(a,b,c){try{return b.call(c,a.a,a.f)}catch(e){return v.$default}}\nu.prototype.clone=function(a,b,c){if(0<w.length){var e=w.pop();u.call(e,a,this);a=e}else a=new u(a,this);a.a.$index=b;a.a.$count=c;return a};var z;window.trustedTypes&&(z=trustedTypes.createPolicy("jstemplate",{createScript:function(a){return a}}));var A={};function B(a){if(!A[a])try{var b="(function(a_, b_) { with (a_) with (b_) return "+a+" })",c=window.trustedTypes?z.createScript(b):b;A[a]=window.eval(c)}catch(e){}return A[a]}\nfunction E(a){var b=[];a=a.split(t);for(var c=0,e=a.length;c<e;++c){var d=a[c].indexOf(":");if(!(0>d)){var g=a[c].substr(0,d).replace(/^\\s+/,"").replace(/\\s+$/,"");d=B(a[c].substr(d+1));b.push(g,d)}}return b};function F(){}var G=0,H={0:{}},I={},J={},K=[];function L(a){a.__jstcache||n(a,function(b){M(b)})}var N=[["jsselect",B],["jsdisplay",B],["jsvalues",E],["jsvars",E],["jseval",function(a){var b=[];a=a.split(t);for(var c=0,e=a.length;c<e;++c)if(a[c]){var d=B(a[c]);b.push(d)}return b}],["transclude",function(a){return a}],["jscontent",B],["jsskip",B]];\nfunction M(a){if(a.__jstcache)return a.__jstcache;var b=a.getAttribute("jstcache");if(null!=b)return a.__jstcache=H[b];b=K.length=0;for(var c=N.length;b<c;++b){var e=N[b][0],d=a.getAttribute(e);J[e]=d;null!=d&&K.push(e+"="+d)}if(0==K.length)return a.setAttribute("jstcache","0"),a.__jstcache=H[0];var g=K.join("&");if(b=I[g])return a.setAttribute("jstcache",b),a.__jstcache=H[b];var h={};b=0;for(c=N.length;b<c;++b){d=N[b];e=d[0];var f=d[1];d=J[e];null!=d&&(h[e]=f(d))}b=""+ ++G;a.setAttribute("jstcache",\nb);H[b]=h;I[g]=b;return a.__jstcache=h}function P(a,b){a.j.push(b);a.o.push(0)}function Q(a){return a.c.length?a.c.pop():[]}\nF.prototype.g=function(a,b){var c=R(b),e=c.transclude;if(e)(c=S(e))?(b.parentNode.replaceChild(c,b),e=Q(this),e.push(this.g,a,c),P(this,e)):b.parentNode.removeChild(b);else if(c=c.jsselect){c=y(a,c,b);var d=b.getAttribute("jsinstance");var g=!1;d&&("*"==d.charAt(0)?(d=parseInt(d.substr(1),10),g=!0):d=parseInt(d,10));var h=null!=c&&"object"==typeof c&&"number"==typeof c.length;e=h?c.length:1;var f=h&&0==e;if(h)if(f)d?b.parentNode.removeChild(b):(b.setAttribute("jsinstance","*0"),r(b));else if(q(b),\nnull===d||""===d||g&&d<e-1){g=Q(this);d=d||0;for(h=e-1;d<h;++d){var k=b.cloneNode(!0);b.parentNode.insertBefore(k,b);T(k,c,d);f=a.clone(c[d],d,e);g.push(this.b,f,k,x,f,null)}T(b,c,d);f=a.clone(c[d],d,e);g.push(this.b,f,b,x,f,null);P(this,g)}else d<e?(g=c[d],T(b,c,d),f=a.clone(g,d,e),g=Q(this),g.push(this.b,f,b,x,f,null),P(this,g)):b.parentNode.removeChild(b);else null==c?r(b):(q(b),f=a.clone(c,0,1),g=Q(this),g.push(this.b,f,b,x,f,null),P(this,g))}else this.b(a,b)};\nF.prototype.b=function(a,b){var c=R(b),e=c.jsdisplay;if(e){if(!y(a,e,b)){r(b);return}q(b)}if(e=c.jsvars)for(var d=0,g=e.length;d<g;d+=2){var h=e[d],f=y(a,e[d+1],b);a.a[h]=f}if(e=c.jsvalues)for(d=0,g=e.length;d<g;d+=2)if(f=e[d],h=y(a,e[d+1],b),"$"==f.charAt(0))a.a[f]=h;else if("."==f.charAt(0)){f=f.substr(1).split(".");for(var k=b,O=f.length,C=0,U=O-1;C<U;++C){var D=f[C];k[D]||(k[D]={});k=k[D]}k[f[O-1]]=h}else f&&("boolean"==typeof h?h?b.setAttribute(f,f):b.removeAttribute(f):b.setAttribute(f,""+h));\nif(e=c.jseval)for(d=0,g=e.length;d<g;++d)y(a,e[d],b);e=c.jsskip;if(!e||!y(a,e,b))if(c=c.jscontent){if(c=""+y(a,c,b),b.innerHTML!=c){for(;b.firstChild;)e=b.firstChild,e.parentNode.removeChild(e);b.appendChild(this.m.createTextNode(c))}}else{c=Q(this);for(e=b.firstChild;e;e=e.nextSibling)1==e.nodeType&&c.push(this.g,a,e);c.length&&P(this,c)}};function R(a){if(a.__jstcache)return a.__jstcache;var b=a.getAttribute("jstcache");return b?a.__jstcache=H[b]:M(a)}\nfunction S(a,b){var c=document;if(b){var e=c.getElementById(a);if(!e){e=b();var d=c.getElementById("jsts");d||(d=c.createElement("div"),d.id="jsts",r(d),d.style.position="absolute",c.body.appendChild(d));var g=c.createElement("div");d.appendChild(g);g.innerHTML=e;e=c.getElementById(a)}c=e}else c=c.getElementById(a);return c?(L(c),c=c.cloneNode(!0),c.removeAttribute("id"),c):null}function T(a,b,c){c==b.length-1?a.setAttribute("jsinstance","*"+c):a.setAttribute("jsinstance",""+c)};window.jstGetTemplate=S;window.JsEvalContext=u;window.jstProcess=function(a,b){var c=new F;L(b);c.m=b?9==b.nodeType?b:b.ownerDocument||document:document;var e=m(c,c.g,a,b),d=c.j=[],g=c.o=[];c.c=[];e();for(var h,f,k;d.length;)h=d[d.length-1],e=g[g.length-1],e>=h.length?(e=c,f=d.pop(),f.length=0,e.c.push(f),g.pop()):(f=h[e++],k=h[e++],h=h[e++],g[g.length-1]=e,f.call(c,k,h))};\n})()</script><script jstcache="0">// Copyright (c) 2012 The Chromium Authors. All rights reserved.\n// Use of this source code is governed by a BSD-style license that can be\n// found in the LICENSE file.\n\n/**\n * @fileoverview This file defines a singleton which provides access to all data\n * that is available as soon as the page\'s resources are loaded (before DOM\n * content has finished loading). This data includes both localized strings and\n * any data that is important to have ready from a very early stage (e.g. things\n * that must be displayed right away).\n *\n * Note that loadTimeData is not guaranteed to be consistent between page\n * refreshes (https://crbug.com/740629) and should not contain values that might\n * change if the page is re-opened later.\n */\n\n/** @type {!LoadTimeData} */\n// eslint-disable-next-line no-var\nvar loadTimeData;\n\nclass LoadTimeData {\n  constructor() {\n    /** @type {?Object} */\n    this.data_ = null;\n  }\n\n  /**\n   * Sets the backing object.\n   *\n   * Note that there is no getter for |data_| to discourage abuse of the form:\n   *\n   *     var value = loadTimeData.data()[\'key\'];\n   *\n   * @param {Object} value The de-serialized page data.\n   */\n  set data(value) {\n    expect(!this.data_, \'Re-setting data.\');\n    this.data_ = value;\n  }\n\n  /**\n   * @param {string} id An ID of a value that might exist.\n   * @return {boolean} True if |id| is a key in the dictionary.\n   */\n  valueExists(id) {\n    return id in this.data_;\n  }\n\n  /**\n   * Fetches a value, expecting that it exists.\n   * @param {string} id The key that identifies the desired value.\n   * @return {*} The corresponding value.\n   */\n  getValue(id) {\n    expect(this.data_, \'No data. Did you remember to include strings.js?\');\n    const value = this.data_[id];\n    expect(typeof value !== \'undefined\', \'Could not find value for \' + id);\n    return value;\n  }\n\n  /**\n   * As above, but also makes sure that the value is a string.\n   * @param {string} id The key that identifies the desired string.\n   * @return {string} The corresponding string value.\n   */\n  getString(id) {\n    const value = this.getValue(id);\n    expectIsType(id, value, \'string\');\n    return /** @type {string} */ (value);\n  }\n\n  /**\n   * Returns a formatted localized string where $1 to $9 are replaced by the\n   * second to the tenth argument.\n   * @param {string} id The ID of the string we want.\n   * @param {...(string|number)} var_args The extra values to include in the\n   *     formatted output.\n   * @return {string} The formatted string.\n   */\n  getStringF(id, var_args) {\n    const value = this.getString(id);\n    if (!value) {\n      return \'\';\n    }\n\n    const args = Array.prototype.slice.call(arguments);\n    args[0] = value;\n    return this.substituteString.apply(this, args);\n  }\n\n  /**\n   * Returns a formatted localized string where $1 to $9 are replaced by the\n   * second to the tenth argument. Any standalone $ signs must be escaped as\n   * $$.\n   * @param {string} label The label to substitute through.\n   *     This is not an resource ID.\n   * @param {...(string|number)} var_args The extra values to include in the\n   *     formatted output.\n   * @return {string} The formatted string.\n   */\n  substituteString(label, var_args) {\n    const varArgs = arguments;\n    return label.replace(/\\$(.|$|\\n)/g, function(m) {\n      expect(m.match(/\\$[$1-9]/), \'Unescaped $ found in localized string.\');\n      return m === \'$$\' ? \'$\' : varArgs[m[1]];\n    });\n  }\n\n  /**\n   * Returns a formatted string where $1 to $9 are replaced by the second to\n   * tenth argument, split apart into a list of pieces describing how the\n   * substitution was performed. Any standalone $ signs must be escaped as $$.\n   * @param {string} label A localized string to substitute through.\n   *     This is not an resource ID.\n   * @param {...(string|number)} var_args The extra values to include in the\n   *     formatted output.\n   * @return {!Array<!{value: string, arg: (null|string)}>} The formatted\n   *     string pieces.\n   */\n  getSubstitutedStringPieces(label, var_args) {\n    const varArgs = arguments;\n    // Split the string by separately matching all occurrences of $1-9 and of\n    // non $1-9 pieces.\n    const pieces = (label.match(/(\\$[1-9])|(([^$]|\\$([^1-9]|$))+)/g) ||\n                    []).map(function(p) {\n      // Pieces that are not $1-9 should be returned after replacing $$\n      // with $.\n      if (!p.match(/^\\$[1-9]$/)) {\n        expect(\n            (p.match(/\\$/g) || []).length % 2 === 0,\n            \'Unescaped $ found in localized string.\');\n        return {value: p.replace(/\\$\\$/g, \'$\'), arg: null};\n      }\n\n      // Otherwise, return the substitution value.\n      return {value: varArgs[p[1]], arg: p};\n    });\n\n    return pieces;\n  }\n\n  /**\n   * As above, but also makes sure that the value is a boolean.\n   * @param {string} id The key that identifies the desired boolean.\n   * @return {boolean} The corresponding boolean value.\n   */\n  getBoolean(id) {\n    const value = this.getValue(id);\n    expectIsType(id, value, \'boolean\');\n    return /** @type {boolean} */ (value);\n  }\n\n  /**\n   * As above, but also makes sure that the value is an integer.\n   * @param {string} id The key that identifies the desired number.\n   * @return {number} The corresponding number value.\n   */\n  getInteger(id) {\n    const value = this.getValue(id);\n    expectIsType(id, value, \'number\');\n    expect(value === Math.floor(value), \'Number isn\\\'t integer: \' + value);\n    return /** @type {number} */ (value);\n  }\n\n  /**\n   * Override values in loadTimeData with the values found in |replacements|.\n   * @param {Object} replacements The dictionary object of keys to replace.\n   */\n  overrideValues(replacements) {\n    expect(\n        typeof replacements === \'object\',\n        \'Replacements must be a dictionary object.\');\n    for (const key in replacements) {\n      this.data_[key] = replacements[key];\n    }\n  }\n\n  /**\n   * Reset loadTimeData\'s data. Should only be used in tests.\n   * @param {?Object} newData The data to restore to, when null restores to\n   *    unset state.\n   */\n  resetForTesting(newData = null) {\n    this.data_ = newData;\n  }\n\n  /**\n   * @return {boolean} Whether loadTimeData.data has been set.\n   */\n  isInitialized() {\n    return this.data_ !== null;\n  }\n}\n\n  /**\n   * Checks condition, throws error message if expectation fails.\n   * @param {*} condition The condition to check for truthiness.\n   * @param {string} message The message to display if the check fails.\n   */\n  function expect(condition, message) {\n    if (!condition) {\n      throw new Error(\n          \'Unexpected condition on \' + document.location.href + \': \' + message);\n    }\n  }\n\n  /**\n   * Checks that the given value has the given type.\n   * @param {string} id The id of the value (only used for error message).\n   * @param {*} value The value to check the type on.\n   * @param {string} type The type we expect |value| to be.\n   */\n  function expectIsType(id, value, type) {\n    expect(\n        typeof value === type, \'[\' + value + \'] (\' + id + \') is not a \' + type);\n  }\n\n  expect(!loadTimeData, \'should only include this file once\');\n  loadTimeData = new LoadTimeData();\n\n  // Expose |loadTimeData| directly on |window|, since within a JS module the\n  // scope is local and not all files have been updated to import the exported\n  // |loadTimeData| explicitly.\n  window.loadTimeData = loadTimeData;\n\n  console.warn(\'crbug/1173575, non-JS module files deprecated.\');</script><script jstcache="0">const pageData = {"details":"Details","errorCode":"ERR_TIMED_OUT","fontfamily":"system-ui, sans-serif","fontsize":"75%","heading":{"hostName":"openreview.net","msg":"This site can’t be reached"},"hideDetails":"Hide details","iconClass":"icon-generic","language":"en","reloadButton":{"msg":"Reload","reloadUrl":"https://openreview.net/forum?id=RmuXDtjDhG"},"suggestionsDetails":[{"body":"Check any cables and reboot any routers, modems, or other network\\n    devices you may be using.","header":"Check your Internet connection"},{"body":"If it is already listed as a program allowed to access the network, try\\n      removing it from the list and adding it again.","header":"Allow Chrome to access the network in your firewall or antivirus\\n          settings."},{"advancedTitle":"Show advanced settings…","body":"Check your proxy settings or contact your network administrator to\\n      make sure the proxy server is working. If you don\'t believe you should\\n      be using a proxy server:\\n      Go to\\n          Applications > System Preferences > Network > Advanced > Proxies\\n          and deselect any proxies that have been selected.","header":"If you use a proxy server…","proxyTitle":"Change proxy settings…","settingsTitle":"Settings"}],"suggestionsSummaryList":[{"summary":"Checking the connection"},{"summary":"\\u003Ca href=\\"#buttons\\" onclick=\\"toggleHelpBox()\\">Checking the proxy and the firewall\\u003C/a>"}],"suggestionsSummaryListHeader":"Try:","summary":{"failedUrl":"https://openreview.net/forum?id=RmuXDtjDhG","hostName":"openreview.net","msg":"\\u003Cstrong jscontent=\\"hostName\\">\\u003C/strong> took too long to respond."},"textdirection":"ltr","title":"openreview.net"};loadTimeData.data = pageData;var tp = document.getElementById(\'t\');jstProcess(new JsEvalContext(pageData), tp);</script></body></html'}
{'title': 'Washing The Unwashable : On The (Im)possibility of Fairwashing Detection', 'authors': ['Ali Shahin Shamsabadi', 'Mohammad Yaghini', 'Natalie Dullerud', 'Sierra Wyllie', 'Ulrich Aïvodji', 'Aisha Alaagib Alryeh Mkean', 'Sébastien Gambs', 'Nicolas Papernot'], 'Conference': 'NeurIPS 2022 Accept', 'date': 'Published: 31 Oct 2022, Last Modified: 11 Jan 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=3vmKQUctNy&name=pdf', 'abstract': '</span><span class="note_content_value">The use of black-box models (e.g., deep neural networks) in high-stakes decision-making systems, whose internal logic is complex, raises the need for providing explanations about their decisions. Model explanation techniques mitigate this problem by generating an interpretable and high-fidelity surrogate model (e.g., a logistic regressor or decision tree) to explain the logic of black-box models. \nIn this work, we investigate the issue of fairwashing, in which model explanation techniques are manipulated to rationalize decisions taken by an unfair black-box model using deceptive surrogate models. More precisely, we theoretically characterize and analyze fairwashing, proving that this phenomenon is difficult to avoid due to an irreducible factor---the unfairness of the black-box model. \nBased on the theory developed, we propose a novel technique, called FRAUD-Detect (FaiRness AUDit Detection), to detect fairwashed models by measuring a divergence over subpopulation-wise fidelity measures of the interpretable model. \nWe empirically demonstrate that this divergence is significantly larger in purposefully fairwashed interpretable models than in honest ones. \nFurthermore, we show that our detector is robust to an informed adversary trying to bypass our detector. The code implementing FRAUD-Detect is available at <a href="https://github.com/cleverhans-lab/FRAUD-Detect" target="_blank" rel="nofollow">https://github.com/cleverhans-lab/FRAUD-Detect</a>.</span>'}
{'title': 'Graph Neural Networks with Local Graph Parameters', 'authors': ['Pablo Barcelo', 'Floris Geerts', 'Juan L Reutter', 'Maksimilian Ryschkov'], 'Conference': 'NeurIPS 2021 Poster', 'date': 'Published: 09 Nov 2021, Last Modified: 05 May 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=yGKklt8wyV&name=pdf', 'abstract': '</span><span class="note_content_value">Various recent proposals increase the distinguishing power of Graph Neural Networks (GNNs) by propagating features between k-tuples of vertices. The distinguishing power of these “higher-order” GNNs is known to be bounded by the k-dimensional Weisfeiler-Leman (WL) test, yet their O(n^k) memory requirements limit their applicability. Other proposals infuse GNNs with local higher-order graph structural information from the start, hereby inheriting the desirable O(n) memory requirement from GNNs at the cost of a one-time, possibly non-linear, preprocessing step. We propose local graph parameter enabled GNNs as a framework for studying the latter&nbsp;kind of approaches and precisely characterize their distinguishing power, in terms of a variant of the WL test, and in terms of the graph structural properties that they can take into account. Local graph parameters can be added to any GNN&nbsp;architecture, and are cheap to compute. In terms of expressive power, our proposal lies in the middle of GNNs and their higher-order counterparts. Further, we propose&nbsp;several techniques to aide in choosing the right local graph parameters. Our results&nbsp;connect GNNs with deep results in finite model theory and finite variable logics. Our experimental evaluation shows that adding local graph parameters often has a&nbsp;positive effect for a variety of GNNs, datasets and graph learning tasks. \n</span>'}
{'title': 'Fit The Right NP-Hard Problem: End-to-end Learning of Integer Programming Constraints', 'authors': ['Anselm Paulus', 'Michal Rolinek', 'Vít Musil', 'Brandon Amos', 'Georg Martius'], 'Conference': 'LMCA2020 Oral', 'date': 'Published: 12 Dec 2020, Last Modified: 05 May 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=-3qCWheZhxU&name=pdf', 'abstract': '</span><span class="note_content_value">Bridging logical and algorithmic reasoning with modern machine learning\ntechniques is a fundamental challenge with potentially transformative impact.\nOn the algorithmic side, many NP-Hard problems can be expressed as integer\nprograms, in which the constraints play the role of their ``combinatorial\nspecification\'\'. In this work, we aim to integrate integer programming solvers\ninto neural network architectures by providing loss functions for \\emph{both}\nthe objective and the constraints. The resulting end-to-end trainable\narchitectures have the power of jointly extracting features from raw data and\nof solving a suitable (learned) combinatorial problem with state-of-the-art\ninteger programming solvers. We experimentally validate our approach on\nartificial datasets created from random constraints, and on solving\n\\textsc{Knapsack} instances from their description in natural language.</span>'}
{'title': 'Semantic Probabilistic Layers for Neuro-Symbolic Learning', 'authors': ['Kareem Ahmed', 'Stefano Teso', 'Kai-Wei Chang', 'Guy Van den Broeck', 'Antonio Vergari'], 'Conference': 'NeurIPS 2022 Accept', 'date': 'Published: 31 Oct 2022, Last Modified: 16 Sept 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=o-mxIWAY1T8&name=pdf', 'abstract': '</span><span class="note_content_value">We design a predictive layer for structured-output prediction (SOP) that can be plugged into any neural network guaranteeing its predictions are consistent with a set of predefined symbolic constraints. Our Semantic Probabilistic Layer (SPL) can model intricate correlations, and hard constraints, over a structured output space all while being amenable to end-to-end learning via maximum likelihood.\nSPLs combine exact probabilistic inference with logical reasoning in a clean and modular way, learning complex distributions and restricting their support to solutions of the constraint. As such, they can faithfully, and efficiently, model complex SOP tasks beyond the reach of alternative neuro-symbolic approaches. We empirically demonstrate that SPLs outperform these competitors in terms of accuracy on challenging SOP tasks such as hierarchical multi-label classification, pathfinding and preference learning, while retaining perfect constraint satisfaction.</span>'}
{'title': 'The Synthesis of XNOR Recurrent Neural Networks with Stochastic Logic', 'authors': ['Arash Ardakani'], 'Conference': 'NeurIPS 2019', 'date': '06 Sept 2019 (modified: 05 May 2023)', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=SygqpVSxLB&name=pdf', 'abstract': '</span><span class="note_content_value">The emergence of XNOR networks seek to reduce the model size and computational cost of neural networks for their deployment on specialized hardware requiring real-time processes with limited hardware resources. In XNOR networks, both weights and activations are binary, bringing great benefits to specialized hardware by replacing expensive multiplications with simple XNOR operations. Although XNOR convolutional and fully-connected neural networks have been successfully developed during the past few years, there is no XNOR network implementing commonly-used variants of recurrent neural networks such as long short-term memories (LSTMs). The main computational core of LSTMs involves vector-matrix multiplications followed by a set of non-linear functions and element-wise multiplications to obtain the gate activations and state vectors, respectively. Several previous attempts on quantization of LSTMs only focused on quantization of the vector-matrix multiplications in LSTMs while retaining the element-wise multiplications in full precision. In this paper, we propose a method that converts all the multiplications in LSTMs to XNOR operations using stochastic computing. To this end, we introduce a weighted finite-state machine and its synthesis method to approximate the non-linear functions used in LSTMs on stochastic bit streams. Experimental results show that the proposed XNOR LSTMs reduce the computational complexity of their quantized counterparts by a factor of 86x without any sacrifice on latency while achieving a better accuracy across various temporal tasks.</span>'}
{'title': 'Emergent Communication of Generalizations', 'authors': ['Jesse Mu', 'Noah Goodman'], 'Conference': 'NeurIPS 2021 Poster', 'date': 'Published: 09 Nov 2021, Last Modified: 16 Sept 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=yq5MYHVaClG&name=pdf', 'abstract': '</span><span class="note_content_value">To build agents that can collaborate effectively with others, recent research has trained artificial agents to communicate with each other in Lewis-style referential games. However, this often leads to successful but uninterpretable communication. We argue that this is due to the game objective: communicating about a single object in a shared visual context is prone to overfitting and does not encourage language useful beyond concrete reference. In contrast, human language conveys a rich variety of abstract ideas. To promote such skills, we propose games that require communicating generalizations over sets of objects representing abstract visual concepts, optionally with separate contexts for each agent. We find that these games greatly improve systematicity and interpretability of the learned languages, according to several metrics in the literature. Finally, we propose a method for identifying logical operations embedded in the emergent languages by learning an approximate compositional reconstruction of the language.</span>'}
{'title': 'Evident: a Development Methodology and a Knowledge Base Topology for Data Mining, Machine Learning and General Knowledge Management', 'authors': ['Mingwu Gao', 'Samer Haidar'], 'Conference': 'NeurIPS 2022 Submitted', 'date': '16 May 2022 (modified: 05 May 2023)', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=_1bgdFHhA70&name=pdf', 'abstract': '</span><span class="note_content_value">Software has been developed for knowledge discovery, prediction and management for over 30 years. However, there are still unresolved pain points when using existing project development and artifact management methodologies. Historically, there has been a lack of applicable methodologies. Further, methodologies that have been applied, such as Agile, have several limitations including scientific unfalsifiability that reduce their applicability. Evident, a development methodology rooted in the philosophy of logical reasoning and EKB, a knowledge base topology, are proposed. Many pain points in data mining, machine learning and general knowledge management are alleviated conceptually. Evident can be extended potentially to accelerate philosophical exploration, science discovery, education as well as knowledge sharing &amp; retention across the globe. EKB offers one solution of storing information as knowledge, a granular level above data. Related topics in computer history, software engineering, database, sensing hardware, philosophy, and project &amp; organization &amp; military managements are also discussed.</span>'}
{'title': 'End-to-end Algorithm Synthesis with Recurrent Networks: Extrapolation without Overthinking', 'authors': ['Arpit Bansal', 'Avi Schwarzschild', 'Eitan Borgnia', 'Zeyad Emam', 'Furong Huang', 'Micah Goldblum', 'Tom Goldstein'], 'Conference': 'NeurIPS 2022 Accept', 'date': 'Published: 31 Oct 2022, Last Modified: 14 Dec 2022', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=PPjSKy40XUB&name=pdf', 'abstract': '</span><span class="note_content_value">Machine learning systems perform well on pattern matching tasks, but their ability to perform algorithmic or logical reasoning is not well understood. One important reasoning capability is algorithmic extrapolation, in which models trained only on small/simple reasoning problems can synthesize complex strategies for large/complex problems at test time. Algorithmic extrapolation can be achieved through recurrent systems, which can be iterated many times to solve difficult reasoning problems. We observe that this approach fails to scale to highly complex problems because behavior degenerates when many iterations are applied -- an issue we refer to as "overthinking." We propose a recall architecture that keeps an explicit copy of the problem instance in memory so that it cannot be forgotten. We also employ a progressive training routine that prevents the model from learning behaviors that are specific to iteration number and instead pushes it to learn behaviors that can be repeated indefinitely. These innovations prevent the overthinking problem, and enable recurrent systems to solve extremely hard extrapolation tasks.</span>'}
{'title': 'Techniques for Symbol Grounding with SATNet', 'authors': ['Sever Topan', 'David Rolnick', 'Xujie Si'], 'Conference': 'NeurIPS 2021 Spotlight', 'date': 'Published: 09 Nov 2021, Last Modified: 16 Sept 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=lZJHxMxUhV_&name=pdf', 'abstract': '</span><span class="note_content_value">Many experts argue that the future of artificial intelligence is limited by the field’s ability to integrate symbolic logical reasoning into deep learning architectures. The recently proposed differentiable MAXSAT solver, SATNet, was a breakthrough in its capacity to integrate with a traditional neural network and solve visual reasoning problems. For instance, it can learn the rules of Sudoku purely from image examples. Despite its success, SATNet was shown to succumb to a key challenge in neurosymbolic systems known as the Symbol Grounding Problem: the inability to map visual inputs to symbolic variables without explicit supervision ("label leakage"). In this work, we present a self-supervised pre-training pipeline that enables SATNet to overcome this limitation, thus broadening the class of problems that SATNet architectures can solve to include datasets where no intermediary labels are available at all. We demonstrate that our method allows SATNet to attain full accuracy even with a harder problem setup that prevents any label leakage. We additionally introduce a proofreading method that further improves the performance of SATNet architectures, beating the state-of-the-art on Visual Sudoku. </span>'}
{'title': 'Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small', 'authors': ['Kevin Ro Wang', 'Alexandre Variengien', 'Arthur Conmy', 'Buck Shlegeris', 'Jacob Steinhardt'], 'Conference': 'MLSW2022', 'date': 'Published: 05 Dec 2022, Last Modified: 05 May 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=rvi3Wa768B-&name=pdf', 'abstract': '</span><span class="note_content_value">Research in mechanistic interpretability seeks to explain behaviors of ML models in terms of their internal components. However, most previous work either focuses on simple behaviors in small models, or describes complicated behaviors in larger models with broad strokes. In this work, we bridge this gap by presenting an explanation for how GPT-2 small performs a natural language task that requires logical reasoning: indirect object identification (IOI). Our explanation encompasses 28 attention heads grouped into 7 main classes, which we discovered using a combination of interpretability approaches including causal interventions and projections. To our knowledge, this investigation is the largest end-to-end attempt at reverse-engineering a natural behavior ``in the wild" in a language model. We evaluate the reliability of our explanation using three quantitative criteria--faithfulness, completeness and minimality. Though these criteria support our explanation, they also point to remaining gaps in our understanding. Our work provides evidence that a mechanistic understanding of large ML models is feasible, opening opportunities to scale our understanding to both larger models and more complex tasks.</span>'}
{'title': 'Counterfactual reasoning: Do Language Models need world knowledge for causal inference?', 'authors': ['Jiaxuan Li', 'Lang Yu', 'Allyson Ettinger'], 'Conference': 'nCSI WS @ NeurIPS 2022 Poster', 'date': 'Published: 21 Oct 2022, Last Modified: 05 May 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=sS5hCtc-uQ&name=pdf', 'abstract': '</span><span class="note_content_value">Current pre-trained language models have enabled remarkable improvements in downstream tasks, but it remains difficult to distinguish effects of statistical correlation from more systematic logical reasoning grounded on understanding of the real world. In this paper we tease these factors apart by leveraging counterfactual conditionals, which force language models to predict unusual consequences based on hypothetical propositions. We introduce a set of tests drawn from psycholinguistic experiments, as well as larger-scale controlled datasets, to probe counterfactual predictions from a variety of popular pre-trained language models. We find that models are consistently able to override real-world knowledge in counterfactual scenarios, and that this effect is more robust in case of stronger baseline world knowledge---however, we also find that for most models this effect appears largely to be driven by simple lexical cues. When we mitigate effects of both world knowledge and lexical cues to test knowledge of linguistic nuances of counterfactuals, we find that only GPT-3 shows sensitivity to these nuances, though this sensitivity is also non-trivially impacted by lexical associative factors. </span>'}
{'title': 'Ordered Memory', 'authors': ['Yikang Shen'], 'Conference': 'NeurIPS 2019', 'date': '06 Sept 2019 (modified: 05 May 2023)', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=BJGuY4Sl8r&name=pdf', 'abstract': '</span><span class="note_content_value">Stack-augmented recurrent neural networks (RNNs)  have been of interest to the deep learning community for some time. However, the difficulty of training memory models remains a problem obstructing the widespread use of such models. In this paper, we propose the Ordered Memory architecture.  Inspired by Ordered Neurons (Shen et. al., 2018), we introduce a new Stick-breaking Attention Mechanism and use its cumulative probability to control the writing and erasing operation of memory.  We also introduce a new Gated Recursive Cell to compose lower level representations into higher level representation. We demonstrate that our model achieves strong performance on the logical inference task (Bowman et. al. 2015) and the ListOps (Nangia and Bowman, 2018) task. We can also interpret the model to retrieve the induced tree structure, and find that these induced structures align with the ground truth. Finally, we evaluate our model on the Stanford Sentiment Treebank tasks (Socher et. al. 2013), and find that it performs comparatively with the state-of-the-art methods in the literature.</span>'}
{'title': 'Learning Perceptual Inference by Contrasting', 'authors': ['Chi Zhang'], 'Conference': 'NeurIPS 2019', 'date': '06 Sept 2019 (modified: 05 May 2023)', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=r1lnV4SlIH&name=pdf', 'abstract': '</span><span class="note_content_value">"Thinking in pictures", i.e., spatial-temporal reasoning, has been widely believed to be a significant ability for humans to perform logical induction and a crucial factor in the intellectual history of technology development. Modern Artificial Intelligence (AI), fueled by massive datasets, deeper models, and mighty computation, has already come to a stage where (super-)human-level performances are observed in certain specific tasks. However, AI\'s current ability in "thinking in pictures" is still far lacking behind. In this work, we study how to improve machines\' reasoning ability on one challenging task of this kind, i.e., Raven\'s Progressive Matrices (RPM). Specifically, we propose to borrow the very idea of "contrast effects" from the field of psychology, cognition, and education to design and train a permutation-invariant model. Inspired by cognitive studies, we equip our model with a simple inference module that is jointly trained with the perception backbone. Combining all the elements, we propose the Contrastive Perceptual Inference network (CoPINet) and empirically demonstrate that CoPINet sets the new state-of-the-art for permutation-invariant models on two major datasets.</span>'}
{'title': 'Robust Learning against Relational Adversaries', 'authors': ['Yizhen Wang', 'Mohannad Alhanahnah', 'Xiaozhu Meng', 'Ke Wang', 'Mihai Christodorescu', 'Somesh Jha'], 'Conference': 'NeurIPS 2022 Accept', 'date': 'Published: 31 Oct 2022, Last Modified: 11 Jan 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=WBp4dli3No6&name=pdf', 'abstract': '</span><span class="note_content_value">Test-time adversarial attacks have posed serious challenges to the robustness of machine-learning models, and in many settings the adversarial perturbation need not be bounded by small <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="0" style="font-size: 121.3%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>ℓ</mi><mi>p</mi></msub></math></mjx-assistive-mml></mjx-container>-norms. Motivated by attacks in program analysis and security tasks, we investigate <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="1" style="font-size: 121.3%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mtext class="mjx-i"><mjx-c class="mjx-c1D45F TEX-I"></mjx-c><mjx-c class="mjx-c1D452 TEX-I"></mjx-c><mjx-c class="mjx-c1D459 TEX-I"></mjx-c><mjx-c class="mjx-c1D44E TEX-I"></mjx-c><mjx-c class="mjx-c1D461 TEX-I"></mjx-c><mjx-c class="mjx-c1D456 TEX-I"></mjx-c><mjx-c class="mjx-c1D45C TEX-I"></mjx-c><mjx-c class="mjx-c1D45B TEX-I"></mjx-c><mjx-c class="mjx-c1D44E TEX-I"></mjx-c><mjx-c class="mjx-c1D459 TEX-I"></mjx-c><mjx-c class="mjx-c20"></mjx-c><mjx-c class="mjx-c1D44E TEX-I"></mjx-c><mjx-c class="mjx-c1D451 TEX-I"></mjx-c><mjx-c class="mjx-c1D463 TEX-I"></mjx-c><mjx-c class="mjx-c1D452 TEX-I"></mjx-c><mjx-c class="mjx-c1D45F TEX-I"></mjx-c><mjx-c class="mjx-c1D460 TEX-I"></mjx-c><mjx-c class="mjx-c1D44E TEX-I"></mjx-c><mjx-c class="mjx-c1D45F TEX-I"></mjx-c><mjx-c class="mjx-c1D456 TEX-I"></mjx-c><mjx-c class="mjx-c1D452 TEX-I"></mjx-c><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mtext></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext mathvariant="italic">relational adversaries</mtext></math></mjx-assistive-mml></mjx-container>, a broad class of attackers who create adversarial examples in a reflexive-transitive closure of a logical relation. We analyze the conditions for robustness against relational adversaries and investigate different levels of robustness-accuracy trade-off due to various patterns in a relation. Inspired by the insights, we propose <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="2" style="font-size: 121.3%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mtext class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c><mjx-c class="mjx-c1D45C TEX-I"></mjx-c><mjx-c class="mjx-c1D45F TEX-I"></mjx-c><mjx-c class="mjx-c1D45A TEX-I"></mjx-c><mjx-c class="mjx-c1D44E TEX-I"></mjx-c><mjx-c class="mjx-c1D459 TEX-I"></mjx-c><mjx-c class="mjx-c1D456 TEX-I"></mjx-c><mjx-c class="mjx-c1D467 TEX-I"></mjx-c><mjx-c class="mjx-c1D452 TEX-I"></mjx-c><mjx-c class="mjx-c2D TEX-MI"></mjx-c><mjx-c class="mjx-c1D44E TEX-I"></mjx-c><mjx-c class="mjx-c1D45B TEX-I"></mjx-c><mjx-c class="mjx-c1D451 TEX-I"></mjx-c><mjx-c class="mjx-c2D TEX-MI"></mjx-c><mjx-c class="mjx-c1D45D TEX-I"></mjx-c><mjx-c class="mjx-c1D45F TEX-I"></mjx-c><mjx-c class="mjx-c1D452 TEX-I"></mjx-c><mjx-c class="mjx-c1D451 TEX-I"></mjx-c><mjx-c class="mjx-c1D456 TEX-I"></mjx-c><mjx-c class="mjx-c1D450 TEX-I"></mjx-c><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mtext></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext mathvariant="italic">normalize-and-predict</mtext></math></mjx-assistive-mml></mjx-container>, a learning framework that leverages input normalization to achieve provable robustness. The framework solves the pain points of adversarial training against relational adversaries and can be combined with adversarial training for the benefits of both approaches. Guided by our theoretical findings, we apply our framework to source code authorship attribution and malware detection. Results of both tasks show our learning framework significantly improves the robustness of models against relational adversaries. In the process, it outperforms adversarial training, the most noteworthy defense mechanism, by a wide margin.</span>'}
{'title': 'Evolving the Olfactory System', 'authors': ['Robert Guangyu Yang', 'Peter Yiliu Wang', 'Yi Sun', 'Ashok Litwin-Kumar', 'Richard Axel', 'LF Abbott'], 'Conference': 'Real Neurons & Hidden Units @ NeurIPS 2019 Poster', 'date': 'Published: 02 Oct 2019, Last Modified: 05 May 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=BylUXXFI8S&name=pdf', 'abstract': '</span><span class="note_content_value">Flies and mice are species separated by 600 million years of evolution, yet have evolved olfactory systems that share many similarities in their anatomic and functional organization. What functions do these shared anatomical and functional features serve, and are they optimal for odor sensing? In this study, we address the optimality of evolutionary design in olfactory circuits by studying artificial neural networks trained to sense odors. We found that artificial neural networks quantitatively recapitulate structures inherent in the olfactory system, including the formation of glomeruli onto a compression layer and sparse and random connectivity onto an expansion layer. Finally, we offer theoretical justifications for each result. Our work offers a framework to explain the evolutionary convergence of olfactory circuits, and gives insight and logic into the anatomic and functional structure of the olfactory system.</span>'}
{'title': 'Scalable Rule-Based Representation Learning for Interpretable Classification', 'authors': ['Zhuo Wang', 'Wei Zhang', 'Ning Liu', 'Jianyong Wang'], 'Conference': 'NeurIPS 2021 Poster', 'date': 'Published: 09 Nov 2021, Last Modified: 16 Sept 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=q_fMLfwTAJY&name=pdf', 'abstract': '</span><span class="note_content_value">Rule-based models, e.g., decision trees, are widely used in scenarios demanding high model interpretability for their transparent inner structures and good model expressivity. However, rule-based models are hard to optimize, especially on large data sets, due to their discrete parameters and structures. Ensemble methods and fuzzy/soft rules are commonly used to improve performance, but they sacrifice the model interpretability. To obtain both good scalability and interpretability, we propose a new classifier, named Rule-based Representation Learner (RRL), that automatically learns interpretable non-fuzzy rules for data representation and classification. To train the non-differentiable RRL effectively, we project it to a continuous space and propose a novel training method, called Gradient Grafting, that can directly optimize the discrete model using gradient descent. An improved design of logical activation functions is also devised to increase the scalability of RRL and enable it to discretize the continuous features end-to-end. Exhaustive experiments on nine small and four large data sets show that RRL outperforms the competitive interpretable approaches and can be easily adjusted to obtain a trade-off between classification accuracy and model complexity for different scenarios. Our code is available at: <a href="https://github.com/12wang3/rrl" target="_blank" rel="nofollow">https://github.com/12wang3/rrl</a>.</span>'}
{'title': 'SCERL: A Benchmark for intersecting language and safe reinforcement learning', 'authors': ['Lan Hoang', 'Shivam Ratnakar', 'Nicolas Galichet', 'Akifumi Wachi', 'Keerthiram Murugesan', 'Songtao Lu', 'Mattia Atzeni', 'Michael Katz', 'Subhajit Chaudhury'], 'Conference': 'LaReL 2022', 'date': 'Published: 21 Oct 2022, Last Modified: 05 May 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=rNmrhsewsUX&name=pdf', 'abstract': '</span><span class="note_content_value">The issue of safety and robustness is a critical focus for AI research. Two lines of research are so far distinct, namely \\(i) safe reinforcement learning, where an agent needs to interact with the world under safety constraints, and (ii) textual reinforcement learning, where agents need to perform robust reasoning and modelling of the state of the environment. In this paper, we propose Safety-Constrained Environments for Reinforcement Learning (SCERL), a benchmark to bridge the gap between these two research directions. The contribution of this benchmark is safety-relevant environments with i) a sample set of 20 games built on new logical rules to represent physical safety issues; ii) added monitoring of safety violations and iii) a mechanism to further generate a more diverse set of games with safety constraints and their corresponding metrics of safety types and difficulties. This paper shows selected baseline results on the benchmark. Our aim is for the SCERL benchmark and its flexible framework to provide a set of tasks to demonstrate language-based safety challenges to inspire the research community to further explore safety applications in a text-based domain.</span>'}
{'title': 'Learning Interpretable Decision Rule Sets: A Submodular Optimization Approach', 'authors': ['Fan Yang', 'Kai He', 'Linxiao Yang', 'Hongxia Du', 'Jingbang Yang', 'Bo Yang', 'Liang Sun'], 'Conference': 'NeurIPS 2021 Spotlight', 'date': 'Published: 09 Nov 2021, Last Modified: 05 May 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=pZHGKM9mAp&name=pdf', 'abstract': '</span><span class="note_content_value">Rule sets are highly interpretable logical models in which the predicates for decision are expressed in disjunctive normal form (DNF, OR-of-ANDs), or, equivalently, the overall model comprises an unordered collection of if-then decision rules. In this paper, we consider a submodular optimization based approach for learning rule sets. The learning problem is framed as a subset selection task in which a subset of all possible rules needs to be selected to form an accurate and interpretable rule set. We employ an objective function that exhibits submodularity and thus is amenable to submodular optimization techniques. To overcome the difficulty arose from dealing with the exponential-sized ground set of rules, the subproblem of searching a rule is casted as another subset selection task that asks for a subset of features. We show it is possible to write the induced objective function for the subproblem as a difference of two submodular (DS) functions to make it approximately solvable by DS optimization algorithms. Overall, the proposed approach is simple, scalable, and likely to be benefited from further research on submodular optimization. Experiments on real datasets demonstrate the effectiveness of our method.</span>'}
{'title': 'Pyramid Attention For Source Code Summarization', 'authors': ['Lei Chai', 'Ming Li'], 'Conference': 'NeurIPS 2022 Accept', 'date': 'Published: 31 Oct 2022, Last Modified: 14 Jan 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=iFJJevyrIEf&name=pdf', 'abstract': '</span><span class="note_content_value">This paper presents a multi-granularity method for source code summarization, which generates a concise functional description for the given code snippet. We notice that skilled programmers write and read source codes hierarchically and pay close attention to conceptual entities like statements, tokens, sub-tokens, and the mapping relations between them. The entities have specific emphasis according to their granularities, e.g., statements in coarse-granularity reveal the global logical semantics of code, and the sub-tokens in fine-granularity are more related to the textual semantics. Driven by this observation, we demonstrate that a multi-granularity formulation incorporating these conceptual entities benefit the code summarization task. Concretely, the source code is transformed into a pyramidal representation, and then a pyramid attention mechanism is applied for efficient feature aggregation among different hierarchies in it. We instantiate our multi-granularity method using the proposed pyramid attention and name it PA-former (Pyramid Attention transformer). We evaluated it on two source code summarization benchmarks where it surpasses the prior works and achieves new state-of-the-art results. Our code and data are available at <a href="https://github.com/leichainju/pa-former" target="_blank" rel="nofollow">https://github.com/leichainju/pa-former</a>.</span>'}
{'title': 'A Variational Edge Partition Model for Supervised Graph Representation Learning', 'authors': ['Yilin He', 'Chaojie Wang', 'Hao Zhang', 'Bo Chen', 'Mingyuan Zhou'], 'Conference': 'NeurIPS 2022 Accept', 'date': 'Published: 31 Oct 2022, Last Modified: 30 Dec 2022', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=PfStAhJ2t1g&name=pdf', 'abstract': '</span><span class="note_content_value">Graph neural networks (GNNs), which propagate the node features through the edges and learn how to transform the aggregated features under label supervision, have achieved great success in supervised feature extraction for both node-level and graph-level  classification tasks. However, GNNs typically treat the graph structure as given and ignore how the edges are formed. This paper introduces a graph generative process to model how the observed edges are generated by aggregating the node interactions over a set of overlapping node communities, each of which contributes to the edges via a logical OR mechanism. Based on this generative model, we partition each edge into the summation of multiple community-specific weighted edges and use them to define community-specific GNNs. A variational inference framework is proposed to jointly learn a GNN-based inference network  that partitions the edges into different communities, these community-specific GNNs, and a GNN-based predictor that combines community-specific GNNs for the end classification task. Extensive evaluations on real-world graph datasets have verified the effectiveness of the proposed method in learning discriminative representations for both node-level and graph-level classification tasks.</span>'}
{'title': 'Numerical Reasoning over Legal Contracts via Relational Database', 'authors': ['Jiani Huang', 'Ziyang Li', 'Ilias Fountalis', 'Mayur Naik'], 'Conference': 'DBAI', 'date': 'Published: 20 Oct 2021, Last Modified: 05 May 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=a9_4vd4dczF&name=pdf', 'abstract': '</span><span class="note_content_value">Numerical reasoning over text requires deep integration between the semantic understanding of the natural language context and the mathematical calculation of the symbolic terms. However, existing approaches are limited in their ability to incorporate domain-specific knowledge and express mathematical formulas over data structures.  Delegating logic reasoning to a relational database is a promising approach to enhance the reasoning complexity. We study the problem of distilling natural language text into a relational database with numerical data structure and querying this database to obtain desired answers. Specifically, given a legal contract and a set of date-related questions in natural language, we utilize pre-trained neural network models to create a relational database to retrieve and generate the target dates. We evaluate our method on the CUAD dataset and demonstrate that our approach has high correct answer coverage and reduces a significant amount of incorrect results even without any labels.</span>'}
{'title': 'Instance-based Learning for Knowledge Base Completion', 'authors': ['Wanyun Cui', 'Xingran Chen'], 'Conference': 'NeurIPS 2022 Accept', 'date': 'Published: 31 Oct 2022, Last Modified: 16 Sept 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=1-F7HbLInPy&name=pdf', 'abstract': '</span><span class="note_content_value">In this paper, we propose a new method for knowledge base completion (KBC): instance-based learning (IBL). For example, to answer (Jill Biden, lived city,? ), instead of going directly to Washington D.C., our goal is to find Joe Biden, who has the same lived city as Jill Biden. Through prototype entities, IBL provides interpretability. We develop theories for modeling prototypes and combining IBL with translational models. Experiments on various tasks confirmed the IBL model\'s effectiveness and interpretability.\n\nIn addition, IBL shed light on the mechanism of rule-based KBC models. Previous research has generally agreed that rule-based models provide rules with semantically compatible premise and hypothesis. We challenge this view. We begin by demonstrating that some logical rules represent {\\it instance-based equivalence} (i.e. prototypes) rather than semantic compatibility. These are denoted as {\\it IBL rules}. Surprisingly, despite occupying only a small portion of the rule space, IBL rules outperform non-IBL rules in all four benchmarks. %KBC can be achieved using only IBL rules in two benchmarks without sacrificing effectiveness.  We use a variety of experiments to demonstrate that rule-based models work because they have the ability to represent instance-based equivalence via IBL rules. The findings provide new insights of how rule-based models work and how to interpret their rules.</span>'}
{'title': 'Emergence of Hierarchical Layers in a Single Sheet of Self-Organizing Spiking Neurons', 'authors': ['Paul Bertens', 'Seong-Whan Lee'], 'Conference': 'NeurIPS 2022 Accept', 'date': 'Published: 31 Oct 2022, Last Modified: 12 Oct 2022', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=cPVuuk1lZb3&name=pdf', 'abstract': '</span><span class="note_content_value">Traditionally convolutional neural network architectures have been designed by stacking layers on top of each other to form deeper hierarchical networks. The cortex in the brain however does not just stack layers as done in standard convolution neural networks, instead different regions are organized next to each other in a large single sheet of neurons. Biological neurons self organize to form topographic maps, where neurons encoding similar stimuli group together to form logical clusters. Here we propose new self-organization principles that allow for the formation of hierarchical cortical regions (i.e. layers) in a completely unsupervised manner without requiring any predefined architecture. Synaptic connections are dynamically grown and pruned, which allows us to actively constrain the number of incoming and outgoing connections. This way we can minimize the wiring cost by taking into account both the synaptic strength and the connection length. The proposed method uses purely local learning rules in the form of spike-timing-dependent plasticity (STDP) with lateral excitation and inhibition. We show experimentally that these self-organization rules are sufficient for topographic maps and hierarchical layers to emerge. Our proposed Self-Organizing Neural Sheet (SONS) model can thus form traditional neural network layers in a completely unsupervised manner from just a single large pool of unstructured spiking neurons.</span>'}
{'title': 'Sample-Efficient Generation of Novel Photo-acid Generator Molecules using a Deep Generative Model', 'authors': ['Samuel C Hoffman', 'Vijil Chenthamarakshan', 'Dmitry Zubarev', 'Daniel P Sanders', 'Payel Das'], 'Conference': 'DGMs and Applications @ NeurIPS 2021 Oral', 'date': 'Published: 07 Dec 2021, Last Modified: 05 May 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=_c8SM_V02Y&name=pdf', 'abstract': '</span><span class="note_content_value">Photo-acid generators (PAGs) are compounds that release acids (<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="0" style="font-size: 121.3%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43B TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em; margin-left: 0.053em;"><mjx-mo class="mjx-n" size="s"><mjx-c class="mjx-c2B"></mjx-c></mjx-mo></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>H</mi><mo>+</mo></msup></math></mjx-assistive-mml></mjx-container> ions) when exposed to light. These compounds are critical components of the photolithography processes that are used in the manufacture of semiconductor logic and memory chips. The exponential increase in the demand for semiconductors has highlighted the need for discovering novel photo-acid generators. While de novo molecule design using deep generative models has been widely employed for drug discovery and material design, its application to the creation of novel photo-acid generators poses several unique challenges, such as lack of property labels. In this paper, we highlight these challenges and propose a generative modeling approach that utilizes conditional generation from a pre-trained deep autoencoder and expert-in-the-loop techniques. The validity of the proposed approach was evaluated with the help of subject matter experts, indicating the promise of such an approach for applications beyond the creation of novel photo-acid generators.</span>'}
{'title': 'Forecasting Market Prices using DL with Data Augmentation and Meta-learning: ARIMA still wins!', 'authors': ['Vedant Shah', 'Gautam Shroff'], 'Conference': 'ICBINB@NeurIPS2021 Poster', 'date': 'Published: 18 Oct 2021, Last Modified: 05 May 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=udRAvWHIb2&name=pdf', 'abstract': '</span><span class="note_content_value">Deep-learning techniques have been successfully used for time-series forecasting and have often shown superior performance\non many standard benchmark datasets as compared to traditional techniques. Here we present a comprehensive and comparative \nstudy of performance of deep-learning techniques for forecasting prices in financial markets. We benchmark state-of-the-art\ndeep-learning baselines, such as NBeats, etc., on data from currency as well as stock markets. We also generate synthetic data using a fuzzy-logic based model of demand driven by technical rules such as moving averages, which are often used by traders. We benchmark\nthe baseline techniques on this synthetic data as well as use it for data augmentation. We also apply gradient-based \nmeta-learning to account for non-stationarity of financial time-series. Our extensive experiments notwithstanding, the\nsurprising result is that the standard ARIMA models outperforms deep-learning even using data augmentation or meta-learning. We\nconclude by speculating as to why this might be the case.</span>'}
{'title': 'Improving Compositionality of Neural Networks by Decoding Representations to Inputs', 'authors': ['Mike Wu', 'Noah Goodman', 'Stefano Ermon'], 'Conference': 'NeurIPS 2021 Poster', 'date': 'Published: 09 Nov 2021, Last Modified: 05 May 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=ms1fOdxXhWH&name=pdf', 'abstract': '</span><span class="note_content_value">In traditional software programs, it is easy to trace program logic from variables back to input, apply assertion statements to block erroneous behavior, and compose programs together. Although deep learning programs have demonstrated strong performance on novel applications, they sacrifice many of the functionalities of traditional software programs. With this as motivation, we take a modest first step towards improving deep learning programs by jointly training a generative model to constrain neural network activations to "decode" back to inputs. We call this design a Decodable Neural Network, or DecNN. Doing so enables a form of compositionality in neural networks, where one can recursively compose DecNN with itself to create an ensemble-like model with uncertainty. In our experiments, we demonstrate applications of this uncertainty to out-of-distribution detection, adversarial example detection, and calibration --- while matching standard neural networks in accuracy. We further explore this compositionality by combining DecNN with pretrained models, where we show promising results that neural networks can be regularized from using protected features.</span>'}
{'title': 'Improving Compositionality of Neural Networks by Decoding Representations to Inputs', 'authors': ['Mike Wu', 'Noah Goodman', 'Stefano Ermon'], 'Conference': 'NeurIPS 2021 Poster', 'date': 'Published: 09 Nov 2021, Last Modified: 05 May 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=jfd_GB546GJ&name=pdf', 'abstract': '</span><span class="note_content_value">In traditional software programs, it is easy to trace program logic from variables back to input, apply assertion statements to block erroneous behavior, and compose programs together. Although deep learning programs have demonstrated strong performance on novel applications, they sacrifice many of the functionalities of traditional software programs. With this as motivation, we take a modest first step towards improving deep learning programs by jointly training a generative model to constrain neural network activations to "decode" back to inputs. We call this design a Decodable Neural Network, or DecNN. Doing so enables a form of compositionality in neural networks, where one can recursively compose DecNN with itself to create an ensemble-like model with uncertainty. In our experiments, we demonstrate applications of this uncertainty to out-of-distribution detection, adversarial example detection, and calibration --- while matching standard neural networks in accuracy. We further explore this compositionality by combining DecNN with pretrained models, where we show promising results that neural networks can be regularized from using protected features.</span>'}
{'title': 'Abstract Reasoning with Distracting Features', 'authors': ['Kecheng Zheng'], 'Conference': 'NeurIPS 2019', 'date': '06 Sept 2019 (modified: 05 May 2023)', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=r1fO9NBxLS&name=pdf', 'abstract': '</span><span class="note_content_value"> Abstraction reasoning is a long-standing challenge in artificial intelligence. Recent studies abstract reasoning suggesting that many of the deep architectures that have triumphed over other domains failed to work well in abstract reasoning. We have first illustrated that one of the main challenges in such reasoning task is the presence of distracting features, which requires the learning algorithm to leverage counter-evidence to reject many of the false hypothesis before the true pattern can be discovered. Small scale experiments suggest that by carefully design a learning trajectory over the types of training data can effectively boost training performance by mitigating the impacts of distracting features. Inspired by that, we designed a reinforcement learning based teacher network to determine the sequence of training for optimization performance. In combined with a deep neural network that extracts logic features, we are able to beat the state-of-the-art models by up to 14.5\\% in RAVEN dataset and 13.5\\% in the PGM dataset.  </span>'}
{'title': 'A Policy-Guided Imitation Approach for Offline Reinforcement Learning', 'authors': ['Haoran Xu', 'Li Jiang', 'Jianxiong Li', 'Xianyuan Zhan'], 'Conference': 'NeurIPS 2022 Accept', 'date': 'Published: 31 Oct 2022, Last Modified: 16 Sept 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=CKbqDtZnSc&name=pdf', 'abstract': '</span><span class="note_content_value">Offline reinforcement learning (RL) methods can generally be categorized into two types: RL-based and Imitation-based. RL-based methods could in principle enjoy out-of-distribution generalization but suffer from erroneous off-policy evaluation. Imitation-based methods avoid off-policy evaluation but are too conservative to surpass the dataset. In this study, we propose an alternative approach, inheriting the training stability of imitation-style methods while still allowing logical out-of-distribution generalization. We decompose the conventional reward-maximizing policy in offline RL into a guide-policy and an execute-policy. During training, the guide-poicy and execute-policy are learned using only data from the dataset, in a supervised and decoupled manner. During evaluation, the guide-policy guides the execute-policy by telling where it should go so that the reward can be maximized, serving as the \\textit{Prophet}. By doing so, our algorithm allows \\textit{state-compositionality} from the dataset, rather than \\textit{action-compositionality} conducted in prior imitation-style methods. We dumb this new approach Policy-guided Offline RL (\\texttt{POR}). \\texttt{POR} demonstrates the state-of-the-art performance on D4RL, a standard benchmark for offline RL. We also highlight the benefits of \\texttt{POR} in terms of improving with supplementary suboptimal data and easily adapting to new tasks by only changing the guide-poicy.</span>'}
{'title': 'Controllable and Compositional Generation with Latent-Space Energy-Based Models', 'authors': ['Weili Nie', 'Arash Vahdat', 'Anima Anandkumar'], 'Conference': 'NeurIPS 2021 Poster', 'date': 'Published: 09 Nov 2021, Last Modified: 05 May 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=kcI3T5qe1jr&name=pdf', 'abstract': '</span><span class="note_content_value">Controllable generation is one of the key requirements for successful adoption of deep generative models in real-world applications, but it still remains as a great challenge. In particular, the compositional ability to generate novel concept combinations is out of reach for most current models. In this work, we use energy-based models (EBMs) to handle compositional generation over a set of attributes. To make them scalable to high-resolution image generation, we introduce an EBM in the latent space of a pre-trained generative model such as StyleGAN. We propose a novel EBM formulation representing the joint distribution of data and attributes together, and we show how sampling from it is formulated as solving an ordinary differential equation (ODE). Given a pre-trained generator, all we need for controllable generation is to train an attribute classifier. Sampling with ODEs is done efficiently in the latent space and is robust to hyperparameters. Thus, our method is simple, fast to train, and efficient to sample. Experimental results show that our method outperforms the state-of-the-art in both conditional sampling and sequential editing. In compositional generation, our method excels at zero-shot generation of unseen attribute combinations. Also, by composing energy functions with logical operators, this work is the first to achieve such compositionality in generating photo-realistic images of resolution 1024x1024.\n</span>'}
{'title': 'Memory safe computations with XLA compiler', 'authors': ['Artem Artemev', 'Yuze An', 'Tilman Roeder', 'Mark van der Wilk'], 'Conference': 'NeurIPS 2022 Accept', 'date': 'Published: 31 Oct 2022, Last Modified: 16 Sept 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=2S_GtHBtTUP&name=pdf', 'abstract': '</span><span class="note_content_value">Software packages like TensorFlow and PyTorch are designed to support linear algebra operations, and their speed and usability determine their success. However, by prioritising speed, they often neglect memory requirements. As a consequence, the implementations of memory-intensive algorithms that are convenient in terms of software design can often not be run for large problems due to memory overflows. Memory-efficient solutions require complex programming approaches with significant logic outside the computational framework. This impairs the adoption and use of such algorithms. To address this, we developed an XLA compiler extension that adjusts the computational data-flow representation of an algorithm according to a user-specified memory limit. We show that k-nearest neighbour, sparse Gaussian process regression methods and Transformers can be run on a single device at a much larger scale, where standard implementations would have failed. Our approach leads to better use of hardware resources. We believe that further focus on removing memory constraints at a compiler level will widen the range of machine learning methods that can be developed in the future.</span>'}
{'title': 'Robust Counterfactual Explanations on Graph Neural Networks', 'authors': ['Mohit Bajaj', 'Lingyang Chu', 'Zi Yu Xue', 'Jian Pei', 'Lanjun Wang', 'Peter Cho-Ho Lam', 'Yong Zhang'], 'Conference': 'NeurIPS 2021 Poster', 'date': 'Published: 09 Nov 2021, Last Modified: 16 Sept 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=wGmOLwb8ClT&name=pdf', 'abstract': '</span><span class="note_content_value">Massive deployment of Graph Neural Networks (GNNs) in high-stake applications generates a strong demand for explanations that are robust to noise and align well with human intuition. Most existing methods generate explanations by identifying a subgraph of an input graph that has a strong correlation with the prediction. These explanations are not robust to noise because independently optimizing the correlation for a single input can easily overfit noise. Moreover, they are not counterfactual because removing an identified subgraph from an input graph does not necessarily change the prediction result. In this paper, we propose a novel method to generate robust counterfactual explanations on GNNs by explicitly modelling the common decision logic of GNNs on similar input graphs. Our explanations are naturally robust to noise because they are produced from the common decision boundaries of a GNN that govern the predictions of many similar input graphs. The explanations are also counterfactual because removing the set of edges identified by an explanation from the input graph changes the prediction significantly. Exhaustive experiments on many public datasets demonstrate the superior performance of our method.</span>'}
{'title': 'Robust Counterfactual Explanations on Graph Neural Networks', 'authors': ['Mohit Bajaj', 'Lingyang Chu', 'Zi Yu Xue', 'Jian Pei', 'Lanjun Wang', 'Peter Cho-Ho Lam', 'Yong Zhang'], 'Conference': 'NeurIPS 2021 Poster', 'date': 'Published: 09 Nov 2021, Last Modified: 16 Sept 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=Uq_tGs7N54M&name=pdf', 'abstract': '</span><span class="note_content_value">Massive deployment of Graph Neural Networks (GNNs) in high-stake applications generates a strong demand for explanations that are robust to noise and align well with human intuition. Most existing methods generate explanations by identifying a subgraph of an input graph that has a strong correlation with the prediction. These explanations are not robust to noise because independently optimizing the correlation for a single input can easily overfit noise. Moreover, they are not counterfactual because removing an identified subgraph from an input graph does not necessarily change the prediction result. In this paper, we propose a novel method to generate robust counterfactual explanations on GNNs by explicitly modelling the common decision logic of GNNs on similar input graphs. Our explanations are naturally robust to noise because they are produced from the common decision boundaries of a GNN that govern the predictions of many similar input graphs. The explanations are also counterfactual because removing the set of edges identified by an explanation from the input graph changes the prediction significantly. Exhaustive experiments on many public datasets demonstrate the superior performance of our method.</span>'}
{'title': 'System III: Learning with Domain Knowledge for Safety Constraints', 'authors': ['Fazl Barez', 'Hosein Hasanbeig', 'Alessandro Abate'], 'Conference': 'MLSW2022', 'date': 'Published: 05 Dec 2022, Last Modified: 05 May 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=85mcrDoWOAH&name=pdf', 'abstract': '</span><span class="note_content_value">Reinforcement learning agents naturally learn from extensive exploration. Exploration is costly and can be unsafe in safety-critical domains. This paper proposes a novel framework for incorporating domain knowledge to help guide safe exploration and boost sample efficiency. Previous approaches impose constraints, such as regularisation parameters in neural networks, that rely on large sample sets and often are not suitable for safety-critical domains where agents should almost always avoid unsafe actions. In our approach, called System III, which is inspired by psychologists\' notions of the brain\'s System I and System II\nwe represent domain expert knowledge of safety in form of first-order logic. We evaluate the satisfaction of these constraints via p-norms in state vector space. In our formulation, constraints are analogous to hazards, objects, and regions of state that have to be avoided during exploration.\nWe evaluated the effectiveness of the proposed method on OpenAI\'s Gym and Safety-Gym environments.\nIn all tasks, including classic Control and Safety Games, we show that our approach results in safer exploration and sample efficiency.\n</span>'}
{'title': 'Surprise-Guided Search for Learning Task Specifications From Demonstrations', 'authors': ['Marcell Vazquez-Chanlatte', 'Ameesh Shah', 'Gil Lederman', 'Sanjit A. Seshia'], 'Conference': 'NeurIPS 2022 Submitted', 'date': '16 May 2022 (modified: 05 May 2023)', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=xjXN3wEvCGG&name=pdf', 'abstract': '</span><span class="note_content_value">This paper considers the problem of learning temporal task specifications, e.g. automata and temporal logic, from expert demonstrations. Task specifications are a class of sparse memory augmented rewards with explicit support for temporal and Boolean composition.  Three features make learning temporal task specifications difficult: (1) the (countably) infinite number of tasks under consideration, (2) an a-priori ignorance of what memory is needed to encode the task, and (3) the discrete solution space - typically addressed by (brute force) enumeration. To overcome these hurdles, we propose Demonstration Informed Specification Search (DISS): a family of algorithms requiring only black box access to (i) a maximum entropy planner and (ii) a task sampler from labeled examples. DISS works by alternating between (i) conjecturing labeled examples to make the provided demonstrations less surprising and (ii) sampling tasks consistent with the conjectured labeled examples. We provide a concrete implementation of DISS in the context of tasks described by Deterministic Finite Automata, and show that DISS is able to efficiently identify tasks from only one or two expert demonstrations.</span>'}
{'title': 'CRAFT: explaining using Concepts from Recursive Activation FacTorization', 'authors': ['Thomas FEL', 'Agustin Martin Picard', 'Louis Béthune', 'Thibaut Boissin', 'Julien Colin', 'David Vigouroux', 'Remi Cadene', 'Thomas Serre'], 'Conference': 'NeurIPS 2022 Submitted', 'date': '16 May 2022 (modified: 05 May 2023)', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=8FuITQn6rG3&name=pdf', 'abstract': '</span><span class="note_content_value">Despite their considerable potential, concept-based explainability methods have received relatively little attention, and explaining what’s driving models’ decisions and where it’s located in the input is still an open problem. To tackle this, we revisit unsupervised concept extraction techniques for explaining the decisions of deep neural networks and present CRAFT – a framework to generate concept-based explanations for understanding individual predictions and the model’s high-level logic for whole classes. CRAFT takes advantage of a novel method for recursively decomposing higher-level concepts into more elementary ones, combined with a novel approach for better estimating the importance of identified concepts with Sobol indices. Furthermore, we show how implicit differentiation can be used to generate concept-wise attribution explanations for individual images. We further demonstrate through fidelity metrics that our proposed concept importance estimation technique is more faithful to the model than previous methods, and, through human psychophysic experiments, we confirm that our recursive decomposition can generate meaningful and accurate concepts. Finally, we illustrate CRAFT’s potential to enable the understanding of predictions of trained models on multiple use-cases by producing meaningful concept-based explanations.</span>'}
{'title': 'Learning Parameterized Task Structure for Generalization to Unseen Entities', 'authors': ['Anthony Zhe Liu', 'Sungryull Sohn', 'Mahdi Qazwini', 'Honglak Lee'], 'Conference': 'Deep RL Workshop NeurIPS 2021', 'date': '12 Oct 2021 (modified: 05 May 2023)', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=64VbMsh3IpY&name=pdf', 'abstract': '</span><span class="note_content_value">Real world tasks are hierarchical and compositional. Tasks can be composed of multiple subtasks (or sub-goals) that are dependent on each other. These subtasks are defined in terms of entities (e.g., "apple", "pear") that can be recombined to form new subtasks (e.g., "pickup apple", and "pickup pear"). To solve these tasks efficiently, an agent must infer subtask dependencies (e.g. an agent must execute "pickup apple" before "place apple in pot"), and generalize the inferred dependencies to new subtasks (e.g. "place apple in pot" is similar to "place apple in pan"). Moreover, an agent may also need to solve unseen tasks, which can involve unseen entities. To this end, we formulate parameterized subtask graph inference (PSGI), a method for modeling subtask dependencies using first-order logic with factored entities. To facilitate this, we learn parameter attributes in a zero-shot manner, which are used as quantifiers (e.g. is_pickable(X)) for the factored subtask graph. We show this approach accurately learns the latent structure on hierarchical and compositional tasks more efficiently than prior work, and show PSGI can generalize by modelling structure on subtasks unseen during adaptation.</span>'}
{'title': 'A Primal Dual Formulation For Deep Learning With Constraints', 'authors': ['Yatin Nandwani'], 'Conference': 'NeurIPS 2019', 'date': '06 Sept 2019 (modified: 05 May 2023)', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=SJzycBrg8H&name=pdf', 'abstract': '</span><span class="note_content_value">For several problems of interest, there are natural constraints which exist over the output label space. For example, for the joint task of NER and POS labeling, these constraints might specify that the NER label  `organization\' is consistent only with the POS labels `noun\' and `preposition\'. These constraints can be a great way of injecting prior knowledge into a deep learning model, thereby improving overall performance. In this paper, we present a constrained optimization formulation for training a deep network with a given set of hard constraints on output labels. Our novel approach first converts the label constraints into soft logic constraints over probability distributions outputted by the network. It then converts the constrained optimization problem into an alternating min-max optimization with Lagrangian variables defined for each constraint. Since the constraints are independent of the target labels, our framework easily generalizes to semi-supervised setting. We experiment on the tasks of Semantic Role Labeling (SRL), Named Entity Recognition (NER) tagging, and fine-grained entity typing and show that our constraints not only significantly reduce the number of constraint violations, but can also result in state-of-the-art performance.</span>'}
{'title': 'Learning Robust Rule Representations for Abstract Reasoning via Internal Inferences', 'authors': ['Wenbo Zhang', 'Likai Tang', 'Site Mo', 'Xianggen Liu', 'Sen Song'], 'Conference': 'NeurIPS 2022 Accept', 'date': 'Published: 31 Oct 2022, Last Modified: 12 Oct 2022', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=UwzrP-B38jK&name=pdf', 'abstract': '</span><span class="note_content_value">Abstract reasoning, as one of the hallmarks of human intelligence, involves collecting information, identifying abstract rules, and applying the rules to solve new problems. Although neural networks have achieved human-level performances in several tasks, the abstract reasoning techniques still far lag behind due to the complexity of learning and applying the logic rules, especially in an unsupervised manner. In this work, we propose a novel framework, ARII, that learns rule representations for Abstract Reasoning via Internal Inferences. The key idea is to repeatedly apply a rule to different instances in hope of having a comprehensive understanding (i.e., representations) of the rule. Specifically, ARII consists of a rule encoder, a reasoner, and an internal referrer. Based on the representations produced by the rule encoder, the reasoner draws the conclusion while the referrer performs internal inferences to regularize rule representations to be robust and generalizable. We evaluate ARII on two benchmark datasets, including PGM and I-RAVEN. We observe that ARII achieves new state-of-the-art records on the majority of the reasoning tasks, including most of the generalization tests in PGM. Our codes are available at <a href="https://github.com/Zhangwenbo0324/ARII" target="_blank" rel="nofollow">https://github.com/Zhangwenbo0324/ARII</a>.</span>'}
{'title': 'DDXPlus: A New Dataset For Automatic Medical Diagnosis', 'authors': ['Arsene Fansi Tchango', 'Rishab Goel', 'Zhi Wen', 'Julien Martel', 'Joumana Ghosn'], 'Conference': 'NeurIPS 2022 Datasets and Benchmarks', 'date': 'Published: 16 Sept 2022, Last Modified: 23 May 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=heBKnuV42O&name=pdf', 'abstract': '</span><span class="note_content_value">There has been a rapidly growing interest in Automatic Symptom Detection (ASD) and Automatic Diagnosis (AD) systems in the machine learning research literature, aiming to assist doctors in telemedicine services. These systems are designed to interact with patients, collect evidence about their symptoms and relevant antecedents, and possibly make predictions about the underlying diseases. Doctors would review the interactions, including the evidence and the predictions, collect if necessary additional information from patients, before deciding on next steps. Despite recent progress in this area, an important piece of doctors\' interactions with patients is missing in the design of these systems, namely the differential diagnosis. Its absence is largely due to the lack of datasets that include such information for models to train on. In this work, we present a large-scale synthetic dataset of roughly 1.3 million patients that includes a differential diagnosis, along with the ground truth pathology, symptoms and antecedents for each patient. Unlike existing datasets which only contain binary symptoms and antecedents, this dataset also contains categorical and multi-choice symptoms and antecedents useful for efficient data collection. Moreover, some symptoms are organized in a hierarchy, making it possible to design systems able to interact with patients in a logical way. As a proof-of-concept, we extend two existing AD and ASD systems to incorporate the differential diagnosis, and provide empirical evidence that using differentials as training signals is essential for the efficiency of such systems or for helping doctors better understand the reasoning of those systems.</span>'}
{'title': 'Contrastive Reinforcement Learning of Symbolic Reasoning Domains', 'authors': ['Gabriel Poesia', 'WenXin Dong', 'Noah Goodman'], 'Conference': 'NeurIPS 2021 Poster', 'date': 'Published: 09 Nov 2021, Last Modified: 16 Sept 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=ZarM_uLVyGw&name=pdf', 'abstract': '</span><span class="note_content_value">Abstract symbolic reasoning, as required in domains such as mathematics and logic, is a key component of human intelligence. Solvers for these domains have important applications, especially to computer-assisted education. But learning to solve symbolic problems is challenging for machine learning algorithms. Existing models either learn from human solutions or use hand-engineered features, making them expensive to apply in new domains. In this paper, we instead consider symbolic domains as simple environments where states and actions are given as unstructured text, and binary rewards indicate whether a problem is solved. This flexible setup makes it easy to specify new domains, but search and planning become challenging. We introduce five environments inspired by the Mathematics Common Core Curriculum, and observe that existing Reinforcement Learning baselines perform poorly. We then present a novel learning algorithm, Contrastive Policy Learning (ConPoLe) that explicitly optimizes the InfoNCE loss, which lower bounds the mutual information between the current state and next states that continue on a path to the solution. ConPoLe successfully solves all four domains. Moreover, problem representations learned by ConPoLe enable accurate prediction of the categories of problems in a real mathematics curriculum. Our results suggest new directions for reinforcement learning in symbolic domains, as well as applications to mathematics education.</span>'}
{'title': 'Refining Language Models with Compositional Explanations', 'authors': ['Huihan Yao', 'Ying Chen', 'Qinyuan Ye', 'Xisen Jin', 'Xiang Ren'], 'Conference': 'NeurIPS 2021 Spotlight', 'date': 'Published: 09 Nov 2021, Last Modified: 05 May 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=dkw9OQMn1t&name=pdf', 'abstract': '</span><span class="note_content_value">Pre-trained language models have been successful on text classification tasks, but are prone to learning spurious correlations from biased datasets, and are thus vulnerable when making inferences in a new domain. Prior work reveals such spurious patterns via post-hoc explanation algorithms which compute the importance of input features. Further, the model is regularized to align the importance scores with human knowledge, so that the unintended model behaviors are eliminated. However, such a regularization technique lacks flexibility and coverage, since only importance scores towards a pre-defined list of features are adjusted, while more complex human knowledge such as feature interaction and pattern generalization can hardly be incorporated. In this work, we propose to refine a learned language model for a target domain by collecting human-provided compositional explanations regarding observed biases. By parsing these explanations into executable logic rules, the human-specified refinement advice from a small set of explanations can be generalized to more training examples. We additionally introduce a regularization term allowing adjustments for both importance and interaction of features to better rectify model behavior. We demonstrate the effectiveness of the proposed approach on two text classification tasks by showing improved performance in target domain as well as improved model fairness after refinement.</span>'}
{'title': 'Focused Quantization for Sparse CNNs', 'authors': ['Xitong Gao'], 'Conference': 'NeurIPS 2019', 'date': '06 Sept 2019 (modified: 05 May 2023)', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=HylmqVHx8B&name=pdf', 'abstract': '</span><span class="note_content_value">Deep convolutional neural networks (CNNs) are powerful tools for a wide range of vision tasks, but the enormous amount of memory and compute resources required by CNNs poses a challenge in deploying them on constrained devices. Existing compression techniques, while excelling at reducing model sizes, struggle to be computationally friendly. In this paper, we attend to the statistical properties of sparse CNNs and present focused quantization, a novel quantization strategy based on power-of-two values, which exploits the weight distributions after fine-grained pruning. The proposed method dynamically discovers the most effective numerical representation for weights in layers with varying sparsities, significantly reducing model sizes. Multiplications in quantized CNNs are replaced with much cheaper bit-shift operations for efficient inference. Coupled with lossless encoding, we build a compression pipeline that provides CNNs with high compression ratios (CR), low computation cost and minimal loss in accuracies. In ResNet-50, we achieved a 18.08x CR with only 0.24% loss in top-5 accuracy, outperforming existing compression methods. We fully compress a ResNet-18 and found that it is not only higher in CR and top-5 accuracy, but also more hardware efficient as it requires fewer logic gates to implement when compared to other state-of-the-art quantization methods assuming the same throughput.</span>'}
{'title': 'Basil: A Fast and Byzantine-Resilient Approach for Decentralized Training', 'authors': ['Ahmed Roushdy Elkordy', 'Saurav Prakash', 'Salman Avestimehr'], 'Conference': 'PRIML 2021 Poster', 'date': 'Published: 04 Nov 2021, Last Modified: 15 May 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=_vj5wbUcgRB&name=pdf', 'abstract': '</span><span class="note_content_value">Decentralized (i.e., serverless) learning across a large number of distributed nodes (e.g., mobile users) has seen a surge of  recent interests. The key advantage of these setups is that they provide privacy for the local data of the users while not requiring a server for coordinating the training. They can, however, suffer substantially from potential Byzantine nodes in the network who can degrade the training performance. Detection and mitigation of Byzantine behaviors in a decentralized learning setting is a daunting task, especially when the data distribution at the users is heterogeneous. As our main contribution, we propose \\texttt{Basil}, a fast and computationally efficient Byzantine robust algorithm  for decentralized training systems, which leverages a novel sequential, memory assisted and performance based criteria for training over a logical ring while filtering the Byzantine users. In the IID dataset distribution setting, we provide the theoretical convergence guarantees of  \\texttt{Basil}, demonstrating its linear convergence rate. Furthermore, for the IID setting, we experimentally demonstrate that \\texttt{Basil} is robust to various  Byzantine attacks,  including the strong  Hidden  attack, while providing up to <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="0" style="font-size: 121.3%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c223C"></mjx-c></mjx-mo></mjx-texatom><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c36"></mjx-c></mjx-mn><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mo>∼</mo></mrow><mn>16</mn><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container> higher test accuracy over the state-of-the-art Byzantine-resilient decentralized learning approach. Additionally, we generalize \\texttt{Basil} to the non-IID dataset distribution setting by proposing Anonymous Cyclic Data Sharing (ACDS), a technique that allows each node to anonymously share a random fraction of its  local non-sensitive dataset (e.g., landmarks images)   with all other nodes. We demonstrate that \\texttt{Basil} alongside ACDS with only <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="1" style="font-size: 121.3%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c35"></mjx-c></mjx-mn><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>5</mn><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container> data sharing provides effective toleration of Byzantine nodes, unlike the state-of-the-art Byzantine robust algorithm that completely fails in the heterogeneous data setting.</span>'}
{'title': 'Large Language Models are Zero-Shot Reasoners', 'authors': ['Takeshi Kojima', 'Shixiang Shane Gu', 'Machel Reid', 'Yutaka Matsuo', 'Yusuke Iwasawa'], 'Conference': 'NeurIPS 2022 Accept', 'date': 'Published: 31 Oct 2022, Last Modified: 16 Sept 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=e2TBb5y0yFf&name=pdf', 'abstract': '</span><span class="note_content_value">Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs\' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding ``Let\'s think step by step\'\' before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects),  without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with large-scale InstructGPT model (text-davinci-002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.</span>'}
{'title': 'VigDet: Knowledge Informed Neural Temporal Point Process for Coordination Detection on Social Media', 'authors': ['Yizhou Zhang', 'Karishma Sharma', 'Yan Liu'], 'Conference': 'NeurIPS 2021 Poster', 'date': 'Published: 09 Nov 2021, Last Modified: 05 May 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=wo_0R04TSrF&name=pdf', 'abstract': '</span><span class="note_content_value">Recent years have witnessed an increasing use of coordinated accounts on social media, operated by misinformation campaigns to influence public opinion and manipulate social outcomes. Consequently, there is an urgent need to develop an effective methodology for coordinated group detection to combat the misinformation on social media. However, existing works suffer from various drawbacks, such as, either limited performance due to extreme reliance on predefined signatures of coordination, or instead an inability to address the natural sparsity of account activities on social media with useful prior domain knowledge. Therefore, in this paper, we propose a coordination detection framework incorporating neural temporal point process with prior knowledge such as temporal logic or pre-defined filtering functions. Specifically, when modeling the observed data from social media with neural temporal point process, we jointly learn a Gibbs-like distribution of group assignment based on how consistent an assignment is to (1) the account embedding space and (2) the prior knowledge. To address the challenge that the distribution is hard to be efficiently computed and sampled from, we design a theoretically guaranteed variational inference approach to learn a mean-field approximation for it. Experimental results on a real-world dataset show the effectiveness of our proposed method compared to the SOTA model in both unsupervised and semi-supervised settings. We further apply our model on a COVID-19 Vaccine Tweets dataset. The detection result suggests the presence of suspicious coordinated efforts on spreading misinformation about COVID-19 vaccines.</span>'}
{'title': 'VigDet: Knowledge Informed Neural Temporal Point Process for Coordination Detection on Social Media', 'authors': ['Yizhou Zhang', 'Karishma Sharma', 'Yan Liu'], 'Conference': 'NeurIPS 2021 Poster', 'date': 'Published: 09 Nov 2021, Last Modified: 05 May 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=sYNr-OqGC9m&name=pdf', 'abstract': '</span><span class="note_content_value">Recent years have witnessed an increasing use of coordinated accounts on social media, operated by misinformation campaigns to influence public opinion and manipulate social outcomes. Consequently, there is an urgent need to develop an effective methodology for coordinated group detection to combat the misinformation on social media. However, existing works suffer from various drawbacks, such as, either limited performance due to extreme reliance on predefined signatures of coordination, or instead an inability to address the natural sparsity of account activities on social media with useful prior domain knowledge. Therefore, in this paper, we propose a coordination detection framework incorporating neural temporal point process with prior knowledge such as temporal logic or pre-defined filtering functions. Specifically, when modeling the observed data from social media with neural temporal point process, we jointly learn a Gibbs-like distribution of group assignment based on how consistent an assignment is to (1) the account embedding space and (2) the prior knowledge. To address the challenge that the distribution is hard to be efficiently computed and sampled from, we design a theoretically guaranteed variational inference approach to learn a mean-field approximation for it. Experimental results on a real-world dataset show the effectiveness of our proposed method compared to the SOTA model in both unsupervised and semi-supervised settings. We further apply our model on a COVID-19 Vaccine Tweets dataset. The detection result suggests the presence of suspicious coordinated efforts on spreading misinformation about COVID-19 vaccines.</span>'}
{'title': 'First is Better Than Last for Language Data Influence', 'authors': ['Chih-Kuan Yeh', 'Ankur Taly', 'Mukund Sundararajan', 'Frederick Liu', 'Pradeep Kumar Ravikumar'], 'Conference': 'NeurIPS 2022 Accept', 'date': 'Published: 31 Oct 2022, Last Modified: 11 Jan 2023', 'link': 'https://openreview.nethttps://openreview.net/attachment?id=yfrDD_rmD5&name=pdf', 'abstract': '</span><span class="note_content_value">    The ability to identify influential training examples enables us to debug training data and explain model behavior. Existing techniques to do so are based on the flow of training data influence through the model parameters. For large models in NLP applications, it is often computationally infeasible to study this flow through all model parameters, therefore techniques usually pick the last layer of weights. However, we observe that since the activation connected to the last layer of weights contains "shared logic", the data influenced calculated via the last layer weights prone to a "cancellation effect", where the data influence of different examples have large magnitude that contradicts each other. The cancellation effect lowers the discriminative power of the influence score, and deleting influential examples according to this measure often does not change the model\'s behavior by much. To mitigate this, we propose a technique called TracIn-WE that modifies a method called TracIn to operate on the word embedding layer instead of the last layer, where the cancellation effect is less severe. One potential concern is that influence based on the word embedding layer may not encode sufficient high level information.  However, we find that gradients (unlike embeddings) do not suffer from this, possibly because they chain through higher layers. We show that TracIn-WE significantly outperforms other data influence methods applied on the last layer significantly on the case deletion evaluation on three language classification tasks for different models. In addition, TracIn-WE can produce scores not just at the level of the overall training input, but also at the level of words within the training input, a further aid in debugging. </span>'}
